[{"content":"教程\r原理\n1 2 3 4 5 6 7 8 9 远程 VS Code ↓（HTTP Proxy = 127.0.0.1:PORT） 远程 SSH 的 127.0.0.1 ↓（RemoteForward） 你本地的 127.0.0.1:PORT ↓ 你本地的代理（Clash / v2ray / shadowsocks） ↓ OpenAI/... 假设使用Clash，默认端口在7890\nSSH Config\r首先是VSC的ssh配置文件\n原本是：\n1 2 3 Host xx.xx.xx.xx HostName xx.xx.xx.xx User niu 增加一行\n1 2 3 4 Host xx.xx.xx.xx HostName xx.xx.xx.xx User niu RemoteForward 7890 127.0.0.1:7890 VSC设置\r打开 VS Code 设置\nCtrl + , 或 Ctrl + Shift + P → Preferences: Open Settings (UI) 搜索 proxy\n找到设置：\n1 Http: Proxy 填：\n1 http://127.0.0.1:7890 reload\r1 2 Ctrl + Shift + P → Developer: Reload Window 此时codex应该就能正常连接登录了\n","date":"2026-01-16T21:55:42+08:00","permalink":"https://example.com/p/%E5%A6%82%E4%BD%95%E5%9C%A8vsc%E4%B8%AD%E7%9A%84remotessh%E4%BD%BF%E7%94%A8%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E7%9A%84%E6%8F%92%E4%BB%B6/","title":"如何在VSC中的RemoteSSH使用需要科学的插件"},{"content":"[TOC]\nRAP Retrieval-Augmented Personalization for Multimodal Large Language Models\rRAP Retrieval-Augmented Personalization for Multimodal Large Language Models\nIntro\r对每个concept进行微调是昂贵的 MyVLM和Yo\u0026rsquo;LLaVA都需要进行额外的训练去添加新的concept从而实现个性化 都需要一些样本的正例与负例进行学习 RAP只需要一张图以及一些相关的信息 支持用户实时修改database，从而调整模型的个性化输出 同时RAP构建了一个Pipeline，用于收集大量训练数据，帮助模型先学会理解如何使用用户个人concept的范式\npaper的贡献总结为：\n提出RAP-MLLM框架，支持用户在不需要训练的情况下让模型适应新的concept 开发了一个训练数据收集pipeline，支持训练个性化assistant 最终模型在image captioning and question answering等个性化任务中表现优异 Method\rRAP Framework\r建立一个数据库，记录每个concept的一个头像、描述，使用视觉特征作为检索键 视觉特征通过一个现有的已经过训练的Encoder 检索 对于用户的图片输入 基于YOLO系列做detector，根据设定好的参数识别出ROI区域 将ROI区域送入Encoder，得到视觉特征，进行检索 计算欧拉距离最近的top-k组concept 对于用户文本输入 通过文本中的name，检索是否存在于database中，进行提取 生成：将多组（图片，描述）以及用户输入图片及描述送入MLM进行回答 Personalization Dataset\r如果只是这样，当时的模型也不一定总能生成准确的回复\n因此希望构建一个数据集，用于提升模型的生成能力\n视觉定位Visual grounding\r基于RefCOCO（单物体目标检测数据集）、ILSVRC2015-VID（ 视频目标检测数据集）、TAO 以及 CustomConcept101、Object365 （多对象数据集）\n输入：裁剪出物体对应的标注图像，使用Gemini1.5生成描述 训练输出：对应物体的边界框 指令遵循Instruction Following\r该部分包括：\n图像标题生成 (Image Captioning)：将目标概念的裁剪图、名称及描述注入模型输入，要求模型生成能反映这些特定概念的标题 问答 (Question Answering) ：利用种子问题迭代生成多样化的对话，涵盖视觉相关问题和纯文本查询 trick\r为了让模型对检索器返回的噪声更加鲁棒，在训练输入中加入噪声概念，但是要求模型的输出仍然不变，学会过滤信息 对裁剪出的图片进行旋转、翻转、3D合成新视角…… 加入LLaVA模型原有的数据集LLaVA-Instruct-665k，防止灾难性遗忘 Training-Free Personalization via Retrieval and Reasoning\rTraining-Free Personalization via Retrieval and Reasoning on Fingerprints\nIntro\r现有方法都需要进行训练 但是VLMs实际上接触的语义概念是足够多的，内部知识丰富 Paper同样构建了一个带有参考图像、描述的数据库\n使用VLM通过distinctive attributes对描述进行enrich\nMethod\rDatabase\r首先构建database\n输入： 参考图像 ($I_i$)：用户提供的包含特定个性化概念图片 概念名称 ($c_i$)：用户为该概念起的名称 类别 ($g_i$)：该概念所属的语义类别（如“毛绒玩具”或“猫”） 中间处理： 利用 VLM 提取指纹属性 ($A_i$)（即能将该物体与其同类区分开的关键特征，如“粉色领结”、“Nashville 吉他标志”）和判别性描述 ($d_i$) 利用视觉编码器和文本编码器生成图像特征 ($f_i^V$) 和文本特征 ($f_i^T$) 输出（存入数据库 $\\mathcal{D}$）： 包含 ${I_i, c_i, g_i, d_i, A_i, f_i^V, f_i^T}$ 的完整条目 文本特征由$d_i$作为输入生成\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Describe the \u0026lt; g_i \u0026gt; in the image identified by the concept-identifier \u0026lt; c_i \u0026gt; and highlight what makes it unique. Your response MUST be in valid JSON format and must follow EXACTLY the format below: { general: a brief description of the object in one sentence., category: category of the object, distinct features: [List of distinct features that makes the object unique], } IMPORTANT: * List only the most distinguishing features that set this object apart. * Avoid generic descriptions that apply to every object in this category. * Do not include any extra text or commentary. Any devi- ation from this format will be considered incorrect. 假设传入一张毛绒玩具的图像，及其名字和物体类别，该阶段返回内容为：\n1 2 3 4 5 6 7 8 9 10 { \u0026#34;general\u0026#34;: \u0026#34;A brown and white Shiba-inu dog plush toy in a sleeping posture.\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Stuffed animal\u0026#34;, \u0026#34;distinct features\u0026#34;: [ \u0026#34;Brown and white fur color\u0026#34;, \u0026#34;Closed eyes indicating a sleeping posture\u0026#34;, \u0026#34;Small pointed ears\u0026#34;, \u0026#34;Soft, plush texture\u0026#34; ] } 同时保存视觉特征和文本特征向量进入database\nMultimodal Retrieval\r输入：查询图像 将图像的视觉向量，分别与数据库中的视觉特征和文本特征进行相似度计算 最终的相似度分数由两者均值得到 输出：top-k的候选concepts 结合两者，避免出现视觉误导\nAttribute-focused CoT reasoning\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 You are a helpful AI agent specializing in image analysis and object recognition. Your task is to analyze and compare a query image with three provided descriptions. Below are the description(s): A. Name: \u0026lt;ci1\u0026gt;, Info: { general: A generic description about \u0026lt;ci1\u0026gt;, category: category of \u0026lt;ci1\u0026gt;, distinct features: [distinct feature 1, distinct feature 2, ...] } B. Name: \u0026lt;ci2\u0026gt;, Info: { general: A generic description about \u0026lt;ci2\u0026gt;, category: category of \u0026lt;ci2\u0026gt;, distinct features: [distinct feature 1, distinct feature 2, ...] } C. Name: \u0026lt;ci3\u0026gt;, Info: { general: A generic description about \u0026lt;ci3\u0026gt;, category: category of \u0026lt;ci3\u0026gt;, distinct features: [distinct feature 1, distinct feature 2, ...] } Your Task: - Compare the query image with each description and answer the following question: Which description matches the subject in the image? Answer in A, B, or C. - List shared attributes between the image and each description very concisely. - If no attributes match for a certain option, generate None. - Provide a brief reasoning for your final answer. Respond strictly in the following JSON format: { \u0026#34;A\u0026#34;: \u0026#34;[Matching attributes for option A]\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;[Matching attributes for option B]\u0026#34;, \u0026#34;C\u0026#34;: \u0026#34;[Matching attributes for option C]\u0026#34;, \u0026#34;Reasoning\u0026#34;: \u0026#34;\u0026lt;Brief justification\u0026gt;\u0026#34;, \u0026#34;Answer\u0026#34;: \u0026#34;\u0026lt;one of A, B, C\u0026gt;\u0026#34; } Any deviation from this format will be considered incorrect. Do not output any additional text. 输入：查询图像 + 候选concepts的文本描述 任务：模型需要列出查询图像与每个候选概念之间共同拥有的属性 输出：预测最佳匹配概念以及共享属性列表 这部分需要使用CoT\nCross-modal Attribute Verification\r这部分的主要目的是排查模型幻觉\n有些属性可能根本没有出现在图像中，造成了模型误判\n使用文本编码器对每个共享的指纹属性进行编码 依次计算与查询图像的相似度 计算得到相似度均值最高的concept（代表共享属性和对应图像最匹配） 如果该concept就是CoT推理得到的最优concept，那么就结束算法\n否则我们需要更加精确的CoT\nPairwise Reasoning\r只提供了文本描述看来是不够的，我们需要结合所有信息进行昂贵的推理\n保证这步确实完成任务\n转化为多个二分类任务：\n输入 查询图像 一个候选concept的原始图像+文本描述 输出 VLM 输出层中 \u0026ldquo;Yes\u0026rdquo; 和 \u0026ldquo;No\u0026rdquo; Token 的置信度 因此全过程先通过检索缩小范围，再通过CoT快速筛选，最后只在属性验证失败时才调用最重的配对推理\nRePIC: Reinforced Post-Training for Personalizing Multi ModalLanguageModels\rRePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models\nIntro\r在多概念图像描述任务中，SFT模型的效果一般不行，依赖大量高质量数据 同一对象的不同视觉图片可能差异较大，特别是姿势、位置、光照、背景变化 整合信息时可能遗漏和不准确 Method\r通过强化学习GRPO进行训练\n数据集采用Subject200K以及合成数据（扩散模型生成）\n提供了大量同一物体在不同光照、姿态下的多样化视觉特征\nOCT - Object Consistency Tuning\r构造正负图像对 正例：图像对包含相同对象 负例：图像对包含不同对象 模型回答：图像A是否包含图像B的同一个物体？ 正例需要回答Yes 负例需要回答No VLT - Visual Localization Tuning\r模型根据指令，预测指定物体的边界框 若IoU大于等于0.5，给出对应奖励 ICT - Identity Consistency Tuning\r要求模型生成一段描述，并且需要包含正确的个性化标签，如\u0026lt;name\u0026gt; 若场景中有m个，模型提到了n个，则获得n/m的奖励 为了避免模型通过生成 This is \u0026lt;name\u0026gt; 这种无意义的简短废话刷分\n要求模型至少输出100个token\nPreferThinker: Reasoning-based Personalized Image Preference Assessment\rPreferThinker: Reasoning-based Personalized Image Preference Assessment\nIntro\r任务：仅给少量“喜欢/不喜欢”参考图，判断候选图哪个更合用户口味 个性化数据极其稀缺 个人的喜好往往是多维度、复杂、难以描述的，无法用一个数值描述 Method\r个体偏好常涉及多维度视觉要素（如艺术风格、颜色、媒介、饱和度、细节等）\n远超“文本-图像对齐”或“美观度”等通用偏好所能刻画\n但是组成“复杂口味”的基本组成要素，多多少少是能够列举出来的、通用的\n要么喜欢“高饱和度”，要么喜欢“低饱和度”\npaper根据参考图像，构建用户画像\n喜欢：…… 讨厌：…… 按照画像进行评估与推理\nProfile\r从Lexica中提取prompt中频率最高、且对生成结果影响最大的视觉元素（15个） 找了 100 个受试者进行投票，让大家选出“最能代表个人偏好”的元素（缩小到5个） 结果：\n1 2 3 4 5 6 7 8 9 Art Style（艺术风格）： 比如 Anime, Realistic, Surrealism。 Color（颜色）： 比如 Pastel, Neon, Grayscale。 Art Medium（艺术媒介）： 比如 Oil Painting, Digital Art, Sketch。 Saturation（饱和度）： 比如 Vibrant, Muted。 Detail（细节度）： 比如 Intricate, Minimalist。 Profile将基于这五个维度进行描述，但问题的关键是如何用丰富的词汇描述准这五个维度\n针对每一个维度，都收集了大量的细粒度描述词 对于颜色，不能只是红、蓝……\n应该包含“Blush Pink”（腮红粉）、“Electric Lime”（电光绿）等具体词汇\nDataset\r模拟8w个用户 随机采样五个视觉偏好要素，为每个用户分配其视觉偏好画像与非偏好画像 真实用户可能同时具有多种偏好：为部分用户分配了多个偏好画像（2w） 基于profile，结合从 Lexica/COCO 等数据集里选的初始 Prompt（19w条） ，扔给 Text-to-Image进行图像生成 生成一组 参考图 (Reference Images)：代表用户的历史喜好 生成两张 候选图 (Candidate Images)：一张符合偏好，一张符合“厌恶偏好” 使用Claude3.7做先预测后评价\n输入（相当于直接给了答案） 参考图（喜欢的 \u0026amp; 不喜欢的） 两张候选图 输出：（需要CoT） 预测：基于参考图，把用户的画像（Art Style: xxx, Color: xxx）写出来 评估：对比两张候选图，在 5 个维度上分别进行解释和打分 作答：给出A和B哪个更喜欢 对数据进行清洗：\n偏好画像预测与真实标签明显不匹配的样本 结论与推理过程矛盾的样本 最后得到：60,000 名用户样本\nTraining……\rpass\n","date":"2025-12-13T21:20:00+08:00","permalink":"https://example.com/p/personalization-survey/","title":"Personalization Survey"},{"content":"Yo\u0026rsquo;LLaVA: Your Personalized Language and Vision Assistant | alphaXiv\n[TOC]\nIntro\rPersonalization\r用户有时候想问的不是我该给一只狗买什么生日礼物\n而是我该给我的狗买什么生日礼物\n模型能够识别、理解并谈论用户特定的概念或对象\n但是模型一般来说学的都是通用的内容，并不知道用户自身的setting\nYo\u0026rsquo;LLaVA\r给定少量（几张）特定对象的照片，模型学会将该对象嵌入为一组潜在 Token（Latent Tokens） 比单纯的文本提示更加到位 Contribution\r防止灾难性遗忘：Yo\u0026rsquo;LLaVA冻结了大部分的预训练参数，只针对一些特殊token 捕捉细粒度视觉细节：引入了困难负样本挖掘（Hard Negative Mining），即使用视觉上相似但并非该对象的图片进行训练，迫使模型学习更具辨别力的特征 Abstract\r读入：五张个性化subject的图片 构造对比学习：检索clip相似的若干图片和随机图片，进行recognition训练（带图） 构造对话文本数据：使用LLaVA对图片进行描述，生成通用对话文本，进行纯文本训练 输出：能够识别图片中的subject和直接描述subject的模型 Related Work\rSoft Prompt Tuning\rSoft Prompt Tuning 并不修改预训练模型的核心权重，而是通过在输入端引入一组可学习的Tokens来进行优化\nMethod\rYo\u0026rsquo;LLaVA的目标是只给若干张subjec的图片，能够做到\n通过视觉识别subject whether is in a photo or not 支持关于subject的VQA任务 ask about \u0026lt;sks\u0026gt;’s location 缺乏图片输入的情况下，在纯文本环境中回答subject的视觉特征 ask questions about intrinsic attributes of \u0026lt;sks\u0026gt; like its color, shape Personalizing the Subject as a Learnable Prompt\r为了识别图片中是否存在特定的subject\nnative的做法是：使用prompt详尽地描述subject的每一个细节，但要么很困难要么做不到\n需要引入Soft Prompt Tuning，对Prompt层进行学习\n1 2 \u0026lt;sks\u0026gt; is \u0026lt;token1\u0026gt;\u0026lt;token2\u0026gt;...\u0026lt;tokenk\u0026gt;. Can you recognize \u0026lt;sks\u0026gt; in this photo? 只需要增加几个新的token和对应输出权重 不需要修改MLMs的预训练权重 因此，对于参数的变化总结为：\n输出层增加的参数：最后输出的分类头参数矩阵由$C\\times N$扩展到$C\\times (N+1)$ $C$表示上一个hidden feature的维度 $N$表示词表维度 \u0026lt;token1\u0026gt;\u0026lt;token2\u0026gt;. . . \u0026lt;tokenk\u0026gt;Prompt层中的可学习token，以及\u0026lt;sks\u0026gt; 定义可训练的参数:\n$$\r\\theta = \\left\\{\\text{, ,...},W_{(:,N+1)}\\right\\}\r$$ 构造大量对话文本$(I,X_q,X_a)$，分别表示图片、提问、标准回答，进行训练\n从而让模型学习到新的concept（注意冻结其他参数）\nEnhancing Recognition with Hard Negative Mining\r对于识别问题，比较简单的训练方式就是询问subject是否在图片中\n显然不能全是正例，不然会训崩\npaper从LAION中抽样了100张图片作为负例，混合训练\n同时我们需要避免模型过度泛化\nsubject：yellow + dog\nmodel: any yellow animal\n为了增强训练，paper采用了hard negative mining的方法\n若subject是一只毛绒狗，则负例应该选择其他不是狗的毛绒动物 因此最后训练数据图片的组成为：\n100张随机抽取 100张通过CLIP计算相似度得到top-m张图（m=100） 正样本（5张左右） 然后paper构造了30条左右的prompt，以不同方式询问与回答：\u0026lt;sks\u0026gt;在图里吗？\nLearning to Engage in Natural Conversations about the Subject\r通过上述算法，模型掌握了识别的能力，但未必掌握了如何描述subject：Describe \u0026lt;sks\u0026gt; in detail.\n因此需要进一步训练，使得模型能够在对话中熟悉掌握subject的本质特性\npaper定义了10份通用对话模板，分为人类、物体两种（方便套用任何需要进行个性化训练的subject）\n避免过于细节的问题，例如：subject的尾巴是什么颜色的？\n显然不是所有物体都有这个特征\n对于每一张图像，使用LLaVA针对问题生成回答，完成数据集构建\n为了让模型真实地记住subject的特征，避免模型通过图像泄露得到答案\n这一阶段训练是纯文本的，只使用问题-答案对进行训练\nExperiment\rTrain\r对于单个subject，使用5张图像作为输入，软提示的视角token数量设定为16\n使用LLaVA-1.5-13B，进行单轮对话训练，GPU选择单卡A6000\nDataset\r数据集的初始构建由40个subject构成：Person (10), Pets (5), Landmarks (5), Objects (15), and Fiction Characters (5).\n每个subject含有10-20张图片，切分为训练集和测试集\nBaselines\rNaive LLaVA LLaVA + Prompt：通过提示词提供subject信息给LLaVA GPT-4V +Prompt +Images（GPT-4V支持多轮图像对话，因此可以提供多个subject的图片，单张图片1k token） 图片信息显著比文本提示更加丰富，因此GPT-4V+Images应当是性能的理论上界（提供了充足的描述）\n文本提示的获得方法：\n人工（约 16 token） 模型生成 为了方便对比，准备了两组文本提示词：\n拼接人工+模型生成（约1.3k的token） 模型再总结（约16 token） Results\rRecognition Ability\r40 个主体，每个主体有 5 到 10 张包含该对应主体的test图像 对于每个主体，其所有的测试图像均作为正样本，而来自其余 39 个类别的测试图像则作为负样本 总共有 333 个正向测试样本和 13,320 个负向测试样本 实验提供图像+问题Can you see if \u0026lt;sks\u0026gt; is in this photo? Answer with a single word or phrase.\nWeighted是正负样本准确率的均值\n过长的描述会对性能产生负面影响（即使用 1.3k 个 token 仅达到 0.650），可能描述了多余内容 给出的参考图像越多，性能越好 Question Answering\r准备了多道A或者B的选择题\n171道视觉（基于图片提问） 400纯文本 Comparison with MyVLM\rAblation Studies\r对Prompt的可学习token长度的消融：越长越好，为了均衡性能，选择16\n对subject传入的图像数的消融：越多越好，选择5\n数据集构建消融\n纯LLaVA： 识别能力随机 描述能力完全不行 构造识别数据：\u0026lt;sks\u0026gt;是否在图中？ 识别能力提升 描述能力完全不行 构造文本对话数据 识别能力略微提升 描述能力具备 构造Hard Negative样本 识别能力显著提升 描述能力具备 ","date":"2025-12-13T21:20:00+08:00","permalink":"https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/","title":"Yo LLaVA Your Personalized Language and Vision Assistant"},{"content":"[TOC]\nIntro\rGenerative recommendation基于AR LLM，面临两个问题：\nunidirectional constraint：无法在建模item语义时，捕获全局依赖 error accumulation：错误累积 补充一些前置知识：\nRQ-VAE（Residual Quantized Variational Autoencoder） 需要把Item表示成一个离散的token序列（Semantic ID），使用类LLM的生成方式进行推荐 RQ-VAE把Item的embedding压缩成离散token RA-VAE是多层级的，每一层会拟合一点，最后输出一个token序列 通过控制层数，控制token序列长度 将生成式推荐运用到dLMs上也面临若干问题：\nMismatch between Residual Quantization (RQ) and Discrete Diffusion 多层级的方案与dlms的并行不是很契合，且dlms序列中所有token同等重要 Beam Search is Not Directly Applicable to Discrete Diffusion Beam Search比较适合自回归的top k，但是固定从左到右 dlms是双向的 Differences between Language Modeling and Recommendation Contribution\nParallel Tokenization：设计了多头VQ-VAE，将物品切分为多个子向量，每个向量并行查找独立的codebook，最终生成ID Discrete Diffusion Training：使用两种mask机制 User-History Mask Next-Item Mask Discrete Diffusion Inference：适配了Beam Search Preliminaries\r推荐任务的问题定义：\n$$\r\\mathcal{H} = \\left [ i_1, i_2, ..., i_{n-1}\\right] $$ 基于用户历史的物品信息，预测下一个可能的物品$i_n$ 生成式推荐将单个物品表示为定长的若干个token\n$$\r\\mathcal{S_H} = \\left[c_{1,1},...,c_{1,M},...,c_{n-1,1},...,c_{n-1,M}\\right]\r$$ 我们需要找到一个最佳的模型$\\theta$，使得：\n$$\r\\theta^* = \\arg \\max_\\theta P_\\theta(s_n\\mid \\mathcal{S_H})\r$$ 这个转化为AR LLM的建模还是非常方便的\n对于dLM：\n$$ P_\\theta(s_n\\mid \\mathcal{S_H}) = \\prod_{t=1}^T\\prod_{m=1}^M\\begin{cases} P_\\theta(c_{n,m}\\mid s_n^t,\\mathcal{S_H}) \u0026amp; if \\space [MASK]\\ 1 \u0026amp; otherwise\n\\end{cases} $$\nMethod\rParallel Tokenization via Multi-Head VQ-VAE\r希望多个 token 之间是“完全平等”的，不应该存在 RQ-VAE 那种“前面的 token 更重要\nEmbedding通过Bert、Sentence-T5等得到$v_i$ 通过Encoder（MLP实现），得到潜在空间$z$ 将向量切成多个子向量，每个向量送入不同的Head 每个子向量查codebook，并行得到token(code index) 将code index对应的向量（code embeddings），进行拼接 Decoder重建$\\hat v_i$ 整个Tokenizer的损失由两部分组成：\n$$\rL_{Recon} = \\left \\| v_i-\\hat v_i \\right \\|^2_2\r$$ （L2范数的平方）\nEncoder：学习如何编码到合适的latent space Codebook：学习到如何覆盖latent space Decoder：重建 $$\rL_{VQ} = \\sum_{m=1}^M \\left ( \\left\\| sg[z_{i,m}] - e_{c_i,m}\\right\\|_2^2 + \\alpha \\left\\|z_{i,m}-sg[e_{c_{i,m}}]\\right\\|^2_2 \\right)\r$$ 对于第一项 sg代表阻止接受梯度，只有$e$会接受梯度 这样只更新codebook，使得codebook更接近latent space向量$z$（请靠近encoder的输出） 对于第二项 sg阻止codebook的梯度 更新encoder，贴近codebook 最终的损失：\n$$\r\\mathcal{L_{\\text{VQ-VAE}}} = \\mathcal{L_\\text{Recon}} + \\mathcal{L_\\text{VQ}}\r$$Discrete Diffusion Training\rUser-History Level Masking\r参考LLaDA的预训练\n让模型学会用户序列内部的关系 Next-Item Level Masking\r参考LLaDA的SFT\n理解同一 item 的 token 内部结构 但是LLaDA-Rec的训练没有分成两个阶段\n提出了一个联合损失函数：\n$$\rL_{total} = L_{Item-Mask} + \\lambda_{His-Mask}L_{His-mask} + \\lambda_{Reg}\\left\\|\\theta\\right\\|^2\r$$Discrete Diffusion Inference\r直接使用模型不太行，没法做到生成前k个推荐项目\n所以如何把Beam-Search嵌入到dlms中\n一开始我们会有一个全部都是[MASK]的序列，长度为$M$​：\n1 [MASK] [MASK] [MASK] ... [MASK] [MASK] dlm可以预测所有位置的生成token的置信度\n假设我们要迭代$T$次，因此$K = M/T$\n每次依照置信度选择前$K$个token位置，其他位置remask\n根据这些位置，以及超参数$B$，迭代出$B$条置信度最高的路径\n做beam search\n不断迭代\n","date":"2025-11-21T15:36:00+08:00","permalink":"https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/","title":"LLaDA-Rec - Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation"},{"content":"[TOC]\nTransformer\r[参考资料](https://datawhalechina.github.io/happy-llm/#/./chapter2/第二章 Transformer架构) 注意力机制\r对于一个token序列，注意力机制建立了三个键值：\nquery：表示想找什么特征 key：表示我有什么特征 value：信息正文 因此为了衡量query和key的特征之间的相似度，我们可以引入点积\n所有的query向量和所有的key做点积\n$$\rQK^\\top\r$$ 得到注意力矩阵，点积越大越相似\n同时为了防止数值爆炸（维数$d_k$越大，累加的数值会越多）\n做一下缩放：\n$$\r\\frac{QK^\\top}{\\sqrt{d_k}}\r$$ 然后通过softmax做一下归一化，确保所有权重之和为1，方便约束数值\n最后对应权重乘上对应的Value\n这样我们计算得到的是上下文文本内容的vector\n$$\r\\text{attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\r$$Self-Attention\r注意力机制处理了两个序列（Q、K来自不同序列）之间互相的查询\n自注意力理所当然是查询自己\n检查自己所说过的内容，确保逻辑一致 对于自注意力，每个Token都会有自己的Q、K、V\n自然三个矩阵的维度是匹配的（跟序列长度有关），(batch_size, seq_len, d_model)\n**Question：**如果你的输入序列有 4 个 token，那么计算 self-attention 的注意力矩阵 size 会是多少？\n**Answer：**每个token对应另外4个token的注意力分数，所以是(4,4)\nMask Self-Attention\r模型学习过程中，有时候会遮蔽一些token，以该机制阻止计算注意力\n最常用的就是通过Mask，遮蔽未来的信息，只允许模型利用历史信息\n如果待学习的文本序列是[BOS] I like you [EOS]\n1 2 3 4 5 \u0026lt;BOS\u0026gt; 【MASK】【MASK】【MASK】【MASK】 \u0026lt;BOS\u0026gt; I 【MASK】 【MASK】【MASK】 \u0026lt;BOS\u0026gt; I like 【MASK】【MASK】 \u0026lt;BOS\u0026gt; I like you 【MASK】 \u0026lt;BOS\u0026gt; I like you \u0026lt;/EOS\u0026gt; 模型能看到的内容就是未被MASK的token\nMASK是一个典型的上三角矩阵，因此我们可以创建一个上三角矩阵作为注意力掩码\n输入：(batch_size, seq_len, hidden_size) 注意力掩码：(1, seq_len, seq_len) batch中所有内容都可以通用（一般seq_len是一样的）\nMulti-Head Attention\r一次注意力计算只能拟合一种相关关系 代词关系、主谓关系、定语修饰……token之间的关系非常多余 我们希望能从多个维度去寻找注意力的相关，自然引入多头注意力机制\n同时对一个语料进行多次注意力计算，每次注意力计算都能拟合不同的关系 最后的多次结果拼接起来作为最后的输出 如图，不同的注意力查询，每个token所关注的其他token都不太一样\n假设文本的输入序列矩阵是$X$，对于第$i$个头\n$$\rQ_i = XW_i^Q, K_i=XW_i^K, V_i = XW_i^V\r$$ 其对应的注意力为：\n$$\r\\text{head}_i = \\text{attention}(Q_i, K_i,V_i)\r$$ 多头注意力表示为：\n$$\r\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_i)W^O\r$$","date":"2025-11-14T20:15:32+08:00","permalink":"https://example.com/p/happy-llm-part1-transformer/","title":"Happy LLM · Part1 · Transformer"},{"content":"Large Language Diffusion Models\n[TOC]\nIntro\rmemory-bound\rAR模型的Latency：每次predict next token，模型权重、KV Cache需要从显存中加载到GPU核心 瓶颈是显存带宽，GPU计算密度远远不够 因此单次decode越多的token，计算密度越高（在memory-bound的限制之下） 对于单次推理\nAR： $$\rx_{t+1}=F(x_{\u003c t};x_t)\r$$ dLMs： $$\rx_{t+1},x_{t+2},...,x_{t+k+1} = F(x_{\u003c t};M_{t+1},M_{t+2},...,M_{t+k+1})\r$$只要没有触发到compute-bound\n两者时间开销是一致的\n意味着多预测的部分$x_{t+2},\u0026hellip;,x_{t+k+1}$​是免费的，没有额外的时间开销\n论文将这部分多预测出来的部分（在不增加开销的情况下），命名为Free Token Slots\n什么是slots？\n模型的输出是多个token，其中需要计算kv的部分认为是slots\n剩余已经计算过kv的token，以下实验证明了影响不大\n因此paper主要聚焦于slots\n这里做了一个基于qwen3-32B的实验（Flash Attention 2、H100、batch size=1）\n三条线分别是prefix=1024、2048、4096（kv cache提前计算好）\n输入：token1 token2 ... token1023 token1024 [slots1] [slots2] ... [slotsn]\n输出：logit1 logit2 ... logit1023 logit1024 logit1 logit2 ... logitn 因此实际影响单次forward的是输入的slots的数量 Free Token Slots：该区间（1 - 100）延迟基本不增加 Cheap Token Slots：该区间延迟增加的不多，比较cheap 蓝色区间：触发了compute-bound independence\rAR是条件概率，按照从左到右依次生成 dLMs从加噪序列中decode所有的token 我们通过策略选择采样$k$个token进行保留，其他进行remask 而被采样的token应该是多个边缘分布的乘积\n$$\r\\prod_i p_\\theta^i(x^i\\mid x_t)\r$$ 这些token都是基于解码前的加噪序列得到的条件概率生成\n彼此可以看成独立的\n这里不太认可。生成的时候并不是模型独立生成每一个token，而是同时生成\n你要说完全独立没关系我觉得是不对的\n省流，从概率建模的角度解释dLMs的生成质量与$k$高度相关 $k=1$时，从左到右（因果掩码），退化成AR LLM $k$越小，质量越高 real-intro\rChallenge：生成模型的质量-并行问题\nAR LLM：生成质量高，但是苦于memory-bound无法提升性能\ndLMs：由于token间的独立性假设，生成质量受限于并发量\nContribution\n提出TiDAR架构，利用Free-token-slots，并行完成基于扩散的草稿和基于自回归的采样 提供完整训练方案，进行全面评估，证明架构的优势 进行详细的消融实验，验证核心设计。同时从扩散模型、投机采样角度进行分析 Method\r修改一下论文顺序，先写一下怎么推理\n为了利用free-token-slots，需要Diffusion和AR同时在一次forward中同时出现 Fully Parallelizable Self-Speculative Generation\r整个过程类似投机采样，整体的思想如下：\n首先由Diffusion Model生成一个draft 由AR进行rejection sampling 如果和AR的prediction一致，则保留 否则直接丢弃 In the each subsequent decoding step, draft tokens from the last step are rejectively sampled by checking whether they match the prediction from the autoregressive joint distribution computed at current step using causal attention.\n概述整个算法（定义block_len为3（可调整）），即我们会以三个token三个token为一组进行讨论\nStep0：Draft初始化 输入Prefix Tokens（实质上就是Prompt）和block size（这里是3）个MASK 为了兼容不同长度，同样调整了Attention Mask的顺序 (block_size + max_seq_len,block_size + max_seq_len) 直接按照真实长度切出来 Step1: 输入\nPrefix Token：ABC，表示为之前已有（被成功采样）的token序列，提前计算好KV-Cache Diffusion Draft：由Diffusion生成的草稿DEF Step2: 并行处理两件事（实质上是一个Transformer模型的单次前向推理）\nAR部分：基于Causal Attention，输出Diffusion Draft部分对应的logits（带有shift）\nDLMs部分：假设所有拒绝采样的结果，提前准备好对应的[MASK]让DLM独立预测（生成下一轮使用的草稿）\n因此我们本质上把ABC DEF [M][M][M] [M][M][M] [M][M][M]作为输入\n送入到TiDAR中，做了一次前向传播\n这里调整了Prefix的顺序，方便复用Attention Mask矩阵，新采样的token直接加到最后\n也就是DEF [M][M][M] [M][M][M] [M][M][M] ABC...\n注意这里只是Attention Mask的顺序，并不是输入序列\n对于第一组[M][M][M]：Attention能看见的是ABC D [M][M][M] 对于第二组[M][M][M]：Attention能看见的是ABC DE [M][M][M] 对于第三组[M][M][M]：Attention能看见的是ABC DEF [M][M][M] Step3: Sampling 根据AR部分的logits输出，决定上一轮草稿保留的内容（例子是保留DE，舍去F） 因此Prefix Token和KV-Cache会将DE加入，得到ABC DE 下一轮的草稿为第二组[M][M][M]的解码结果：F\u0026quot;G\u0026quot;H\u0026quot; Step4：输出 Prefix Token：ABC DE及其KV-Cache Diffusion Draft：由Diffusion生成的草稿F\u0026quot;G\u0026quot;H\u0026quot; 一些思考：\n显然block_len是不能无限大的，结合Figure1，通过单个Transformer架构的slots需要在Free-Token-Slots的范围内 paper只为AR采样1、2、3个token做了准备草稿，默认接受至少一个 Diffusion-AR Dual-mode Backbone Training\r论文其实是先讲的这一部分，但是先看完推理后再回来看train比较好\n前期工作Block Diffusion提出了一种块内双向注意力，块间因果掩码的方法\nTiDAR进行了修改：保留最后一个块（双向注意力），其他内容（或者叫前缀）全部因果掩码\n带来如下好处\n允许我们像AR一样计算链式的联合概率分布$p(x_i\\mid x_{\u0026lt;i})$，方便进行拒绝采样，保证高质量，并且计算似然和AR一样高效 在预训练和微调过程中可以计算前缀部分的NTP损失，损失信号的密度更高，充分利用数据中每一个token AR部分：shifted by one position dLM部分：一一对齐 TiDAR对于扩散部分的token，全部掩码为[MASK]，直接消除选择哪一种掩码策略的思考\n提高了扩散损失的密度（每一个token都参与） 平衡了AR和扩散的损失：强制两者参与损失计算的token数量都是相等的（序列长度） 先前框架的不平衡：dLM的信号容易被AR淹没 AR：稠密的（len-1个token参与） dLMs：损失取决于多少token被掩码（远少于len） 平衡的好处：更容易通过一个简单的超参数（加权因子）进行控制 允许在推理时一步扩散，避免多步迭代 对于输入序列，扩充相同的长度掩码ABC DEF -\u0026gt; MMM MMM 前一半序列的注意力就是Causal Attention 对于Diffusion部分（后一半），按照block size逐块考虑（采用双向注意力） 第一组MMM恢复目标是ABC，前缀为空 第二组MMM恢复目标是DEF，前缀为ABC 若后续还有别的组，前缀会继续累积 TiDAR的建模目标可以表示为：\n$$\r\\mathcal{L}_{TiDAR}(\\theta) = \\frac{1}{\\alpha+1}\\left( \\sum_{i=1}^{S-1}\\frac{\\alpha}{S-1}\\cdot\\mathcal{L}_{AR}(x_i,x_{i+1};\\theta)+ \\sum_{i=1}^{S-1}\\frac{1}{S-1}\\cdot \\mathcal{L}_{Diff}(\\left[mask\\right],x_i;\\theta)\\right )\r$$ $\\alpha \\in[0,1]$，损失函数的平衡项（paper里设为1） $\\left{x_i\\right}_S$是输入序列 AR和Diffusion都是做对应的交叉熵 Experiment\r（这里笔记忘记保存了，补一些重点）\n由于AR部分的logits是带label shift的，Diffusion部分没有\n实质上是一个模型，没有切割\n自回归部分根据ABCD会进行一个E的预测\nDiffusion部分的第一个Mask，也会根据attention得到相同的信息，预测E''\n所以这里涉及到了相信自回归还是相信Diffusion、或是兼顾的思考\n作者顺手做了一个实验\n$$\r\\text{logits}_{\\text{mix}} = \\beta*\\text{logits}_i^{\\text{AR}} + (1-\\beta)*\\text{logits}_i^{\\text{Diff}},i\\in|\\text{Vocab}|\r$$ 通过$\\beta$对齐了两个logits\n通过$\\alpha$控制loss比例，横轴是$\\beta$ 总体没有明显性能差距，因此质量来源是拒绝采样，而非AR比DLM好 Limitations\rBatch_Size：paper只做了1的情况（毕竟贴近推理，而不是吞吐量测试） 长上下文：翻了一倍文本，压力比较大 free token slots的探索：需要更加系统化的视角 cuda 调度 …… ","date":"2025-11-14T16:11:00+08:00","permalink":"https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/","title":"TiDAR - Think in Diffusion, Talk in Autoregression"},{"content":"[TOC]\nContent\r2511.06254 LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation\nnote：笔记 abstract：生成式推荐与离散扩散模型 2510.10481 UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models\nnote：笔记 abstract：扩充LLaDA的长上下文 ICLR 2026：6666 2509.24389 LLaDA-MoE: A Sparse MoE Diffusion Language Model\nnote：笔记 abstract：修改了LLaDA的Dense Transformer为MoE架构 2509.13866 Masked Diffusion Models as Energy Minimization\n如何以理论可解释的方式为离散域的掩码扩散模型（MDM）设计最优采样调度 2505.19223 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models\nnote：笔记（难度较高，未读完） abstract：提出VRPO方法，对LLaDA进行强化学习 ICLR 2026：642 2505.16933 LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning\nnote：笔记 abstract：实现了图片理解的多模态LLaDA模型 2502.09992 Large Language Diffusion Models\nnote：笔记 abstract：LLaDA ","date":"2025-11-13T12:56:00+08:00","permalink":"https://example.com/p/awesome-llada/","title":"Awesome LLaDA"},{"content":"Large Language Diffusion Models\n[TOC]\nIntro\rLLaDA-MoE：激活1.4B参数的情况下，超过先前8B的DLMs性能，取得DLMs的SOTA，与Qwen2.5-3B-Instruct性能相当 LLaDA-MoE\rArchitecture\rRMSNorm、SwiGLU、RoPE、QK-LayerNorm Train\rPretrain Stage 1：从头开始训练，10T Pretrain Stage 2：从相同的底层数据重新采样10T（提高数学、代码的权重），继续训练 Annealing Stage 1：从Pretrain Stage 2中最好的checkpoint开始，训练500B的高质量文本 Annealing Stage 2：将RoPE的base从10000提高到50000（扩充4k到8k的上下文），500B SFT Annealing：用更高质量的数据让模型“收敛得更好”\n训练阶段（预训练+SFT）的损失函数同LLaDA 预训练1%是随机长度，99%是4096定长（同LLaDA） MoE Routing\r$$\rp_t = \\text{Softmax}(\\text{Router}(h_t))\\\\\ro_t = \\sum_i p_{t,i}E_i(h_t), \\quad \\text{where }p_{t,i} \\in\\text{Topk}(p_t) $$ $h_t$是hidden state $E$是专家网络 MoE选取p最大的k个专家网络进行加权\n为了平衡负载，采用了标准的MoE auxiliary loss\nLoad Balancing Loss\n$P_i$：token级的专家$i$被分配的平均概率 $f_i$：经过所有token每个专家被选中的频率 $N$​：专家数 $$\r\\mathcal{L}_{LB} = N\\sum_{i=1}^N f_iP_i\r$$通过该损失避免某个专家被频繁选中\nZ-Loss $z_t$表示$\\text{Router}(h_t)$ $z_{t,j}$即为第$j$个专家的打分 $$\r\\mathcal{L}_Z=\\frac{1}{T}\\sum_{i=1}^T\\left(\\log \\sum_{j=1}^N e^{z_{t,j}}\\right)^2\r$$通过该损失抑制logits分布，防止softmax极端化\nLLaDA-MoE为LB设置0.01权重，为Z-Loss设定0.001\nExperiments\r","date":"2025-11-11T13:54:00+08:00","permalink":"https://example.com/p/llada-moe-asparse-moediffusion-language-model/","title":"LLaDA-MoE ASparse MoEDiffusion Language Model"},{"content":"Large Language Diffusion Models\n[TOC]\nIntro\r现象\r扩散语言模型的Local perception LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs\nLongLLaDA · 大海捞针实验\nLongLLaDA观测到这个现象是RoPE+双向上下文带来的 提出了一种Training Free的方法，调整了RoPE机制，提升上下文能力 关键挑战\r如何将AR LLM的上下文扩展技术（不重新预训练）迁移到DLM training-free在AR方面证明了效果不如post-training，是否对DLM也是一致的 希望模型通过后训练，调整内部机制 核心贡献\ra Diffusion-aware NTK method 无需从头训练 受神经切线核（Neural Tangent Kernel, NTK）理论启发，开发了一个适配DLM的NTK方法 能适应扩散模型的迭代去噪特性，使 RoPE 可以稳定外推到 128K tokens 比较了后训练过程中使用的mask策略，分析对优化稳定性和长程回忆的影响 UltraLLaDA，与LongLLaDA、LLaDA进行benchmark，证明是SOTA Preliminary Work\rRoPE：通过旋转向量的方式引入位置信息，旋转角度与位置index是线性关系\n为了增加上下文，RoPE需要外推到更远的Token\n外插：直接应用到更大的index，但是会造成信号失真与混乱（模型没见过这么大的） 内插：等比例缩放index到小窗口内，但是会比较模糊 $$\r\\cos(k\\alpha^{\\frac{-2i}{d}})\\\\\r\\cos(\\frac{k}{\\lambda}\\alpha^{\\frac{-2i}{d}})\r$$序列位置$k$的编码向量（维度为$d$）的第$i$个分量\nNTK-Aware Scaling 定义如下：\n$$\r\\lambda = \\left(\\frac{T_{target}}{T_{train}}\\right)^{\\frac{d}{d-2}}\r$$ 将原本的底数$\\alpha$转化为$\\alpha\\lambda$\n$$\r\\cos(k(\\alpha\\lambda)^{\\frac{-2i}{d}}) = \\cos(k\\alpha^{\\frac{-2i}{d}}\\left(\\frac{T_{target}}{T_{train}}\\right)^{\\frac{-2i}{d-2}})\r$$ 低维度：$i$比较小，频率高，$\\frac{-2i}{d-2}$接近1，式子近似为$\\cos(k\\alpha ^ {\\frac{-2i}{d}})$ 高维度：$i$比较大，频率低，$\\frac{-2i}{d-2}$接近-1，式子近似为$\\cos(\\frac{k}{\\lambda}\\alpha^{\\frac{-2i}{d}})$ 省流：高频部分外插，低频部分内插\n上面都是定性的理解\n正确的公式实际是：\n$$\r\\lambda_{baseline} = b^{-1}\\cdot\\left ( \\frac{T_{target}}{2\\pi} \\right)^{\\frac{d}{d_{crit}}}, d_{crit} = 2\\left \\lceil \\frac{d}{2}\\log_b \\frac{T_{train}}{2\\pi} \\right \\rceil\r$$ b通常是10000\n输入：$T_{target}, T_{train}$ 输出：$\\lambda$ LongLLaDA将此方法从AR LLM迁移到DLM，并且不进行后训练\nMethod\r从最开始的现象已经说明，RoPE + 双向上下文是不可或缺的\n但是LongLLaDA没有任何关于双向上下文的适配，潜力没有被开发完全\nDiffusion-aware NTK in UltraLLaDA\rAR和DLM能看见的上下文窗口是不一样的\nAR LLM：$[-T_{train}-1,0]$ DLM：$[-(T_{train}-1), T_{train}-1]$ DLM的真实上下文信息应该是2倍，因此需要对NTK的输入进行修正：\n输入：$T_{Ecap} \\approx 2T_{target}, T_{cap} \\approx 2T_{train}$ 带来更小的频率，该缩放机制增加了所有维度上的RoPE周期\n从而有效减缓 RoPE 旋转速度，并延长所有注意力维度上的位置波长\nCase Study of Masking for Diffusion LLM Context Extension\r后训练数据准备\n来源：PG19 处理：短文档通过拼接到达64k，长文档切割成64k的chunk 存在问题：跨文档干扰，跨越文档边界进行注意力计算，错误吸收上下文信息\nAR：由于Causal Mask，只能看见之前的文档，天然限制了部分干扰 DLM：能看见所有的文档，干扰非常强 处理策略（idea来自AR LLM）\nbaseline：直接拼接，什么都不做 Adaptive Attention Masking：只计算文档内部的注意力 End-of-document：文档之间插入special token（并没有显式地禁止注意力跨文档），采用Full Attention 对三种策略都进行了训练（结合前文所提NTK）\n训练参数：\n对特定任务采用不同策略训练的UltraLLaDA模型 后续的实验证明：\n采用直接拼接后训练的模型常产生不连贯结果，这可能是由于无关内容相互渗透所致 Experiments\rTrain-Free NTK\r目的：在Train-Free的情况下，修正NTK的上下文长度参数输入的作用 方法：未做后训练，只修改编码方式进行测试 结论：引入双向覆盖对于扩展DLM的上下文长度至关重要 NIAH\rNeedle-in-a-haystack long-context retrieval task\n该任务将单个相关语句嵌入长达 128K 标记的干扰文本中，要求模型准确检索目标语句\n全部100%检索成功 由于模型缺陷，LongLLaDA 无法进行 32K 以上的评估\n结论：后训练方法即使在极长上下文（128K）中仍能保持卓越的检索能力，而Train-Free会随上下文长度增加快速失效 PPL\r基于PG19中128K长度的文档的语言建模困惑度评估 结论：UltraLLaDA的训练在超长序列建模中具有很强的鲁棒性 LongBench\r截断在16K上下文长度（大部分任务只有这么长） 单/多文档问答、摘要、上下文学习、合成推理任务、代码补全 结论：长上下文训练不仅扩展了上下文长度，即使在 16K 范围内（基线模型能力范围内）也能在挑战性任务上实现质量增益 归因于后训练过程带来的长距离连贯性与理解能力的提升 RULER\r在4K至32K上下文长度下（涵盖检索、聚合、问答及多跳变量追踪任务） 在检索（NIAH）和追踪（VT）类别中均展现出强劲的扩展性 在聚合（AGG）及部分问答任务（QA）上的提升相对有限 消融实验\r针对NTK和跨文档策略进行消融实验 EOD 拼接策略在较短或中等长度下表现更优 在更长序列中，自适应掩码策略会反超EOD拼接 ","date":"2025-11-11T13:54:00+08:00","permalink":"https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/","title":"UltraLLaDA Scaling the Context Length to 128K for Diffusion Large Language Models"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\n以计算采样数据的平均数为例\n采样大量数据，最后一次性计算平均值 采样一个算一次，迭代计算，手上的均值最终会趋向真实 我们采样第二种方法，记：\n$$\rw_{k+1} = \\frac{1}{k}\\sum_{i=1}^k x_i\r$$ 则：\n$$\r\\begin{split}\rw_{k+1} \u0026= \\frac{1}{k}\\sum_{i=1}^k x_i = \\frac{1}{k}(\\sum_{i=1}^{k-1}x_i+x_k)\\\\\r\u0026= \\frac{1}{k}\\left [ (k-1)w_k + x_k \\right ] = w_k - \\frac{1}{k}(w_k-x_k)\r\\end{split}\r$$Robbins-Monro Algorithm\r对于一个方程：\n$$\rg(w) = 0\r$$ 我们希望求出解$w^$，使得$g(w^) = 0$\n为了简化问题，$g(w)$是一个单调递增的函数\n如果函数已知，我们可以通过牛顿迭代法求解\n但是如果函数未知，我们通过采样：\n$$\ry_k = g(w_k)\r$$ 往往带有噪声：\n$$\r\\widetilde{g}(w_k,\\eta_k) = g(w_k) + \\eta_k\r$$ 我们定义一个迭代过程：\n$$\rw_{k+1} = w_k - a_k\\widetilde{g}(w_k,\\eta_k)\r$$ 当$w_k\u0026gt;w^*$时，$g(w_k)\u0026gt;0$时 $w_{k+1} = w_k - a_k\\widetilde{g}(w_k,\\eta_k) \u0026lt; w_k$ 所以会往$w^*$走一步 当$w_k\u0026lt;w^*$时，$g(w_k)\u0026lt;0$​时 $w_{k+1} = w_k - a_k\\widetilde{g}(w_k,\\eta_k) \u0026gt; w_k$ 所以会往$w^*$走一步 但其实这个非常理想，条件也比较苛刻，需要满足\n$0\u0026lt;c_1\\leq \\nabla g(w)\\leq c_2,\\text{ for all } w$\n$c_1$保证函数单调递增，且必然存在0解 $c_2$​保证梯度有界，防止垂直 $\\sum a_k = \\infty, \\sum a_k^2 \u0026lt; \\infty$​\n促进探索：$a_k$​是步长，若不趋近无穷，代表调整幅度与范围是有限的\n抑制噪声：噪声的方差大致是$\\sum a_k^2 \\mathbb{E}\\left| \\eta_k^2\\right |$，$\\sum a_k^2 \u0026lt; \\infty$​保证方差不会发散\n同时暗含了$a_k\\to 0$，使得最后的$w^*$趋于定值 $\\mathbb{E}(\\eta_k|\\mathcal{H}_k) =0, \\text{ and }\\mathbb{E}(\\eta_k^2|\\mathcal{H}_k \u0026lt; \\infty)$\n噪声条件期望为0，可以保证噪声是不依赖于历史的（不会随着采样步数增加而增加/减少，即不存在系统性偏移） 条件方差不会无穷大，从而避免出现极端大的扰动，保证理论分析（如大数定律、中心极限定理）可用 懒得纠结太多了\nStochastic Gradient Descent\r随机梯度下降的目标是：\n$$\r\\min_w \\mathcal{J}(w) = \\mathbb{E}\\left[f(w,X)\\right]\r$$ 是一个含有随机变量的式子，需要优化一个参数$w$\nMethod1：Gradient Descent，沿梯度的反方向走一个步长 $$\rw_{k+1} = w_k -\\alpha_k \\nabla_w \\mathbb{E}[f(w_k,X)]\r$$ 但是实际上我们很难获得所有样本分布$X$（数据是收集不完的）\n所以这是一个理想的情况\nMethod2：Batch Gradient Descent 考虑使用一个Batch进行近似 $$\r\\mathbb{E}[\\nabla_wf(w_k,X)] \\approx \\frac{1}{n}\\sum_i^n\\nabla_w f(w_k,x_i)\\\\\rw_{k+1} = w_k - \\alpha_k \\frac{1}{n}\\sum_i^n\\nabla_w f(w_k,x_i)\r$$ 但是每次迭代，都需把每一个Batch进行扫描，并不是很轻松\nMethod3：Stochastic Gradient Descent 其实就是batch变成1了 $$\rw_{k+1} = w_k - \\alpha_k \\nabla_w f(w_k,x_i)\r$$ 为了证明SGD算法是收敛的，我们可以尝试构造一个方程：\n$$\rg(w) = \\mathbb{E}\\left[\\nabla_w f(w,X)\\right] = 0\r$$ 我们使用RM算法求解，则有：\n$$\r\\widetilde{g}(w,\\eta) = g(w) + \\eta = \\mathbb{E}\\left[\\nabla_w f(w,X)\\right] + \\eta\r$$ 则有迭代式：\n$$\rw_{k+1} = w_k - a_k\\widetilde{g}(w_k,\\eta_k) = w_k - a_k\\nabla_wf(w_k,x_k) $$ 所以事实上SGD就是一个RM\n截止到此，后续内容我应该不会继续深入，理论性比较强\n之后如果有涉猎到的话再进行学习\n","date":"2025-11-04T22:53:00+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap6-stochastic-approximation-and-sgd/","title":"强化学习的数学原理 · Chap6 · Stochastic Approximation and SGD"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\nModel\rmodel-based：概率模型已知 model-free：概率模型未知 抛硬币模型：\nmodel-based: 基于0.5的概率直接把期望、分布全部都算出来 model-free: 重复采样多次，以样本均值作为期望 MC Basic\r最简单的基于蒙特卡洛的强化学习算法\n本质上是转化Policy Iteration为Model-Free 其中Policy Iteration中的一步：\n$$\r\\pi_{k+1}=\\arg \\max_\\pi \\sum_a \\pi(a|s){\\color{red}q_{\\pi_k}(s,a)}, \\quad s\\in \\mathcal{S}\r$$ 其中这一步的action value可以表示为：\n$$\r\\begin{split}\rq_{\\pi_k}(s,a) \u0026=\\sum_{r}p(r|s,a)r+\\gamma\\sum_{s'}p(s'|s,a)v_{\\pi_k}(s') \\\\ \u0026= \\mathbb{E}\\left[ G_t \\mid S_t = s, A_t = a\\right]\r\\end{split}\r$$ 第一个公式是依赖于模型$p$的，而第二个公式就可以通过采样去近似\n基于状态$s$采取动作$a$，依据策略$\\pi_k$，生成一个episode，记return为$g(s,a)$\n则我们可以将$g(s,a)$看作一个$G_t$的采样\n因此我们大量重复采样，得到序列$\\left { g^{(j)}(s,a)\\right }$\n则可以近似：\n$$\rq_{\\pi_k}(s,a) = \\mathbb{E}\\left [ G_t\\mid S_t = s,A_t = a \\right] \\approx\\frac{1}{N}\\sum_jg^{(j)}(s,a)\r$$ 没有模型的时候，你最好有数据（统计学叫Sample，强化学习叫Experience）\n因此整个算法的流程：\n从$\\pi_0$出发，迭代到$k$\nStep1：Policy Evaluation 本质上就是希望得到所有的$q$ 对每一个$(s,a)$对，生成大量的episodes 计算均值，作为$q_{\\pi_k}(s,a)$ Step2：Policy Improvement 贪心策略：$\\pi_{k+1}$基于最大的$q$去选择 这里和model-based是一样的 因此我们可以直接通过这个方法得到$\\pi$，而不是state value\n但是算法效率比较差 后续有优化\n甚至这个算法也只是一个idea 没有具体的名字\n是作者取的\nEposide Length是重要的、需要够长：采样的步数太短，会导致均值不够准确 MC Exploring Starts\rMC Basic的数据采样效率非常低\n需要对每一个$(s,a)$对进行大量采样\n$$\rs_1,a_1 \\to s_2,a_5 \\to {\\color{red}s_1,a_2} \\to s_2,a_3 \\to a_5,a_1 \\to ...\r$$ 在MC Basic中，这一条episode只会用来计算最开始的$s_1,a_1$​的action value\n但其实中间的$s,a$对是可以当成一次采样的\n我们可以从两个角度开始优化：\n复用：我们希望一整条episode都能被利用，有两个方法 First Visit：只采样每个episode中每个$(s,a)$​第一次出现 样本独立 Every Visit：每次出现$(s,a)$​都当成一次采样 样本更多，但是相关性会变强 即时更新 Basic：跑完所有的episode，统一做一次Policy Improvement 但可以用单个 episode的return来立即更新action value 本质上可以看作是Truncated Policy Improvement 算法流程：\n初始化策略$\\pi_0$​\n“Exploring starts” 意思是：所有 (s, a) 都有可能成为 episode 的起点。\n随机选择一个起点$s_0,a_0$​​——Exploring Starts 需要保证所有的状态都是非0的概率能作为起点 按照当前策略$\\pi$生成一个episode 反向更新（相当于后缀和，方便计算return） 每个$s,a$​会维护一个列表 First Visit：如果当前$s,a$对是$(s_0,a_0,s_1,a_1,s_2,a_2,\u0026hellip;)$中没有出现过的（第一次出现） 将当前的return加入到对应列表 计算列表中的均值，更新$q(s,a)$ 根据$q$实时更新$\\pi$ MC Epsilon Greedy\r但现实中Exploring Starts难以实现，有时候我们很难自由选择一个起点开始采样\n你总不能每次采样一个起点，就搬动机器人过去一次吧\n前两个算法之所以要对不同的$s,a$做采样，本质原因是：\n$$\r\\pi(a|s) = \\begin{cases}\r1 \u0026 \\text{ if } a=a^*(s) \\\\\r0 \u0026 \\text{ if } a\\neq a^*(s)\r\\end{cases}\r$$ 我们的策略是greedy的，因此会导致从真实起点出发，非常多的状态无法被覆盖到\n因此无法针对单一起点做反复采样\n所以我们可以考虑引入一点随机性：\n$$\r\\pi(a|s) = \\begin{cases}\r1-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1) \u0026 \\text{ for the greedy action }\\\\\r\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \u0026 \\text{ for the other }|\\mathcal{A}(s)|-1 \\text{ actions} \\end{cases} \\\\\r\\text{where } \\varepsilon \\in [0,1] \\text{ and } |\\mathcal{A}(s)| \\text{ is the number of actions.}\r$$ 假设有5个action，其中greedy action是$a_0$，定义$\\varepsilon=0.2$\naction $a_0$ $a_1$ $a_2$ $a_3$ $a_4$ probability 0.84 0.04 0.04 0.04 0.04 同时，我们可以保证任意时刻greedy action的概率都是最大的\n$\\varepsilon=1$时，所有action概率相同\n$$\r1-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1)=1-\\varepsilon + \\frac{\\varepsilon}{|\\mathcal{A}(s)|} \\geq \\frac{\\varepsilon}{|\\mathcal{A}(s)|}\r$$算法维持了一个平衡：\n探索性：越大的$\\varepsilon$带来越强的探索性，只要episode足够长，就能探索所有状态 最优性：越大的$\\varepsilon$带来的策略自然是更差的，因为每一步走向greedy的概率变低了 可以动态调整，一开始大，逐渐变小\n算法流程：\n去掉了Exploring Starts的条件：需要保证所有的状态都是非0的概率能作为起点 修改策略更新的分布 $$\r\\pi(a|s) = \\begin{cases}\r1-\\frac{\\varepsilon}{|\\mathcal{A}(s)|}(|\\mathcal{A}(s)|-1) \u0026 a=a^*(s)\\\\\r\\frac{\\varepsilon}{|\\mathcal{A}(s)|} \u0026 a\\neq a^*(s)\r\\end{cases} \\\\\r$$ 其他都是和前文算法一致\n","date":"2025-11-03T01:10:34+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap5-monte-carlo-learning/","title":"强化学习的数学原理 · Chap5 · Monte Carlo Learning"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\nValue Iteration\r基于BOE：\n$$\rv = f(v) = \\max_\\pi(r_\\pi+\\gamma P_\\pi v)\r$$ 我们不断做迭代：\n$$\rv_{k+1} = f(v_k), k=1,2,3,...\r$$ 这个就是value iteration\n整个算法可以拆成以下步骤：\nStep1：Policy Update $$\r\\pi_{k+1} = \\arg\\max_\\pi (r_\\pi+\\gamma P_\\pi v_k)\r$$ 基于上一步的state value，找出当前最优的策略\nStep2：Value Update $$\rv_{k+1} = r_{\\pi_{k+1}} + \\gamma P_{\\pi_{k+1}}v_k\r$$基于该策略，更新所有的state value\n理论上$v_k$不能被称为state value\n由于所有的state都是在优化过程，因此不能保证所有state都符合贝尔曼公式\n因此只是一个临时状态\n从代码实现的角度：\nStep0：若$\\left| v_k - v_{k-1}\\right | \u0026gt; \\text{eps}$ 否则结束，说明收敛 对所有的state，计算出所有的$q(s,a)$ 得到$a^*_k(s,a) = \\arg \\max_a q_k(s,a)$ Step1： $$\r\\pi_{k+1}(a|s) = \\begin{cases}\r1 \u0026 \\text{ if } a=a^*_k(s) \\\\\r0 \u0026 \\text{ if } a\\neq a^*_k(s)\r\\end{cases}\r$$ 这是一个贪心的策略\nStep2： $$\rv_{k+1}(s) = \\max_a q_k(s,a)\r$$ Value Iteration从当前的state value出发，决定了下一个Policy\n使用新的Policy去更新state value\nPolicy Iteration\r与之相比，还有一种迭代方式是以Policy作为基础\n与Value Iteration不同，我们的出发点是一个初始策略$\\pi_0$\n进行如下迭代：\nStep1：Policy Evaluation 计算基于当前策略$\\pi_k$的state value $$\rv_{\\pi_k} = r_{\\pi_k} + \\gamma P_{\\pi_k} v_{\\pi_k}\r$$ 这里的求解依赖于一个迭代的过程\n因此整个Policy Iteration，内含一个小的迭代\nStep2：Policy Improvement 基于当前state value，找出最优策略 $$\r\\pi_{k+1} = \\arg \\max_\\pi (r_\\pi + \\gamma P_\\pi v_{\\pi_k})\r$$ 从代码实现的角度\nStep1：$v_{\\pi_k}$基于策略$\\pi_k$不断迭代，直到$v_{\\pi_k}$收敛 Step2：基于$v_{\\pi_k}$算出所有的$q_{\\pi_k}$ 得到$a^*_k(s,a) = \\arg \\max_a q_k(s,a)$​ 同样做以下事情： $$\r\\pi_{k+1}(a|s) = \\begin{cases}\r1 \u0026 \\text{ if } a=a^*_k(s) \\\\\r0 \u0026 \\text{ if } a\\neq a^*_k(s)\r\\end{cases}\r$$ 策略迭代会出现一个现象：接近终点的策略会先变好\n因为一开始都是乱七八糟的，接近终点的策略会更先找到方向\nTruncated Policy Iteration\rpolicy：基于$\\pi$进行policy evaluation，得到$v_\\pi$，再policy improvement得到新的$\\pi$ value：基于$v$做policy update更新得到当前最优$\\pi$，根据$\\pi$做value update得到新的$v$ 对两个算法进行对齐\n前面说了：Policy Iteration是一个大的迭代包含一个小的迭代过程\n而value iteration的第一次迭代得到的$v_1$，事实上就是Policy iteration小迭代中的第一个中间量\n我们显然不会在这里进行无穷次的迭代\n所以这个迭代次数就可以一般化成Truncated Policy Iteration（截断策略迭代）\n当迭代次数为：\n1次：value iteration 无穷次：policy iteration（因此这个算法不存在） 所以Truncated Policy Iteration是两个算法的一般化形式\n","date":"2025-10-31T14:05:34+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap4-value-iteration-and-policy-iteration/","title":"强化学习的数学原理 · Chap4 · Value Iteration and Policy Iteration"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\n笔记丢失了一次\n这是补档版本，稍微粗略一点……心态小炸\nOptimal Policy\r当一个策略，在任意状态都比另一个策略的state value更好（或者相等）\n我们就认为这个策略优于另一个策略\n当一个策略比所有策略都好，它就是最优策略\nBellman Optimality Equation\r$$\r\\begin{split}\rv(s) \u0026= {\\color{red}\\max_\\pi} \\sum_a \\pi(a|s) \\left( \\sum_r p(r|s,a)r + \\gamma \\sum_{s'} p(s'|s,a)v(s')\\right), \\quad \\forall s\\in \\mathcal{S} \\\\\r\u0026= {\\color{red}\\max_\\pi} \\sum_a \\pi(a|s) q(s,a), \\quad \\forall s\\in \\mathcal{S}\r\\end{split}\r$$ 相比于贝尔曼公式，BOE实质上就是多了一个优化问题\n已知：$p(r|s,a),p(s\u0026rsquo;|s,a)$ 未知：$v(s),v(s\u0026rsquo;)$，需要我们针对不同的$\\pi$​去计算 求解：$\\pi$ 自然有向量形式：\n$$\rv = \\max_\\pi (r_\\pi + \\gamma P_\\pi v)\r$$Maximization\r我们先考虑固定$v$，看看能不能求解$\\pi$​\n$$\rv = \\max_\\pi\\sum_a \\pi(a|s)q(s,a)\r$$ 其中$q(s,a)$自然也是一个已知的量（所有的$v$都可以知道，自然可以算出$q$）\n同时有：\n$$\r\\sum_a \\pi(a|s) = 1\r$$ 那么本质上$\\sum_a \\pi(a|s)q(s,a)$是对action value做加权平均\n为了最大化这一项，我们肯定需要给最大的$q$最多的权值，也就是1\n可以得到策略：选择action最大的行动$a^*$\n$$\r\\pi(a|s) = \\begin{cases}\r1 \u0026 \\text{ if } a= a^*\\\\\r0 \u0026 \\text{ if } a\\neq a^*\r\\end{cases}\r$$ 这样我们就能得到原式：\n$$\rv = \\max_\\pi\\sum_a \\pi(a|s)q(s,a) = \\max_{a\\in \\mathcal{A}(s)}q(s,a) \\\\\r\\text{where }a^* = \\arg \\max_a q(s,a)\r$$ 只要我们固定$v$，我们肯定就能得到最优策略，自然计算出右边的值\n因此右边的值可以直接表示成一个只关于$v$的函数：$v=f(v)$\n表达成向量形式：\n$$\rf(v):=\\max_\\pi (r_\\pi+\\gamma P_\\pi v)\r$$ 搜索一下不动点定理或叫Contraction mapping theorem\n基于这个定理，可以证明$f(v)$是符合这个定理的条件的\n大概就是：\n函数具有压缩性：有$\\left|v_1-v_2\\right | \\leq \\gamma\\left|f(v_1)-f(v_2)\\right |$ 也就是满足这个条件，就会有\n必然存在唯一的一个不动点$f(v) =v$ 因此，我们可以不断套用$v,f(v),f(f(v)),\u0026hellip;$\n最后会收敛得到不动点，具体证明略过\n实际上的操作就是：\n$$\rv_{k+1}=f(v_k) =\\max_\\pi (r_\\pi+\\gamma P_\\pi v_k)\r$$ 那么如何证明最后收敛到的结果$v^*$就是最优解呢？\n固定$v=v^$，自然可以得到当前的策略$\\pi^$\n$$\r\\pi^* = \\arg \\max_\\pi (r_\\pi+\\gamma P_\\pi v^*)\r$$ 将$\\pi^*$代入：\n$$\rv^* = \\max_\\pi (r_\\pi+\\gamma P_\\pi v_*) = r_{\\pi^*}+\\gamma P_{\\pi^*} v^*\r$$ 你会发现变成state value的公式了\n也就是说$v^$，就是策略$\\pi^$​的state value\n我们考虑策略替换成任意其他的策略$\\pi$时：\n$$\rv^* = r_{\\pi^*}+\\gamma P_{\\pi^*} v^* \\geq r_\\pi + \\gamma P_\\pi v^*\r$$ 令该策略对应state value的贝尔曼公式：\n$$\rv = r_{\\pi}+\\gamma P_{\\pi} v\r$$ 做一个减法则有：\n$$\rv^*-v \\geq (r_\\pi + \\gamma P_\\pi v^*)-(r_{\\pi}+\\gamma P_{\\pi} v) = \\gamma P_\\pi(v^*-v)\r$$令$\\Delta = v^*-v$，则有：\n$$\r\\Delta \\geq \\gamma P_\\pi\\Delta\r$$ 我们只需要把右边的$\\Delta$不停代换为$\\gamma P_\\pi\\Delta$：\n$$\r\\Delta \\geq \\gamma P_\\pi\\Delta \\geq \\gamma^2 P_\\pi^2\\Delta \\geq ... \\geq \\gamma^n P_\\pi^n\\Delta \\geq 0\r$$ 故对于策略$\\pi^*$，其state value始终是最大的\n因此是最优策略\nAnalysis\rChange $r\\to ar+b$会发生什么？ 什么都不会发生 可以证明 $v\u0026rsquo; = av^*+\\frac{b}{1-\\gamma}I$，所有state value的相对大小没有发生变化 自然action value不会有其他选择，policy不改变 如何鼓励走最短路径？ 其实不用一直给agent做-1（损失能量），催促agent $\\gamma$本身就在催促agent尽快到终点，否则终点的贡献变少了 ","date":"2025-10-30T23:38:34+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap3-bellman-optimality-equation/","title":"强化学习的数学原理 · Chap3 · Bellman Optimality Equation"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\nState Value\rreturn 用来衡量不同的trajectory的好坏\n首先有如下定义\n$$\rS_t \\overset{A_t}{\\rightarrow}R_{t+1}, S_{t+1}\r$$ $t$表示时间步 从状态$S_t$出发，采取$A_t$，转移到$S_{t+1}$，获得了$R_{t+1}$的奖励 $S,A,R$​这里表示的是一个随机变量\n因此上述内容服从以下概率分布：\n$S_t \\to A_t$ ：$\\pi(A_t| S_t)$ $S_t, A_t \\to R_{t+1}$：$p(R_{t+1}| S_t,A_t)$ $S_t, A_t \\to S_{t+1}$：$p(S_{t+1}| S_t,A_t)$ 推广到多步骤之后：\n$$\rS_t \\overset{A_t}{\\rightarrow}R_{t+1}, S_{t+1} \\overset{A_{t+1}}{\\rightarrow}R_{t+2}, S_{t+2}\\overset{A_{t+2}}{\\rightarrow}R_{t+3} ...\r$$ 则可以定义这个trajectory的reward是：\n$$\rG_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\r$$ $G_t$也表示一个随机变量\n我们定义所有从$t$出发的trajectory的期望$G_t$，即为state-value functionorstate value\n$$\rv_\\pi(s) = \\mathbb{E}(G_t| S_t = s)\r$$ return 针对单个、单次的尝试（确定性的）\nstate value是一种平均情况\nBellman Equation\r递推形式\r描述不同state的state value之间的关系\n对于单个trajectory：\n$$\r\\begin{split} G_t \u0026= R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\\\\\r\u0026= R_{t+1} + \\gamma \\left( R_{t+2} + \\gamma^2 R_{t+3} + ...\\right)\\\\\r\u0026= R_{t+1} + \\gamma G_{t+1}\r\\end{split}\r$$ 非常动态规划（或者叫递推）的一个步骤\n因此可以推广到期望的情况（事实上可以直接写）\n$$\r\\begin{split}\rv_\\pi(s) \u0026= \\mathbb{E}(G_t| S_t = s) \\\\\r\u0026= \\mathbb{E}(R_{t+1} + \\gamma G_{t+1}| S_t = s) \\\\\r\u0026= \\mathbb{E}(R_{t+1}| S_t = s) + \\gamma \\mathbb{E}(G_{t+1}| S_t = s)\r\\end{split}\r$$ 我们考虑去代换两个期望符号\n对于第一个期望，其意义是：从$s$出发，得到奖励的均值\n始终注意：$R_{t+1}$代表是一个随机变量\n$$\r\\begin{split}\r\\mathbb{E}(R_{t+1}| S_t = s) \u0026= \\sum_{a} \\pi(a|s) \\mathbb{E}(R_{t+1}| S_t=s,A_t = a) \\\\\r\u0026=\\sum_a \\pi(a|s) \\sum_r p(r|s,a) r\r\\end{split}\r$$ 对于第二个期望，其意义是：从下一个时刻$t+1$出发可以得到的期望奖励\n$$\r\\begin{split}\r\\mathbb{E}(G_{t+1}| S_t = s) \u0026= \\sum_{s'} \\mathbb{E}(G_{t+1}| S_{t+1} = s') p(s'| s) \\\\\r\u0026=\\sum_{s'} \\mathbb{E}(G_{t+1}| S_{t+1} = s') \\sum_{a}\\pi(a| s)p(s'| s,a)\\\\\r\u0026=\\sum_{s'} v_{\\pi}(s') \\sum_{a}\\pi(a| s)p(s'| s,a)\r\\end{split}\r$$ 综上，整理一下原式子：\n$$\r\\begin{split}\rv_\\pi(s) \u0026= \\mathbb{E}(R_{t+1}| S_t = s) + \\gamma \\mathbb{E}(G_{t+1}| S_t = s) \\\\\\\\\r\u0026= \\sum_a \\pi(a|s) \\sum_r p(r|s,a) r+\\gamma\\sum_{s'} v_{\\pi}(s') \\sum_{a}\\pi(a| s)p(s'| s,a)\\\\\r\u0026= \\sum_a \\pi(a| s) \\left[ \\sum_r p(r| s,a)r + \\gamma \\sum_{s'}p(s'| s,a){\\color{Red} v_\\pi(s')} \\right], \\forall s\\in S\r\\end{split}\r$$ 这里的$S$是state space\n这样我们就得到了贝尔曼公式，由两个部分组成：\nimmediate reward future reward 通常我们会使用给定的$\\pi(a|s)$​，因此这个过程可以被视作Policy Evaluation\n$p(r|s,a),p(s\u0026rsquo;|s,a)$在这里是已知的\n但其实不知道也有办法去求解\n向量形式\r我们先把贝尔曼公式进行重写，用一些其他符号进行替代：\n$$\rv_\\pi(s) = r_\\pi(s) + \\gamma \\sum_{s'} p_\\pi(s'|s)v_\\pi(s')\r$$ 表示基于策略$\\pi$获取的当前奖励，以及之后的期望奖励\n我们令所有的状态依次编号为：$1\\to n$\n对于状态$s_i$，其state value表示为：\n$$\rv_\\pi(s_i) = r_\\pi(s_i) + \\gamma \\sum_{s_j} p_\\pi(s_j|s_i)v_\\pi(s_j)\r$$ 把所有$s_i$写在一起，自然就是向量形式：\n$$\rv_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi\r$$ $v_\\pi = \\left[ v_\\pi(s_1), \u0026hellip;,v_\\pi(s_n) \\right ]^T \\in \\mathbb{R}^n$ $r_\\pi = \\left[ r_\\pi(s_1), \u0026hellip;,r_\\pi(s_n) \\right ]^T \\in \\mathbb{R}^n$ 并且有：\n$$\rP_\\pi \\in \\mathbb{R}^{n\\times n}, \\text{where} \\space [P_\\pi]_{i,j} = p_\\pi(s_j|s_i)\r$$理解一下图：\nSolve State Values\r我们需要通过state value衡量当前的policy的表现\n因此需要进行求解\nMethod 1 · closed-form solution\r$$\rv_\\pi = r_\\pi + \\gamma P_\\pi v_\\pi\\\\\r(I-\\gamma P_\\pi)v_\\pi = r_\\pi\\\\\rv_\\pi = (I-\\gamma P_\\pi)^{-1}r_\\pi\r$$ 但是由于需要求逆矩阵，计算量过高，这个方法一般不会使用\nMethod 2 · iterative solution\r$$\rv_{\\pi, k+1} = r_\\pi + \\gamma P_\\pi v_{\\pi,k}\\\\\r$$ 我们不断迭代这个等式，可以使得最后有：\n$$\rv_{\\pi,k} \\to v_\\pi = (I-\\gamma P_\\pi)^{-1}r_\\pi, k \\to \\infty\r$$Action Value\r与state value相比，不止固定了当前的状态，action value同时固定了当前执行的动作 action value意义很大，有时候我们倾向于采取能带来最大value的action 定义如下：\n$$\rq_\\pi(s,a) = \\mathbb{E}\\left[G_t|S_t=s，A_t=a\\right]\r$$ 同时有：\n$$\r\\underbrace{\\mathbb{E}[G_t \\mid S_t = s]}_{v_{\\pi}(s)} = \\sum_{a} \\underbrace{\\mathbb{E}[G_t \\mid S_t = s, A_t = a]}_{q_{\\pi}(s,a)} \\pi(a \\mid s)\r$$ 因此：\n$$\r{\\color{Red} v_\\pi(s)} = \\sum_a \\pi(a|s){\\color{Red} q_\\pi(s,a)}\\\\\rq_\\pi(s,a) = \\sum_r p(r| s,a)r + \\gamma \\sum_{s'}p(s'| s,a){\\color{Red} v_\\pi(s')}\r$$ 第一个式子：通过所有action value，求解当前state value 第二个式子：通过所有state value，求解当前action value 求解的方式比较多样\n通过state value 通过数据采样进行求解（后文） ","date":"2025-10-30T20:25:34+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/","title":"强化学习的数学原理 · Chap2 · Bellman Equation"},{"content":"[TOC]\nhttps://www.bilibili.com/video/BV1sd4y167NS\n概念定义\rState：$s$\nstate space: the set of states $S = \\left{ s_i \\right }$ Action：$a$\naction space of a state: $\\mathcal{A}(s_i) = \\left{a_i \\right}$ State Transition：$s_i \\overset{a}{\\rightarrow} s_j$\n在确定性的情况，可以使用S+A的表格进行标识 State Transition Probability：表示成条件概率分布 Policy: determine what actions to take at a state\n$\\pi$：对于不确定的情况，同样是一个条件概率 Reward: a real number we get after taking an action\npositive/negative: it represents encouragement or punishment corner case zero reward: no punishment can positive mean punishment: 取决于怎么设计 Reward本质上是一种人机交互的接口 Trajectory: a state-action-reward chain\n$s_i\\xrightarrow[a_i]{r_i}s_{i+1}\\xrightarrow[a_{i+1}]{r_{i+1}}s_{i+2}$ return: the sum of all the rewards along a trajectory discounted return：trajectory可能会无限长（走到终点之后仍有奇怪多余的操作），导致reward发散到无穷 discount rate: $\\gamma \\in [0,1)$ 防止return发散，通过指数级别的系数累乘，保证return是收敛的 Episode: the agent may stop at some terminal states, the resulting trajectory is called an episode(or a trial)\nEpisodic task（有终止任务） Continuing task（持续任务） 在数学上，可以通过把 episodic task 转换为 continuing task 来统一处理两类任务 Option 1：吸收状态 (absorbing state)，当智能体到达目标状态后，就永远停留在那里，之后的奖励永远设为0 Option2：普通状态 (normal state)，智能体可以离开目标状态，任务会继续，每次进入目标状态时都会获得奖励 数学与建模上使用Option2会更方便、统一，$\\gamma$会控制一切 Markov Decision Process · MDP\rSets\nState: the set of states$\\mathcal{S}$ Action: the set of actions $\\mathcal{A}(s)$ is associated for state$s \\in \\mathcal{S}$ Reward: the set of rewards $\\mathcal{R}(s,a)$ Probability distribution\nState transition probability: at state $s$, taking action $a$, the probability to transit to state $s\u0026rsquo;$ is $p(s\u0026rsquo;|s, a)$ Reward probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s, a)$ Policy: at state $s$, the probability to choose action a is $\\pi(a|s)$\nMarkov property\n$p(s_{t+1} | a_{t+1}, s_t, \u0026hellip;, a_1, s_0) = p(s_{t+1} | a_{t+1}, s_t)$ $p(r_{t+1} | a_{t+1}, s_t, \u0026hellip;, a_1, s_0) = p(r_{t+1} | a_{t+1}, s_t)$ Markov decision process becomes Markov process once the policy is given.\n似乎这里的定义和比较官方的定义有一些差别\nMDP是一个在马尔可夫性质上的时序决策模型，由一个五元组定义：\n$$\rS,A,P,R,\\gamma\r$$ 状态集合State Space 动作集合Action Space 状态转移概率Transition Probability 奖励函数Reward 折扣因子Discount Factor 一个MDP的动态过程可以描述为一个与时间交互的循环：\n处于某个状态 执行一个动作 根据状态转移概率转移到某个状态 获得动作的即时奖励 重复 ","date":"2025-10-29T23:38:34+08:00","permalink":"https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/","title":"强化学习的数学原理 · Chap1 · 基本概念"},{"content":"[TOC]\nMMaDA: Multimodal Large Diffusion Language Models\r问题\n先前的多模态架构混合，不同模态需要不同组件、不同数据处理方式 扩散模型后训练策略欠缺研究 如何文本与视觉模态协同学习、各方面性能超过各领域现有模型 如何确保模型具有泛化能力 核心贡献\n统一Diffusion架构：消除模态专用组件，保持跨任务性能 混合Long-CoT的后训练：统一CoT格式，对齐跨模态推理过程，协同训练 UniGRPO：专用的强化学习方法 SOTA：文本推理、多模态理解、文生图三方面均是SOTA（AR、混合、扩散） Method\rPretrain\rData Tokenization 文本：采用LLaDA的tokenizer 图像：采用Show-o所使用的pretrained image quantizer 基于MAGVIT-v2架构（一个图像离散化模型） MAGVIT-v2的输入与输出\n输入：单张静态图片的像素阵列、由多帧图像组成的序列 输出：一个token序列 论文中采用$F=16$的下采样因子\n对于$H\\times W$的图像，转化为一维的$\\frac{H\\times W}{F^2}$长度序列\n统一的概率建模与目标 定义MMaDA为一个Mask Token Predictor，直接预测文本与图像的[MASK] 仅在[MASK]的图像或文本Token上做统一交叉熵损失 $$\rL_{unity}(\\theta) = -E_{t,x_0,x_t}\\left[\\frac{1}{t}\\sum_{i=1}^L I(x_t^i = [MASK])\\log p_\\theta (x_0^i|x_t)\\right]\r$$Post-Training with Mixed Long-CoT Finetuning\rMMaDA明确面向：\n推理密集型任务（例如数学） 具备World-knowledge-aware的文生图 事实一致性非常重要 为进行稳定的后训练，论文整理了一个包含三类核心任务（文本推理、多模态推理、文本到图像生成）CoT数据集\n利用这篇数据，在RL之前通过SFT做冷启动\n统一的CoT格式：消除不同任务的输出异构性 1 |\u0026lt;special_token\u0026gt;| \u0026lt;reasoning_process\u0026gt; |\u0026lt;special_token\u0026gt;| \u0026lt;result\u0026gt; 后续证明了有益于跨模态的协同训练与对齐\n希望文本推理逻辑指导图像生成\n多样性、复杂性、准确性 通过已有的LLM、VLM，合成多样化的数据 使用模型过滤，只保留高质量、长形式的CoT样本 MMaDA进行了混合任务的CoT微调\n保留提示词，对response进行加噪 通过预训练得到的Predictor进行损失计算 $$\rL_{Mixed-SFT}(\\theta) = -E_{t,p_0,r_0,r_t}\\left[\\frac{1}{t}\\sum_{i=1}^{L'} I(r_t^i = [MASK])\\log p_\\theta (r_0^i|p_0,r_t)\\right]\r$$Post-Training with Unified RL\r自回归模型：每个Token的条件概率都非常好计算，适合RL Diffusion：过程复杂，无法直接使用传统强化学习方法 局部掩码依赖：只有[MASK]处有预测概率，其他位置已知 掩码比例敏感：训练必须兼容不同噪声程度的恢复 LLaDA采样大量样本，造成RL开销巨大 非自回归序列似然： AR模型：句子概率可以通过token概率乘积计算 Diffusion：很难计算 UniGRPO\r这部分搁置一下 后续补一下RL的知识\n主要有三个关键点\n结构化加噪策略 序列对数似然近似为：被遮位置对数概率的平均 用旧策略和当前策略的“近似序列似然”做比值 UniGPRO的奖励是多样化的\n文本推理奖励 答案正确奖励 格式奖励（\u0026lt;think\u0026gt;\u0026lt;think\u0026gt;） 多模态推理奖励 同上 CLIP奖励：使用原始 CLIP 分数衡量文本-图像的语义一致性 文生图奖励 同上 图像奖励：反映人类偏好得分 Inference\r文本生成：采用半自回归采样 Masking Schedule采用线性计划，与LLaDA一致 图像生成：采用低置信度重掩码 余弦噪声调度 Experiments\r一般的benchmark跳过\n","date":"2025-10-17T13:39:00+08:00","permalink":"https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/","title":"Multimodal Diffusion Language Model · BirdResearch · 202510"},{"content":"安装WSL\r1 wsl --install 如果你的系统不支持 wsl --install ，可手动启用功能\n系统 \u0026gt; 可选功能 \u0026gt; 更多 Wndows 功能，打开 Hyper-V 和适用于 Linux 的 Windows子系统\n设置为wsl2\n1 wsl --set-default-version 2 查看发行版本：\n1 wsl --list --online 例如：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 C:\\Users\\95443\u0026gt;wsl --list --online 以下是可安装的有效分发的列表。 使用“wsl.exe --install \u0026lt;Distro\u0026gt;”安装。 NAME FRIENDLY NAME AlmaLinux-8 AlmaLinux OS 8 AlmaLinux-9 AlmaLinux OS 9 AlmaLinux-Kitten-10 AlmaLinux OS Kitten 10 AlmaLinux-10 AlmaLinux OS 10 Debian Debian GNU/Linux FedoraLinux-42 Fedora Linux 42 SUSE-Linux-Enterprise-15-SP6 SUSE Linux Enterprise 15 SP6 SUSE-Linux-Enterprise-15-SP7 SUSE Linux Enterprise 15 SP7 Ubuntu Ubuntu Ubuntu-24.04 Ubuntu 24.04 LTS archlinux Arch Linux kali-linux Kali Linux Rolling openSUSE-Tumbleweed openSUSE Tumbleweed openSUSE-Leap-16.0 openSUSE Leap 16.0 Ubuntu-20.04 Ubuntu 20.04 LTS Ubuntu-22.04 Ubuntu 22.04 LTS OracleLinux_7_9 Oracle Linux 7.9 OracleLinux_8_10 Oracle Linux 8.10 OracleLinux_9_5 Oracle Linux 9.5 openSUSE-Leap-15.6 openSUSE Leap 15.6 我这边安装Ubuntu-22.04 1 2 3 # 安装 Ubuntu-22.04 到默认位置 # 但这个方式有点看网速 你先别 wsl --install -d Ubuntu-22.04 建议从中这里开始\n网址：https://cloud-images.ubuntu.com/releases/22.04/release/\n选择这个：\n1 ubuntu-22.04-server-cloudimg-amd64-root.tar.xz 创建路径\n1 mkdir D:\\wsl\\Ubuntu-22.04 将下载的压缩包导入\n1 2 # 导入到指定目录 wsl --import Ubuntu-22.04 D:\\wsl\\Ubuntu-22.04 C:\\Users\\95443\\Downloads\\ubuntu-22.04-server-cloudimg-amd64-root.tar.xz 后面那个是你下载的地址\n启动：\n1 wsl -d Ubuntu-22.04 然后这个时候应该是root用户\n我们创建一个user\n1 2 3 4 5 6 # 创建新用户并设置密码 adduser biribiribird # 将用户添加到sudo组 usermod -aG sudo biribiribird # sudo vim /etc/wsl.conf 插入内容：\n1 2 [user] default = biribiribird 这样启动wsl的默认用户就确定了\n回到windows\n1 2 3 4 5 # 关闭所有 WSL 实例 wsl --shutdown # 重新启动 Ubuntu-22.04 生效默认用户 wsl -d Ubuntu-22.04 设置一下默认的wsl\n1 wsl --set-default Ubuntu-22.04 1 wsl -l -v # 检查 ","date":"2025-10-16T21:55:42+08:00","permalink":"https://example.com/p/wsl2%E9%83%A8%E7%BD%B2/","title":"WSL2部署"},{"content":"安装TeX\rAcquiring TeX Live as an ISO image - TeX Users Group\n选择.iso\n下载之后点击.iso打开\n安装脚本\nVscode\r安装LaTex Workshop扩展\nsetting.json\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 { \u0026#34;latex-workshop.latex.recipes\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;xelatex\u0026#34;, \u0026#34;tools\u0026#34;: [\u0026#34;xelatex\u0026#34;, \u0026#34;xelatex\u0026#34;] } ], \u0026#34;latex-workshop.latex.tools\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;xelatex\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;xelatex\u0026#34;, \u0026#34;args\u0026#34;: [ \u0026#34;-synctex=1\u0026#34;, \u0026#34;-interaction=nonstopmode\u0026#34;, \u0026#34;-file-line-error\u0026#34;, \u0026#34;%DOC%\u0026#34; ] } ], \u0026#34;latex-workshop.latex.recipe.default\u0026#34;: \u0026#34;lastUsed\u0026#34;, \u0026#34;editor.mouseWheelZoom\u0026#34;: true, \u0026#34;latex-workshop.view.pdf.sidebar.view\u0026#34;: null } ","date":"2025-10-12T01:20:42+08:00","permalink":"https://example.com/p/vscode--latex/","title":"Vscode + LaTeX"},{"content":"UV\r1 2 pip install uv uv --version 1 2 mkdir myproject \u0026amp;\u0026amp; cd myproject uv init 生成一个虚拟环境\n1 uv venv 1 2 source .venv/bin/activate # 启动环境 deactivate # 关闭环境 ","date":"2025-10-02T16:20:42+08:00","permalink":"https://example.com/p/python-uv%E6%8C%87%E5%8D%97/","title":"Python · uv指南"},{"content":"AR LLM\r对于自回归模型，假设输入是：[你, 好, ！]\n词汇表是：\n1 2 3 4 5 6 7 8 0: \u0026lt;pad\u0026gt; 1: 你 2: 好 3: 我 4: 很 5: 好 6: ！ 7: \u0026lt;eos\u0026gt; [TOC]\nDream 7B: Diffusion Large Language Models\r2508.15487 Dream 7B: Diffusion Large Language Models\n问题： AR模型对于需要整体考虑的任务（长期规划、多约束）场景表现差 AR模型对于长文本的一致性较差 在各类通用任务中，要达到与Qwen2.5等顶尖自回归模型相当的性能仍存在显著差距 贡献： 基于自回归的LLM 初始化和上下文自适应噪声调度技术来实现扩散语言模型的规模化训练 Dream 7B Base和Dream 7B Instruct Approach\r使用Transformer以偏移方式，预测所有[MASK] 常规的MDM是直接预测对应位置的[MASK]，需要重新训练一个新的Transformer\nAR-based LLM Initialization\r自回归模型的训练目标就是使用第$i$个隐藏状态预测$i+1$的token\n因此我们以偏移方式进行预测，没有打破这种位置关系\n因此将已有的自回归模型参数作为初始值\n保留AR模型的知识 加速收敛 Context-Adaptive Token-Level Noise Rescheduling\r先前衡量噪声程度一般都是句子级别的：LLaDA衡量某个句子在$t$时刻的权重是$\\frac{1}{t}$\n本文发现不同token之间的上下文信息是不同的，因此需要对噪声的衡量更加精细，避免学习的不平衡\n公式化地，定义损失函数：\n$$\rL(\\theta) = -\\mathbb{E}_{x_0,t,x_t}\\sum_{i=1}^{L}1\\left [x_t^i=M\\right] \\cdot w(t,x_t,i) \\cdot \\log p_\\theta(x_0^i\\mid x_t)\r$$ 对于LLaDA，其$w(t,x_t,i) = \\frac{1}{t}$\n考虑对于某个token的上下文信息：\n距离越近的unmask的token提供的信息越丰富 因此论文定义为：\n$$\rw(t,x_t,i) = \\frac{1}{2}\\sum_{j=1}^L\\left [x_t^j\\neq M\\right] Geo(p, |i-j|-1)\r$$ 其中$Geo$表示几何分布核：\n$$\rGeo(p,d) = (1-p)^d\\cdot p, \\quad d\\geq 0\r$$ 距离$d$越大，贡献越小 超参数$p$： Train\rDream-7B采用了与Qwen2.5-7B完全相同的Transformer架构配置\nPretrain\nSFT\n采用了之前的技巧，训练上与LLaDA没什么不同（注意损失函数）\nExperiment\rBase模型\r推理任务中（ARC-E、ARC-C）表现良好\n规划任务中领先幅度巨大\n训练数据量非常小\n结论：\n初始化策略和上下文自适应噪声调度有效性 Dream-Instruct\r180万条数据，进行3轮微调\n论文中没做分析 这里和LLaDA一样，SFT之后效果落后，甚至出现了性能下降 扩散大语言模型在遵循指令任务中具备与基于自回归的大语言模型相匹敌的潜力，为未来高级扩散大语言模型后训练方案奠定了基础\nAR Initialization的贡献\r验证：AR LLM初始化是有效的 实验设计：\nLLaMA3.2-1B参数初始化的Dream-1B和从头训练的Dream1B Loss始终更低，证明了初始化是有效的 同时在这个实验中，论文说明了学习率的影响非常大：\n大的学习率：破坏AR LLM的有益特性 小的学习率：阻碍学习扩散的过程 （但似乎没写上下文自适应噪声调度机制的消融实验）\nPlanning Ability\rDream 模型在两项任务中始终优于其他同等规模的基线模型 扩散语言模型在解决涉及多重约束或特定目标优化的问题时具有天然优势（？） Trade-off\rDiffusion language models provide a unique advantage through their adjustable inference process\n基于时间步长的方法为推理时缩放引入了新的维度，可与现有技术协同工作，例如 OpenAI o1和DeepSeek R1等大型语言模型中使用的思维链推理 这种可调节的计算质量权衡代表了扩散模型区别于传统自回归模型的关键优势。 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models\r2505.19223 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models\n看不懂\n大概就是通过VRPO这个方法，基于LLaDA的工作，对LLaDA-instruct进行RL\nLongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs\r2506.14429 LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs\n核心问题：扩散型LLMs在长文本处理领域的研究空白\n为什么扩散LLM在直接长度外推时保持稳定的困惑度并呈现局部感知特性 针对自回归 LLM 建立的长度扩展技术能否迁移至扩散架构 自回归基线相比，扩散 LLM 在长上下文基准测试中表现如何？会显现哪些独特能力或局限性？ 贡献：\n揭示了其在上下文外推过程中保持稳定困惑度和局部感知的独特特性，并通过RoPE机制进行了解释 基于 NTK 的 RoPE 外推法与缩放定律可无缝迁移至扩散 LLMs，实现 6 倍上下文扩展 benchmark表明：扩散 LLMs 在检索任务中与自回归模型表现相当，在聚合任务中稍显不足，但在问答任务中表现卓越 Long-Context Phenomenology of Diffusion LLMs\r大海捞针测试（Needle-In-A-Haystack, NIAH）\n在一个超长的上下文（haystack，干草堆）里，研究者会插入一小段关键信息（needle，针）\n模型的任务是：在生成或问答过程中，能否准确地“找到”并使用这段信息。\n这类测试会改变针的位置（例如放在靠前、中间或靠后部分）以及上下文的总长度，用来观察模型在不同深度和不同长度下的表现。\n实验目的：揭示扩散 LLM 在长上下文中出现的局部感知 (local perception)\n实验设计：\n输入：在不同长度（最多32k）的长上下文中插入一个needle\n输出：限定模型输出最多32个token\n实验对象\nDLM：block size = 32，采样步数 = 32 LLM：默认 评估指标\n找到Needle的成功率 模型在不同深度（前文、中间、后文）找到Needle的能力 附录中补充了其他DLM模型的实验\nAR LLM在8K内的上下文表现完美，超过8K长度无法完成任何任务 DLM出现了类似**滑动窗口（窗口长度为4k）**的表现 DLM受采样步数影响较大，因此定量补充了实验：\n表明扩散 LLMs 的长上下文性能虽受采样步数影响，但仍受限于模型支持的最大上下文长度 机制分析\r自回归只能看见后续的：$[0, T_{train} - 1]$（LLaMA的$T_{train} = 8192$​） DLM是双向注意力：$[1-T_{train},T_{train}-1]$（LLaDA的$T_{train}=4096$） 对于单个token，可以同时出现在左边的上下文窗口，也可以出现在右边的上下文窗口 留坑：RoPE\nLLaMA完全丢失了负相对位置的信息，外推能力受限 LLaDA虽然$T_{train}$比较小，但是能够接受到一个负正窗口 LLaMA：只学习了从头往后一个个token读取的能力\n它可以知道，第2个token是第1个token的后一个……第1000个token是第999个token的后一个……\n（像翻书一样可以一页一页翻）\n但是一旦碰到第10000页，它推理不出这是9999页过来的（超出上下文，没有学习过这种关系）\nLLaDA：双向上下文\n可以推断出9999是10000的前一页\n论文补充了t-SNE可视化实验\n观察了两个模型最后的Q和K states\nLLaDA随着上下文长度增加，仍然保持形状 LLaMA出现了明显的聚类分离，表示内部出现了distribution shift Context Extension\r将 NTK-based RoPE extrapolation（一种在自回归 LLM 中已验证的旋转位置嵌入扩展方法）迁移到扩散式 LLM\n缩放旋转基数 β0，让正弦/余弦函数周期变长，相当于“拉伸坐标轴”，从而容纳更长的上下文\n小幅扩展有效： 8k 或 16k，几乎在所有深度下都保持接近 100% 的检索准确率。\n中等扩展出现性能下降：24k ，出现lost-in-the-middle现象\n自回归模型中同样有的现象 大规模扩展失败：模型无法再有效外推，说明方法的实际上限已到达。\n附录中对同类的DLM做了相同的实验\nExperiment\rSD、MD、Sum 和 Syn 分别代表单文档问答、多文档问答、摘要和合成任务\nAvg 是所有子任务按评估数据数量加权的平均得分\n平均得分媲美AR LLM 检索（NIAH）/聚合（AGG）/问答（QA)\n检索任务：相当 聚合任务：不如AR LLM 问答任务：超过AR LLM LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning\rLLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning\n问题：完全基于扩散机制的多模态大语言模型能否达到与AR LLM相匹敌的性能？\n论文贡献\n首个完全基于扩散模型的多模态大语言模型 在多个基准测试中展现出卓越的可扩展性 在混合型及纯扩散式多模态大语言模型中均SOTA Visual Instruction Tuning\rVison Tower（CLIP或SigLIP）：图像转视觉表征\nMLP connector：嵌入LLM词空间\nLanguage Tower：LLM\n主流的多模态大模型架构之一，只需要相对较少的数据（less than 100w 图文数据对）\n本文主要研究如何在DLM中进行Visual Instruction Tuning\nMethod\rLanguage Tower：LLaDA 8B（与LLaMA3-8B相当的语言模型） Vison Tower：SigLIP MLP Connector：a two-layer MLP Training\r训练阶段引入了含有多轮对话的数据\n为了简化描述，文章以2轮对话的数据进行说明，定义符号：\n$\\mathcal{v}$：Vison Tower和MLP Connector生成的视觉表征向量 $[M]$​：掩码标记 数据：$(\\mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2)$ $p_0^1 = [ p_0^{1,i}]$：首轮提示文本 $p_0^2 = [ p_0^{2,i}]$ ：次轮提示文本 对于一个二轮对话，训练目标定义为：\n$$\rL(\\theta) = -\\mathbb{E}_{\\mathcal{v},t,p_0^1,r_0^1,r_t^1,p_0^2,r_0^2,r_t^2}\\left[\\frac{1}{t}\\sum_{i=1}^{L_{p_1}}\\sum_{j=1}^{L_{p_2}}1\\left[r_t^{1,i}=M\\wedge r_t^{2,j}=M\\right] \\cdot \\log p_\\theta(r_0^{1,i},r_0^{2,j}\\mid \\mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2 ) \\right]\r$$ 在多轮对话场景下，不同轮次的响应是强相关的\n用户的问题可能在第 1 轮，答案在第 2 轮 推理链条往往横跨多个回合，不能只看单独的 token 模型必须在预测某个 token 时，同时考虑另一轮对话中的掩码 token\n这样就把 跨轮次的依赖关系 学进去，而不是每轮单独学\n联合约束迫使模型去捕捉 对话轮次之间的因果逻辑\n理论上这个式子在先前工作中已经被证明为整个任务的负对数似然上界\n在多轮对话中似乎可以采用causal mask，阻止早期对话轮次访问了后期的对话轮次 后文消融实验证明双向注意力的效果更好（实现对整体对话语境的全面理解） 该机制在近期视频扩散模型中已证实可有效提升生成视频的时间连贯性\n本身训练的流程和LLaDA的SFT流程比较相似，加噪只会在Response中，且同时对多轮对话中的Response进行加噪\n一次性让模型恢复所有对话中的MASK\nTraining Strategies\r整个训练过程参考了LLaVA的训练策略\n建立语言和视觉对齐关系并培养视觉指令跟随能力\n训练目标函数与上文相同\n阶段一：语言-图像对齐 目的：图像与语言的分布不一致，如果直接做指令调优，模型学习跨模态语义很困难 方法：将视觉表征与 LLaDA 的词向量进行对齐 冻结Vison Tower和Language Tower（这两个本身进行过预训练），只训练MLP Connector 数据集：LLaVA-Pretrain 阶段二：视觉指令调优Visual Instruction Tuning 目的：（单图像训练）建立基本的图像理解能力，（多图像训练）扩展到时序和跨图像推理 方法：（两个阶段）解冻所有层 单图像训练Single image：在 1,000 万单图像样本上训练，增强对单张图像的理解与响应能力。 统一视觉训练阶段one vision：在 约 200 万多模态样本（包括单图、多图和视频）上训练，使模型具备处理复杂场景的能力 数据集：MAmmoTH-VL 数据集 阶段三：多模态推理增强 Multimodal Reasoning Enhancement 目的：增强模型处理复杂任务的多模态推理能力，加入reasoning data提升数学、跨图像和逻辑推理任务的表现 方法 推理训练：使用来自 VisualWebInstruct聚焦推理的多模态数据对 LLaDA-V 进行训练（90 万个问答对，详尽的推理链和最终答案） 平衡训练：参考qwen系列，融合VisualWebInstruct（其中50%添加\\think）和MAmmoTH-VL(one vison部分，全部添加\\no_think，鼓励直接回答) Inference\r推理时根据已有的对话记录，对当前的prompt进行单轮的response生成\n重掩码策略采用low-confidence strategy\nExperiment\r可扩展性\nLLaDA-V 随着训练数据增加性能持续提升 在 多学科与数学推理任务 上，LLaDA-V 扩展性明显优于 LLaMA3-V 但在 图表/文档理解 和 真实场景理解 任务上，LLaMA3-V 表现更优 Benchmark\n对于已有的混合或扩散模型，LLaDA-V是SOTA 对比LLaMA3-V：6 个任务上超越 对比Qwen2-VL：整体仍落后 图表/文档理解和 RealWorldQA 上表现稍差 消融实验\n对比了Causal Mask和无Mask（多轮对话） 12个benchmark中7个更优 Conclusion\r图像接入SigLIP的方式比较简单，会丢失分辨率和信息，造成图表问题表现差 LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding\r2508.01617 LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding\n没怎么看，大概是把LLaDA-V的工作调整到了垂类领域\n一些比较有趣的实验分析：\nDLM在一些垂类领域非常合适，可以显式地控制一个大概的生成长度 模型可能出现重复 token（如 “the the the …”）的问题，尤其在采样步数较少或长度设定较大时 直接使用LLaDA-V的参数做微调的性能反而更差，需要从LLaDA-instruct出发，重新走3个步骤 LaViDa: A Large Diffusion Language Model for Multimodal Understanding\rhttps://arxiv.org/abs/2505.16839\n问题：\nAR LLM对强双向上下文要求的任务（文本填充、从图像中提取信息填充到json格式）很弱\n视觉语言场景中对输出模式的要求特别严格 标准扩散模型训练的数据效率低下，未被遮盖的token不参与损失函数计算，容易遗失关键语义信息（关键词）\n现有的推理方式缺少了KV cache的支持（双向上下文固有的缺陷）\n短文本环境中是可容忍的 对于VLM任务无法接受，常常伴有数百个visual token 固定比例的unmask在迭代次数较少时效果非常差\n贡献：\n第一个DLM视觉语言模型\n一种互补的mask方案，确保每个token都能参与到学习过程中，提高数据效率\nPrefix-DLM decoding：缓存多模态的提示词与图像输入，从而加速推理过程\n受文生图技术的启发，采用了时间步偏移策略，自适应调整每次迭代的解码数量\nMethod\rVision Encoder和DLM通过MLP进行连接\n模型输入：图像$I$和文本$P$\nVision Encoder\r$I \\to_{resize} 768\\times 768$ 切分成四个不重叠的部分（$I_{1:4} = (384,384)$）；直接resize原图为$(384,384)$，得到$I_5$ 每一个子图被独立地通过Vision Encoder（SigLIP-400M ）进行编码 $I_i \\to V_i, \\text{size} = 27 \\times 27$，五个子图总共产生了3645个embeddings $2\\times 2$平均池化（缩短序列，提升训练效率）：$14\\times 14$，总共980个embeddings 经过MLP构成的Projection Network，Flatten到1D 这里是输出5个一维向量，还是拼在一起的1个一维向量？\n暂时没去研究\nDLM\rDLM的输入：\n视觉嵌入向量 文本提示词$P$ 带有掩码的$X_t$ 输出：\n概率分布，用于获取$X_0$ 论文采用LLaDA和Dream作为DLM\nComplementary Mask\r文本中信息量非常密集，一般就是几个词\n对于之前的加噪方法，不一定能恰好遮蔽需要的词\n例如The answer is dog.，加噪为：The [MASK] [Mask] dog.\n事实上我们只需要引入它的互补形式：[MASK] answer is [MASK]\n同时，这份数据会直接copy视觉以及提示词部分的嵌入\n因此对实际训练开销的影响较小\nPrefix-DLM\r定义序列长度为$L$，推理的迭代次数$K$，我们有NFE（fraction of the number of functional evaluations）：\n$$\r\\text{NFE} = \\frac{K}{L}\r$$ NFE=100%时，每次迭代生成一个Token NFE=50%时，每次迭代生成2个Token 但实际上由于毫无推理优化，DLM的速度比自回归慢\nCausal Mask：可以不断复用之前的token的kv矩阵（之前的token没有变化，且之前的token对未来的token不感兴趣） Full Mask：每个token都会看前后的内容，因此需要不断重新计算 Prefix-DLM：I和P部分不会发生变化，遮蔽了对未来answer的token Schedule Shift\r喷了一下等步长的解码，提出了非线性的递推：\n$$\rt'_i = \\frac{\\alpha t_i}{1+(\\alpha -1)t_i}\r$$ 设计一个时间重参数化的函数，要求满足：\n$t_0 = 0,t_1 = 1$，仍然保持边界 单调递增 曲率可控，方便控制早晚阶段的速度 简单，防止数值不稳定或梯度爆炸 论文希望早期降噪快，后期降噪慢\n希望先快速搭建一个骨架，后面慢慢填充\n对这个式子求导：\n$$\r\\frac{dt_i'}{dt_i} = \\frac{\\alpha}{(1+(\\alpha-1)t)^2}\r$$ 当$t=0$，导数为$\\alpha$，$t=1$，导数为$\\frac{1}{\\alpha}$\n可以通过$\\alpha$​和1的大小关系，控制是先快后慢，还是先慢后快\nExperiments\r略过benchmark部分\nReasoning Distillation\rpaper通过蒸馏VL-Rethinker-7B模型（19.2K CoT examples）\n训练得到LaViDa-Reason\n在MathVista、MathVerse和MathVision上均得到提升\nText Infilling\r对于实际的文本填充任务，不需要生成所有内容，只需要填写需要填写的\n任意时间步长开始生成\n例如：\nThere is a [M][M][M][M] in the image.\n这里我们希望是dog或者traffic light，也是就是variable-length completions\n因此利用了第二阶段（激活全部参数）的20%数据，进行第三阶段训练，得到LaViDa-FIM\n文本中插入随机长度的[S][S]……[S][FIM]序列 推理时，在掩码段后面附加[FIM]，得到There is a [M][M][M][M][FIM] in the image. 模型自然可以生成： There is a dog[S][S][S][FIM] in the image. There is a traffic light[S][S][FIM] in the image. 约束性诗歌生成：模型根据图像生成一首诗，每行以特定音节开头\n强调了结构性约束和上下文一致性，测试双向生成能力\nSentence：满足行级别的约束的比例 Sample：满足样本级约束的比例 Speed vs. Quality Trade off\r设定长度固定32，通过调整迭代次数K，进行实验\n（数据集COCO2017图像描述 500张，生成图像标题）\nNFE=100%，稍慢于AR LLM，但是性能更强 50-75%的性能和速度都很不错 有效的加速推理，性能下降少 先快后慢是对的 消融实验部分\n验证了互补掩码 验证了图像分辨率的影响（输入分辨率） OCR（上面四个）的提升更为明显 一般视觉理解任务（最后一个）提升不多 STOP，接下来需要进入any2any的调研\n这部分内容另开一篇\n","date":"2025-10-01T15:54:00+08:00","permalink":"https://example.com/p/diffusion-language-model-birdresearch-202510/","title":"Diffusion Language Model · BirdResearch · 202510"},{"content":"Large Language Diffusion Models\n[TOC]\nIntro\r理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布\n因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布\n因此指令跟随、上下文学习并不是ARM的专利\nApproach\r概率公式\r前向过程Forward Process\r序列中逐渐添加mask，直到所有序列全部masked\n每个标记有一定概率masked，或者保持unmasked状态 给定原始数据$x_0$，随机采样的一个时间点$t \\in [0,1]$ $t=0$表示起点，全部token都是unmasked $t=1$表示终点，全部token都已经masked 序列中每个token的masked概率就是$t$ 该时刻的序列被定义为$x_t$​ Bert的mask比例是固定的\n反向过程Reverse Process\r参数为$\\theta$的模型，生成序列$x_0$的概率$p_\\theta(x_0)$就是我们需要训练的模型\n我们希望其尽可能接近真实数据的分布$p_{data}(x_0)$\n反向过程的目标：$x_{t=1}$出发，恢复$x_0$ 方法：通过mask predictor，逐步填充masked token Mask Predictor\rLLaDA的核心是一个mask predictor\n$$\rp_\\theta\\left (\\cdot\\mid x_t \\right)\r$$ 其中的·表示一个占位符\n例如：\n1 I [MASK] cats. 则$p_\\theta(\\cdot \\mid [I,[MASK],cats])$ 就是一个基于词表的概率分布表\ntoken p like 0.9 eat 0.1 此时Mask Predictor会对所有[MASK]进行预测\n不像ARM只预测一个token\n假设序列为：$(x_t^1, x_t^2,\u0026hellip;,x_t^L)$\n我们的目标即为，对于masked位置$i$，最大化概率：\n$$\rp_\\theta(x_0^i\\mid x_t)\r$$其中$x_0^i$就是原序列的ground truth\n损失函数\r对于单个masked位置，我们希望概率尽可能大\n因此需要使用交叉熵损失\n补习一下交叉熵\n假设真实分布是$q(y)$，模型分布是$p_\\theta(y)$\n交叉熵定义为：\n$$\rH(q,p_\\theta) = -\\sum_y q(y)\\log{p_\\theta(y)}\r$$ 在该任务下，$q(y)$​是一个独热分布\n$$\rq(y) = \\left\\{\\begin{matrix}\r1\u0026,y=x_0^i \\\\\r0\u0026,otherwise\r\\end{matrix}\\right.\r$$ 代入得\n$$\r\\mathcal{L}(x_0^i,x_t) = -\\log p_\\theta(x_0^i\\mid x_t)\r$$ 故整体的损失就只需要对所有masked位置求和\n$$\r\\mathcal{L}(x_0,x_t) = -\\sum_{i=1}^L1\\left[ x^i_t=M\\right] \\log{p_\\theta\\left( x_0^i\\mid x_t\\right)}\r$$ 其中$1\\left[ x^i_t=M\\right] $表示指示函数，确保代入计算的数值是[MASK]\n但是这样是不合理的，序列中[MASK]越多，损失似乎会越大\n因此需要做一下归一化\n对于$x_t$，其在该时刻会有$tL$个token被masked\n因此需要代入一个$\\frac{1}{t}$​（值得一提的是，$\\frac{1}{t} \\geq 1$）\n$$\r\\mathcal{L}(x_0,x_t) = -\\frac{1}{t}\\sum_{i=1}^L1\\left[ x^i_t=M\\right] \\log{p_\\theta\\left( x_0^i\\mid x_t\\right)}\r$$ 建立关于模型参数$\\theta$的损失函数则有：\n$$\rL(\\theta) \\triangleq\r-\\mathbb{E}_{t,x_0,x_t}\\left[\\frac{1}{t}\\sum_{i=1}^L1\\left[ x^i_t=M\\right] \\log{p_\\theta\\left( x_0^i\\mid x_t\\right)}\\right]\r$$ $\\triangleq$代表定义为 这里不是在陈述一个“事实”，而是在引入损失函数的定义。\n如果写$=$，读者可能会以为“这是某个推导得到的等式”；\n如果写 $\\triangleq$，读者一眼就知道：哦，这是“定义”，不是推导。\n从均匀分布$U(0,1)$采样的任意$t$，从数据集中采样的任意数据$x_0$，根据前向传播方法得到的$x_t$ 负对数似然的上界\r我们本身的目标是使得$p_\\theta(x_0)$的分布接近$p_{data}(x_0)$\n但是我们从未单独定义、训练$p_\\theta(x)$这个模型\n而是定义了一个$p_\\theta(x_0^i\\mid x_t)$，不断重复迭代，起到了$p_\\theta(x)$的作用\n因此我们上述内容得到的$L(\\theta)$是作为模型$p_\\theta(x_0^i\\mid x_t)$的损失函数\n我们如何保证训练出这个模型，可以使得$p_\\theta(x_0)$的分布接近$p_{data}(x_0)$？\n我们定义真实的似然函数是\n$$\r\\mathcal{L}(\\theta) = -\\mathbb{E}_{p_{data}(x_0)}\\left [\\log{p_{\\theta}(x_0)}\\right]\r$$ 按照真实分布，采样数据$x_0$，得到的似然函数期望 我们需要最小化这个式子\n定义前向加噪分布$q$：\n$$\rq(x_t\\mid x_0) = \\prod_{i=1}^L \\left [ (1-t)\\times 1(x_t^i=x_0^i) + t\\times 1(x_t^i=M) \\right]\r$$ 其描述了通过已知的前向过程，从$x_0$得到$x_t$的概率\n$q$是我们引入的噪声分布，并非需要训练的参数模型，不使用$p$定义\n由于前向过程是已知的，我们考虑使用它表示$p_\\theta$\n$$\rp_\\theta(x_0) = \\sum_{x_t}p_\\theta(x_0\\mid x_t)p_\\theta(x_t)= \\sum_{x_t}p_\\theta(x_0, x_t) $$ 原始句子出现的总概率就是把所有可能路径的概率加起来。\n（先得到$x_t$，再生成$x_0$）\n引入已知的$q$\n$$\rp_\\theta(x_0) = \\sum_{x_t}q(x_t\\mid x_0)\\frac{p_\\theta(x_0,x_t)}{q(x_t\\mid x_0)}\r$$ 其中$\\sum_{x_t}q(x_t\\mid x_0)] \\times (\\cdot)$，可以理解为对$q(x_t\\mid x_0)$的期望\n离散情况下：$\\mathbb{E}_{x\\sim r}(g(x)) = \\sum_x r(x)g(x)$\n此时$x_0$是当作固定值，所有都可以看作关于$x_t$的函数\n因此则有：\n$$\rp_\\theta(x_0) = \\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\frac{p_\\theta(x_0,x_t)}{q(x_t\\mid x_0)}\\right]\r$$ 任意从$q$分布中采样$x_t$\n采样 Jensen不等式：\n$$\r\\log \\mathbb{E}(z) \\geq \\mathbb{E}(\\log{z})\r$$ 此时则有：\n$$\r\\log{p_\\theta}(x_0) = \\log{\\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\frac{p_\\theta(x_0,x_t)}{q(x_t\\mid x_0)}\\right]} \\geq \\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\log{p_\\theta(x_0,x_t)} - \\log{q(x_t\\mid x_0)} \\right]\r$$$$\r-\\log{p_\\theta}(x_0) \\leq -\\mathbb{E}_{ q(x_t\\mid x_0)}\\log{p_\\theta(x_0,x_t)} + \\mathbb{E}_{ q(x_t\\mid x_0)} \\log{q(x_t\\mid x_0)}\r$$ 分解一下联合概率\n$$\r\\log{p_\\theta(x_0,x_t)} = \\log{p_\\theta(x_0\\mid x_t)} + \\log{p_\\theta(x_t)}\r$$ 代回则有：\n$$\r-\\log{p_\\theta}(x_0) \\leq -\\mathbb{E}_{ q(x_t\\mid x_0)}\\log{p_\\theta(x_0\\mid x_t)}-\\mathbb{E}_{ q(x_t\\mid x_0)}\\log{p_\\theta(x_t)} + \\mathbb{E}_{ q(x_t\\mid x_0)} \\log{q(x_t\\mid x_0)}\r$$ $p_\\theta(x_t)$：其中$x_t$是前向过程人为生成的，因此与$\\theta$无关 $q(x_t\\mid x_0)$是噪声项，与$\\theta$无关 因此可以写成\n$$\r-\\log{p_\\theta}(x_0) \\leq -\\mathbb{E}_{ q(x_t\\mid x_0)}\\log{p_\\theta(x_0\\mid x_t)} + \\text{const}\r$$ 两边同时对真实数据计算期望\n$$\r-\\mathbb{E}_{p_{data}(x_0)}(\\log{p_\\theta}(x_0) ) \\leq -\\mathbb{E}_{p_{data}(x_0)}(\\mathbb{E}_{ q(x_t\\mid x_0)}\\log{p_\\theta(x_0\\mid x_t)}) + \\text{const}\r$$ 左边实质上就是我们需要优化的目标，命名为负对数似然$\\text{NLL}$\n根据\n$$\r\\log {p_\\theta(x_0\\mid x_t)} = \\sum_{i=1}^L 1\\left [ x^i_t=M\\right ] \\log{p_\\theta}(x_0^i\\mid x_t)\r$$ 代入得：\n$$\r-\\mathbb{E}_{p_{data}(x_0)}(\\log{p_\\theta}(x_0) ) \\leq -\\mathbb{E}_{p_{data}(x_0)}\\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\sum_{i=1}^L 1\\left [ x^i_t=M\\right ] \\log{p_\\theta}(x_0^i\\mid x_t)\\right] + \\text{const}\r$$ 事实上右边就是：\n$$\r-\\mathbb{E}_{p_{data}(x_0)}\\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\sum_{i=1}^L 1\\left [ x^i_t=M\\right ] \\log{p_\\theta}(x_0^i\\mid x_t)\\right] = -\\mathbb{E}_{t,x_0,x_t}\\left[\\sum_{i=1}^L1\\left[ x^i_t=M\\right] \\log{p_\\theta\\left( x_0^i\\mid x_t\\right)}\\right] = tL(\\theta)\r$$ 上述内容都是正项（负概率对数），$t \\in [0,1]$，因此满足\n$$\r-\\mathbb{E}_{p_{data}(x_0)}\\mathbb{E}_{ q(x_t\\mid x_0)}\\left[\\sum_{i=1}^L 1\\left [ x^i_t=M\\right ] \\log{p_\\theta}(x_0^i\\mid x_t)\\right] + \\text{const} \\leq L(\\theta) + \\text{const}\r$$ 则有：\n$$\r\\text{NLL} = -\\mathbb{E}_{p_{data}(x_0)}(\\log{p_\\theta}(x_0) ) \\leq L(\\theta) + \\text{const}\r$$ 至此，成功证明了$L(\\theta)$决定了$\\text{NLL}$​的上界（常数可以忽略）\nPre-Training\r输入：mask predictor$p_\\theta$，训练数据$p_{data}$ 输出：$p_{\\theta}$（收敛） mask predictor采用Transformer架构 不采用causal mask，能看见双向上下文 未使用KV Cache，采用标准的Vanilla Multi-Head Attention，每个头单独一份k,q,v Transformer架构尽量与LLaMA3对齐，从attention和FFN两个参数大头中，选择了减少FFN的参数量，保持参数规模可以比较 在自回归 LLM 生成时，生成新 token 时可以复用之前的 K/V 矩阵（不用重新算整个序列的注意力）。\n这是 KV cache 的意义：极大加速推理，节省显存。\n但 LLaDA 每一步预测的是 全局被 mask 的位置（不是单个 token），所以每一步输入分布会变，全序列 K/V 都要重新计算 → KV cache 无法使用。\n99%的数据固定长度4096 1%的数据随机采样长度 SFT\r对于问答对$(p_0,r_0)$，我们不改变提问部分，只对回答部分进行掩码加噪得到$r_t$\n损失函数设计为：\n$$\r-\\mathbb{E}_{t,p_0,r_0,r_t}\\left[\\frac{1}{t}\\sum_{i=1}^{L'}1[r^i_t = M]p_\\theta(r_0^i\\mid p_0,r_t)\\right]\r$$ $r_0$的长度是天然动态的，使用EOS填充 Inference\r给定$p_0$，我们从完全掩码的$r_1$开始\n设定超参数如下：\n迭代次数（采样步骤总数）：a trade off between efficency and quality 生成长度：实质上是一个上界 假设我们从时间$t\\in(0, 1]$转移到$s\\in[0,t)$，需要做的事是：\n$p_0,r_t$作为模型的输入，预测$r_0$（模型会unmask所有被掩码的token） 由于我们只转移到$s$，因此需要保留$sL$个掩码 对预测出的$r_0$，从中随机remask$\\frac{s}{t}L$个token 得到$r_s$ $s = t, r_t = r_s$重复迭代 默认将$|t-s|$​是一个定值，以定长的步长进行迭代\n理论上remask策略是随机的\n但是论文给出了两种基于退火的remask策略\n在生成过程中，需要随机性逐步递减，冻结高确定性的部分、把随机性集中在不确定区域\nlow-confidence remasking：取置信度最低的$\\frac{s}{t}L$个预测token进行remask semi-autoregressive remasking：对序列进行分块，从左到右顺序生成 Experiments\r实验1 · Scalability\r验证：LLaDA是否与自回归模型ARM具有相同的可拓展性 目的：证明论文的核心论点：\n​\t理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布\n​\t因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布\n​\t因此指令跟随、上下文学习并不是ARM的专利\n实验设计\r针对MDM和ARM两类语言模型，进行如下控制变量\n模型结构：采样同一套Transformer架构（优化器、参数量……各种机制），只修改了mask 参数量：在1B规模下完全一致，在7B规模由于资源限制有一些不同 Transformer：causal mask 数据：预训练语料相同 唯一的实验的变量：\nFLOPs：使用6ND公式作为横轴 N 是模型的非 embedding 参数量（固定，比如 1B 或 8B） D 是训练过的 token 数量（数据量，可以变化） 实验通过改变 D ，计算出 6 × N × D （即训练 FLOPs）作为横轴的计算预算 实验指标：\nMMLU、ARC-C、CMMLU、PIQA、GSM8K、HumanEval （多任务、推理、中文、物理、数学、代码）\n实验结果\r部分任务体现优势 对于性能稍逊的任务（PIQA），差距也在逐渐缩小 同时喷了先前的一篇工作的结论：达到相同的似然需要16倍算力\n似然是间接指标（LLaDA的lower bound） 先前的工作只有GPT2的参数量，本文提高到7-8B Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024.\n结论：LLaDA 在相同训练规模与算力条件下，表现出与 ARM 相似甚至更强的可扩展性。\n实验2 · Benchmark\r验证：LLaDA经过预训练和SFT之后是否能够和已有的ARM在上下文学习与指令遵循能力上进行竞争 实验设计\r实验对象：LLaDA、一系列参数相当的模型 Base阶段：比较了所有模型的预训练base模型 instruct阶段：LLaDA只进行了SFT，其他模型均完成了SFT+RL 原文：交给未来的工作 任务：通用、数学科学、代码、中文等常见benchmark 实验结果\r在所有任务上超过LLaMA2 7B，与LLaMA3 8B相当\n所有模型的训练数据存在差异 作者认为LLaDA的优势区间与劣势区间的主要原因在数据质量与分布上 GSM8K数据集上体现了显著的优势，论文针对这个情况做了补充实验，证明不存在数据泄露\nSFT数据质量较差，出现了性能下降（MMLU） 没有采用RL，因此性能略微落后LLaMA3 8B 结论：\n在数据集透明度不足的情况下，以丰富的标准化流程、多样化任务，足以证明LLaDA的性能卓越，是唯一具备竞争力的非自回归模型 实验2 · 补充实验\r验证：LLaDA在GSM8K数据集中的优势不来源于数据泄露（data leakage），检测在全新数据集中仍然能保证推理能力\n省流：找了一个2024年的新数据集，模仿GSM8K的形式做一遍实验\nLLaDA在所有难度（解题步骤数）中均显著优势 两类模型随着难度上升准确度逐渐下降，但是LLaDA下降较慢 结论：\nLLaDA允许模型在每一步同时考虑全局 token 关系，因此在多变量方程、层次关系推理中优于单向自回归 实验3 · Reversal Reasoning and Analyses\rReversal Curse（反向诅咒）：ARM从左到右生成序列，因此反向生成或逆序推理的表现很差\n验证：LLaDA是否克服了反向诅咒\n实验设计\r数据：496对著名中文诗句（上下两句），每一句子（A,B）构成两个任务 Forward：给定A预测B Backward：给定B预测A 1 2 窈窕淑女的下一句是什么？直接输出句子即可。 Answer: 君子好逑。 不拘一格降人才的上一句是什么？直接输出句子即可。 Answer: 我劝天公重抖擞。 GPT-4o 和 Qwen2.5 均有更大数据和RL优化，但仍失败\nLLaDA 虽仅 SFT，无RL，仍在 reversal 上大幅领先\n附录补充\r论文从三个角度补充了为什么LLaDA是无方向偏置的模型\n理论证明：LLaDA本质上等价于在所有生成顺序上做平均，从而消除方向偏置 解释为什么 diffusion 结构在数学上是方向对称的 实现机制：理论正确的情况下，需要确保算法实现不出现从左到右 确保生成算法本身不引入方向信息 超参数层面：通过实验说明采样步数与效率不会干扰方向一致性 排除方向性差异由采样精度造成的可能性 A.2 Inference\r目的：证明LLaDA训练和推理目标等价于对所有可能生成顺序的平均建模 对于训练时的核心目标函数：\n$$\rL(\\theta) \\triangleq\r-\\mathbb{E}_{t,x_0,x_t}\\left[\\frac{1}{t}\\sum_{i=1}^L1\\left[ x^i_t=M\\right] \\log{p_\\theta\\left( x_0^i\\mid x_t\\right)}\\right]\r$$ 训练目标是：模型在任意mask模式下，都能预测出原token\n该训练模式不会看到任何固定方向的序列，故天然是双向建模的\n推不动了，pass一下\nRemasking\r反向过程的核心：预测 - 重新掩码 - 继续预测\n上文提到了三种不同的掩码策略\n论文使用GSM8K进行了消融实验\n生成长度固定512 采样步数固定256（长度的一半） block：32 Base模型：最低置信度即可，半自回归是不需要的 Instruct模型：必须是最低置信度+半自回归 单独最低置信度会严重降低性能 论文解释：SFT阶段引入了大量EOS，模型一般会给EOS较大的置信度。因此推理时EOS会被大量生成，并且几乎不可能被remask（置信度非常高）\n因此需要引入半自回归，保证每个块内收敛出连续的内容，抑制EOS早产\n尽管引入半自回归，但是块内仍然是并行的（？）\n补充一下，模型对生成长度这个超参数非常不敏感\n但对采样步数非常敏感（生成长度1024）\n结论：\nLLaDA不受自回归方向性的约束，具有更平衡的前后向建模能力 Case Studies\r附录中展示了一些其他例子，说明生成的对话是出色的（单轮、多轮）\n","date":"2025-09-26T15:46:00+08:00","permalink":"https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"Diffusion Language Model · 论文笔记（一）"},{"content":"[TOC]\n【李宏毅】2025最新的Diffusion Model教程！1小时清楚扩散模型原理，简直不要太爽！人工智能|机器学习|OpenAI_哔哩哔哩_bilibili\nDenoise Model\r第一步，从正态分布中随机一个二维向量（纯噪声）作为最开始的输入 第二步，假设迭代次数为1000，将输入送入Denoise Model进行推理，得到新的图片 不断重复步骤（同时需要输入当前step数字，表示噪声的严重程度，见图片下方） 完成迭代，得到生成的图片 输入与输出\r如果直接让模型输出denoise之后的图片，本质上需要让模型学会生成新的图片，实践证明是一件比较难的事情\n但是我们将模型训练成一个Noise Predicter，推理出图片中的噪音是什么样的\n最后使用图片减去噪音，则可以得到迭代出的图片\nTrain\r数据可以自己造，迭代一定次数，每次生成一个正态分布随机出的噪声\n生成的噪声就是ground truth\nText2Img\r一般来说我们希望通过文字prompt生成想要的图片，而不是让模型自己决定生成什么样的图片\n但其实也很简单，将文字作为输入送入到Denoise Model中即可\nDiffusion图片生成\r主流的图片生成模型Stable Diffusion、DALLE……的Framework基本都差不多\nGeneration Model的输入来自于，最后输出一个类似压缩版本的图片向量 文字经过text encoder得到向量 从正态分布中随机一个初始向量 通过Decoder生成高清图片 Text Encoder\r文字Encoder对最终模型的影响远大于Noise Predictor，不管采用多大的U-Net，似乎效果没什么变\nDecoder\r取决于Framework中的技术路线\nGeneration Model生成一张小图，Decoder进行高清图生成\n找大量图片进行down sample，即可完成数据集制作 Generation Model生成Latent representation（潜在表示）\n使用auto-encoder的方法进行训练 Generation Model\rDiffusion Model 输出：图片 Generation Model 输出：Latent Representation 同样会使用Forward Process进行数据生成\n因此随机的noise应该加在Latent Representation上\n数学推导\r优化目标\r图像生成的本质：\n从一个高斯中随机出的向量$z$，通过生成网络$G(z)$得到的分布$x$，希望尽可能接近真实\n文字等其他信息作为condition（本质上是条件概率） 记模型的参数为$\\theta$，则模型的概率分布为$P_\\theta{(x)}$，真实数据的分布为$P_{data}{(x)}$\n$$\rSample \\left\\{ x^1, x^2, ..., x^m\\right \\} \\space from \\space P_{data}(x)\\\\\r\\theta^*=\\arg \\max_\\theta \\prod_{i=1}^{m}P_\\theta{(x^i)}\r$$ 希望模型能以尽可能高的概率生成我们观察到的数据\n$$\r\\theta^*=\\arg \\max_\\theta \\sum_{i=1}^{m}\\log P_\\theta{(x^i)} =\\arg \\max_\\theta \\frac{1}{m}\\sum_{i=1}^{m}\\log P_\\theta{(x^i)}\r$$ 只要我们从真实数据中采样的数据量$m$​足够大，就可以用真实数据的期望，代替样本的均值\n$$\r\\theta^*=\\arg \\max_\\theta \\mathbb{E}_{x\\sim P_{data}}\\left [ \\log P_\\theta(x) \\right]\r$$转换成积分形式：\n$$\r\\theta^*=\\arg \\max_\\theta \\int P_{data}(x)\\log P_\\theta(x) dx\r$$ 概率乘上数值\nKL散度\r有两个概率分布：\nP分布：代表真实的情况或我们观察到的数据分布。 Q分布：代表我们模型预测的分布、一个近似分布 现在的问题是：如果我们使用Q分布来编码来自P分布的数据，会犯多大的“错误”？会损失多少信息？\n使用KL散度进行表示\nKL散度是不对称的\n用Q近似P的损失 不等于 用P近似Q的损失 对于离散型变量：\n$$\rD_{KL}(P\\mid\\mid Q) = \\sum_i P(i)\\log (\\frac{P(i)}{Q(i)})\r$$ $P(i)$是事件$i$在真实分布$P$中发生的概率。 $Q(i)$是事件$i$在近似分布$Q$中发生的概率。 对于连续型变量：\n$$\rD_{KL}(P\\mid\\mid Q) = \\int_{-\\infty}^{\\infty}p(x)\\log (\\frac{p(x)}{q(x)})dx\r$$ 这里使用的是概率密度函数\n$$\r\\int P_{data}(x)\\log P_\\theta(x) dx = \\int P_{data}(x)\\log P_\\theta{(x)}dx - \\int P_{data}(x) \\log P_{data}(x)dx + \\int P_{data}(x) \\log P_{data}(x)dx \\\\\r= \\int P_{data}(x)\\log \\frac{P_\\theta(x)}{P_{data}(x)}dx + \\int P_{data}(x) \\log P_{data}(x)dx\r$$ 第二项是与$\\theta$无关的一项（常数），这样不影响$\\theta$​优化，因此可以直接舍弃\n$$\r\\theta^* = \\arg \\max_\\theta \\int P_{data}(x)\\log \\frac{P_\\theta(x)}{P_{data}(x)}dx\\\\\r= \\arg \\max_\\theta \\left [ -\\int P_{data}(x)\\log \\frac{P_{data}(x)}{P_\\theta(x)}dx\\right ]\\\\\r= \\arg \\min_\\theta \\left [ \\int P_{data}(x)\\log \\frac{P_{data}(x)}{P_\\theta(x)}dx\\right ]\\\\\r= \\arg \\min_\\theta D_{KL}(P_{data}\\mid\\mid P_\\theta)\r$$Denoising Diffusion Probabilistic Models\r那么如何计算$P_\\theta(x)$​呢？对于DDPM，图片是逐渐Denoise得到的\n我们假设最后得到的干净图片是$x_0$，一开始的随机噪声是$x_T \\sim \\mathcal{N}(0,I)$​\n先补充一些数学\n马尔可夫性质（离散时间）\r对于一个随机过程$\\left { X_t \\right }_{t\\in T}$，$T$是离散的时间序列$\\left { 0,1,2,\u0026hellip;\\right}$\n若对于任意时间点$t$，满足：\n$$\rP(X_t=x\\mid X_{t-1}=x_{t-1},...,X_1=x_1) = P(X_t=x\\mid X_{t-1}=x_{t-1})\r$$ 也就是说，给定当前状态，未来状态与过去状态条件独立。这就是“无记忆性”。\n重参数化技巧\r对于深度学习过程，若途中从高斯中进行采样，是不可导的，无法进行反向传播\n1 x = torch.normal(mean=mu, std=sigma) x的产生是随机的，无法写出关于参数的函数，无法反向传播到mu和sigma\n因此我们希望引入一个具体的表达方式，方便求导\n可以采用的是：\n$$\rx = \\mu + \\sigma \\odot \\epsilon\r$$ 其中$\\epsilon \\sim \\mathcal{N} (0,I)$，$\\odot$表示逐元素乘法，$I$是单位矩阵\n1 2 3 4 5 6 7 8 9 A = [[1, 2], [3, 4]] B = [[5, 6], [7, 8]] A ⊙ B = [[1*5, 2*6], [3*7, 4*8]] = [[5, 12], [21, 32]] 我们只需要学习$\\mu,\\sigma$，而$\\epsilon$作为随机噪声，在单次的传播过程中是固定的，不影响求导\n$$\r\\mu = \\left [\\mu_1, \\mu_2, ..., \\mu_d\\right]\\\\\r\\sigma = \\left [\\sigma_1, \\sigma_2, ..., \\sigma_d\\right]\\\\\r\\epsilon = \\left [\\epsilon_1, \\epsilon_2, ..., \\epsilon_d\\right],\\epsilon \\sim \\mathcal{N}(0,1)\r$$因此\n$$\rx = \\mu + \\sigma \\odot \\epsilon\\\\\rx_i = \\mu_i + \\sigma_i \\cdot \\epsilon_i,\\forall i\\\\\r\\therefore x_i\\sim \\mathcal{N}(\\mu_i,\\sigma^2_i)\r$$ 各维度是独立的\n分布\r假设你想知道\u0026quot;今天是否下雨了\u0026quot;（未知事物），但你无法直接看窗外。\n先验分布：在没有任何证据时，你根据历史数据猜测\u0026quot;今天下雨的概率是20%\u0026quot; 证据：你听到有人说\u0026quot;地上是湿的\u0026quot; 后验分布：在知道\u0026quot;地上是湿的\u0026quot;这个证据后，你更新对\u0026quot;今天下雨\u0026quot;的概率判断（比如变成70%） 正态分布（高斯分布）\r性质1：均值与方差的线性组合 若$z = ay+b$，且$y\\sim \\mathcal{N}(\\mu, \\sigma^2)$，其他均为常数\n则：\n$$\rz \\sim \\mathcal{N}(a\\mu + b,a^2\\sigma^2 )\r$$ 性质2：独立正态分布的和 若$y_1,y_2$都是独立的正态分布变量，则他们的和$z=y_1+y_2$也是正态分布\n$$\rz\\sim \\mathcal{N}(\\mu_1+\\mu_2, \\sigma^2_1+\\sigma_2^2)\r$$ 加噪\r加噪是一个正向过程\n从数据集的原始图片$x_0$出发，最终需要得到$x_T \\sim \\mathcal{N}(0,I)$\n如果每一步按照前文所说进行简单加噪\n$$\rx_t =x_{t-1} + \\epsilon_t, \\quad \\epsilon_t\\sim \\mathcal{N}(0,\\sigma^2I)\r$$ 其中$\\sigma$可以让我们自己定义\n故随着时间累计：\n$$\rx_t = x_0 + \\sum_{i=1}^t\\epsilon_i\r$$ $\\epsilon_i$是独立同分布的，故\n$$\rx_t \\sim \\mathcal{N}\\left ( x_0, t\\sigma^2I \\right)\r$$ 这样无法保证最后$x^T$是一个标准正态分布\n且随着$t$增加，数值的不稳定性会上升\n加噪目标：\n最终是标准正态分布的纯噪声 加噪可控、平滑过渡 数值稳定 设计：\n$$\rx_t = \\sqrt{1-\\beta_t}\\cdot x_{t-1} + \\sqrt{\\beta_t}\\cdot \\epsilon_t\r$$ 其中$\\beta_t$是预先设定的缩放系数，在$(0,1)$之间\n从原理上能够解释为：衰减旧图片，添加新噪声\n推导一下方差：\n$$\r\\text{Var}(x_t) = \\text{Var}(\\sqrt{1-\\beta_t}\\cdot x_{t-1} + \\sqrt{\\beta_t}\\cdot \\epsilon_t)\\\\\r= (1-\\beta_t)\\text{Var}(x_{t-1}) + \\beta_t\\text{Var}(\\epsilon_t)\r$$ 如果$x_{t-1}$之前的方差都是按照某种方式保持的很好，方差为1\n同时我们知道$\\epsilon_t$是标准正态分布，方差也是1\n$$\r\\text{Var}(x_t)= (1-\\beta_t)\\text{Var}(x_{t-1}) + \\beta_t\\text{Var}(\\epsilon_t) = 1-\\beta_t + \\beta_t = 1\r$$ 所以后续按照此方法，能够始终保持数值稳定\n高效加噪\r一般迭代次数还是足够多的， 如果使用循环就会很慢\n我们可以对公式进行展开：\n迭代1次 $$\rx_1 = \\sqrt{1-\\beta_1}\\cdot x_0 + \\sqrt{\\beta_1}\\cdot \\epsilon_1\r$$ 迭代2次 $$\rx_2 = \\sqrt{1-\\beta_2}\\cdot x_1 + \\sqrt{\\beta_2}\\cdot \\epsilon_2 \\\\\r= \\sqrt{1-\\beta_2}\\cdot (\\sqrt{1-\\beta_1}\\cdot x_0 + \\sqrt{\\beta_1}\\cdot \\epsilon_1) + \\sqrt{\\beta_2}\\cdot \\epsilon_2 \\\\\r= \\sqrt{(1-\\beta_1)(1-\\beta_2)}x_0 + \\sqrt{(1-\\beta_2)\\beta_1} \\cdot \\epsilon_1 + \\sqrt{\\beta_2} \\cdot \\epsilon_2\r$$ 迭代3次 $$\rx_3 = \\sqrt{(1 - \\beta_3)(1 - \\beta_2)(1 - \\beta_1)} \\cdot x_0 + \\sqrt{(1 - \\beta_3)(1 - \\beta_2)\\beta_1} \\cdot \\epsilon_1 + \\sqrt{(1 - \\beta_3)\\beta_2} \\cdot \\epsilon_2 + \\sqrt{\\beta_3} \\cdot \\epsilon_3\r$$令\n$$\r\\alpha_i = 1-\\beta_i,\\quad \\overline{\\alpha}_t = \\prod_{i=1}^{t}\\alpha_i $$ $x_0$的系数：$\\sqrt{\\overline{\\alpha}_t}$ $\\epsilon_i$的系数：$\\sqrt{\\beta_i\\prod_{j=i+1}^t \\alpha_j} = \\sqrt{\\beta_i\\cdot\\frac{\\overline{\\alpha}_t}{\\overline{\\alpha}_i}}$ 得：\n$$\rx_t = \\sqrt{\\overline{\\alpha}_t}x_0 + \\sum_{i=1}^t\\sqrt{\\beta_i\\cdot\\frac{\\overline{\\alpha}_t}{\\overline{\\alpha}_i}}\\cdot \\epsilon_i\r$$ 左边是定值，右边显然服从某个正态分布\n并且方差完全取决于系数，令\n$$\r\\sigma^2_t=\\sum_{i=1}^t\\beta_i\\cdot\\frac{\\overline{\\alpha}_t}{\\overline{\\alpha}_i} = \\sum_{i=1}^t\\beta_i\\prod_{k=i+1}^t(1-\\beta_k)\r$$此时有：$x_t\\sim \\mathcal{N}(0,\\sigma^2_tI)$\n这里有点跳跃，我们希望之间使用一个更简单的表示替换掉这个\n如果我们能够找到一个方差相等的一项，就可以替换\n这里智慧的假设是\n$$\r1-\\overline{\\alpha}_t\r$$ 使用数学归纳法进行证明，当 $t=2$​时：\n$$\r1-\\overline{\\alpha}_t = 1-(1-\\beta_1)(1-\\beta_2) = \\beta_1+\\beta_2-\\beta_1\\beta_2\\\\\r\\sigma^2=\\beta_1(1-\\beta_2) + \\beta_2 = \\beta_1+\\beta_2-\\beta_1\\beta_2\r$$ 两者相等\n推导两者的递推关系\n前者：\n$$\r1-\\overline{\\alpha}_t = 1 - \\overline{\\alpha}_{t-1}\\cdot(1-\\beta_t) = (1-\\beta_t)(1-\\overline{\\alpha}_{t-1}) + \\beta_t\r$$ 后者：\n$$\r\\sigma_t^2 =\\sum_{i=1}^t\\beta_i\\prod_{k=i+1}^t(1-\\beta_k) \\\\\r= \\sum_{i=1}^{t-1}\\beta_i(1-\\beta_t)\\prod_{k=i+1}^{t-1}(1-\\beta_k) + \\beta_t \\\\\r= (1-\\beta_t)\\sum_{i=1}^{t-1}\\beta_i\\prod_{k=i+1}^{t-1}(1-\\beta_k) + \\beta_t \\\\\r= (1-\\beta_t)\\sigma_{t-1}^2+\\beta_t\r$$两者递推关系相同，故有：\n$$\r\\sigma^2_t = 1-\\overline{\\alpha}_t\r$$ 原噪声项满足\n$$\r\\sum_{i=1}^t\\sqrt{\\beta_i\\cdot\\frac{\\overline{\\alpha}_t}{\\overline{\\alpha}_i}}\\cdot \\epsilon_i \\sim \\mathcal{N}(0,(1-\\overline{\\alpha}_t)I)\r$$ 替换成满足同一个分布的噪声项\n$$\r\\sqrt{ 1-\\overline{\\alpha}_t}\\cdot \\epsilon\r$$ 依旧可以使得$x_t \\sim \\mathcal{N}(0, (1-\\overline{\\alpha}_t)I)$\n所以完全可以写成：\n$$\rx_t = \\sqrt{\\overline{\\alpha}_t}x_0 +\\sqrt{ 1-\\overline{\\alpha}_t}\\cdot \\epsilon, \\quad \\epsilon\\sim\\mathcal{N}(0,I)\r$$至此，我们可以通过该公式直接从一个干净的$x_0$得到一个加噪图片$x_t$，极大简化了训练过程\n并且该式符合重参数化的\n为什么上面的公式是$\\epsilon$？\n由于我们保证数值稳定，因此本质上是加上关于$\\epsilon_i$的一个线性组合（并且线性组合系数权重始终为1），因此等价于只加上一次的标准正态分布\n去噪\r去噪是训练的核心步骤与目标\n定义真实的逆向分布：\n$$\rq\\left(x_{t-1}\\mid x_t\\right)\r$$ 由于上文所提到的运算关系，计算$q(x_t\\mid x_{t-1})$​是没问题的\n这里我们打断一下，不妨推导一下$q(x_t\\mid x_{t-1})$的分布\n首先有如下关系：\n$$\rx_t = \\sqrt{1-\\beta_t}\\cdot x_{t-1} + \\sqrt{\\beta_t}\\cdot \\epsilon_t\r$$ 则有（已知$x_{t-1}$的情况推$x_t$，故可以把$x_{t-1}$看作定值，且$\\epsilon \\sim \\mathcal{N}(0,I)$）：\n$$\r\\mathbb{E}(x_t\\mid x_{t-1}) = \\sqrt{1-\\beta_t}\\mathbb{E}(x_{t-1}) + \\sqrt{\\beta_t}\\mathbb{E}(\\epsilon_t) = \\sqrt{1-\\beta_t}\\cdot x_{t-1} \\\\\r\\text{Var}(x_t\\mid x_{t-1}) = \\beta_tI\r$$ 故：\n$$\rq(x_t\\mid x_{t-1})\\sim \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}\\cdot x_{t-1}, \\beta_tI)\r$$ 补充解释一下这个诡异的分号，其代表该分布是关于$x_t$的分布，即只有$x_t$是随机变量\n？？？？？？\n但是反过来计算逆向分布是一件非常难的事情\n我们需要学一个模型，定义为：\n$$\rp_\\theta\\left(x_{t-1}|x_t\\right)\r$$","date":"2025-09-22T20:32:34+08:00","permalink":"https://example.com/p/diffusion-model-%E6%9D%8E%E5%AE%8F%E6%AF%852023/","title":"Diffusion Model · 李宏毅2023"},{"content":"Colab SSH\r在colab中新建一个init.sh\n1 pip install colab_ssh --upgrade 在notebook中：\n1 2 # 初始化colab ssh !bash /content/drive/MyDrive/ssh/init.sh 然后挂载一下云盘\n1 2 3 # 挂载云盘 from google.colab import drive drive.mount(\u0026#39;/content/drive/\u0026#39;) 启动colab-ssh\n1 2 3 # Install colab_ssh on google colab from colab_ssh import launch_ssh_cloudflared, init_git_cloudflared launch_ssh_cloudflared(password=\u0026#34;colab_passwd\u0026#34;) #这里改成你自己的密码 点击\n然后下载对应的客户端\n然后将这个程序存放到本地中的某个路径，假设是V:\\colab\\cloudflared-windows-amd64.exe\nVS code\r首先确保安装了插件Remote - SSH\n配置文件按colab给定的内容写，然后把路径替换，注意是绝对路径\n1 2 3 4 5 Host *.trycloudflare.com HostName %h User root Port 22 ProxyCommand V:\\colab\\cloudflared-windows-amd64.exe access ssh --hostname %h 然后复制这一串\n作为连接命令去使用remote连一下就好了\n","date":"2025-09-08T16:18:42+08:00","permalink":"https://example.com/p/colab-ssh%E6%8C%87%E5%8D%97/","title":"Colab SSH指南"},{"content":"[TOC]\nUnifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors | alphaXiv\nkaushal0494/UnifyingAITutorEvaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors\nPaper-Cool\rQ1: 这篇论文试图解决什么问题？\r这篇论文试图解决的问题是如何有效地评估大型语言模型（LLMs）作为AI助教在教育对话中的表现，特别是在学生错误纠正方面。具体来说，论文试图解决以下几个问题：\n评估标准的统一性：之前对AI助教的评估工作受限于主观协议和基准，缺乏统一的评估框架。论文提出了一个基于学习科学原则的统一评估分类体系，包含八个教学维度，旨在评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。 评估的客观性和可靠性：现有的评估方法往往依赖于主观评价，缺乏客观性和可比性。论文通过提出一个新的评估基准MRBench，并提供人工标注的八个教学维度，来提高评估的客观性和可靠性。 AI助教的实际教学能力：论文通过对比不同LLMs作为AI助教的能力和人类助教的表现，揭示了当前LLMs在教学能力上的不足，并指出了改进的方向。 评估过程的简化和标准化：论文希望通过提出的分类体系、基准和人工标注的标签，简化AI助教的评估过程，并帮助跟踪AI助教发展的进步。 总的来说，这篇论文旨在通过提出一个新的评估框架和基准，来解决当前AI助教评估中存在的问题，推动AI助教的发展，并为未来的研究提供便利。\nQ2: 有哪些相关研究？\r相关研究涵盖了以下几个领域：\nAI助教评估： Tack和Piech (2022) 以及 Tack 等人 (2023) 从教师语言、学生理解和帮助学生的角度评估模型的响应。 Macina 等人 (2023) 专注于连贯性、正确性和公平教学来评估模型作为教师的响应。 Wang 等人 (2024a) 评估了有用性、关怀和人类化。 Daheim 等人 (2024) 集中于针对性、正确性和可执行性来评估教师响应的质量。 自然语言生成（NLG）和LLM基础评估： 一般领域的NLG指标，如BLEU (Papineni et al., 2002)、BERTScore (Lin, 2004) 和 DialogRPT (Gao et al., 2020) 等，被用作衡量AI助教响应的连贯性和类人性，但这些指标不考虑教学价值，且常需要真实答案来评估匹配响应。 面向教学的评估： 传统学习科学中的评估方法主要设计用于评估人类教师，由于缺乏自我报告，不易直接应用于AI教师。 研究人员考虑不同的教学维度及其相关定义 (Wollny et al., 2021; Tack et al., 2023; Denny et al., 2024)。 学生错误纠正任务： 专注于数学领域的教育对话，特别是学生的失误或困惑，AI教师的目标是回应以纠正这些错误或困惑。 这些相关研究提供了评估AI助教的不同视角和方法，但存在评估方案和定义的差异，缺乏标准化，使得跟踪现有AI教师的进步和实际性能变得复杂。这篇论文提出的评估分类体系旨在统一这些评估方法，并提供一个基于学习科学原则的评估框架。\nQ3: 论文如何解决这个问题？\r论文通过以下几个步骤解决AI助教评估的问题：\n提出统一评估分类体系： 论文提出了一个基于学习科学原则的统一评估分类体系，包含八个教学维度，用于评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。 构建评估基准MRBench： 论文编译并发布了一个新的评估基准MRBench，该基准包含来自两个公共数据集的192个对话和1596个响应，这些响应来自七个最先进的基于LLM的和人类助教，并提供了八个教学维度的黄金标注。 人类和LLM评估： 利用部分对话历史，展示了学生的失误，生成了七个最先进的LLM作为助教的响应，并通过人类和LLM基础评估来评估这些模型的教学能力。 分析LLM作为评估者的可靠性： 论文探讨了LLM作为评估者的可靠性，通过将LLM生成的评估与人类判断进行相关性分析，指出LLM在复杂教学维度上的评估是不可靠的。 讨论改进方向： 论文基于人类评估指出了LLMs在响应中表现出的特定教学能力的不足，并讨论了改进AI助教教学能力的最相关方向。 资源公开： 论文承诺将公开分类体系、基准和人类标注，以促进未来在这一重要领域的研究。 通过这些步骤，论文旨在简化AI助教的评估过程，并帮助跟踪AI助教发展的进步。此外，该研究还为创建和标注可用于强化学习和微调的数据集提供了可能性，帮助未来的AI助教与人类和教学价值观保持一致。\nQ4: 论文做了哪些实验？\r论文中进行了一系列实验来验证提出的评估分类体系的有效性，并评估不同LLM作为AI助教的教学能力。以下是主要的实验步骤：\n评估分类体系验证： 进行了验证实验，以确定提出的八个评估维度是否足够且无冗余。实验包括四名注释者对八个随机抽样的对话进行独立标注，并计算Fleiss\u0026rsquo; kappa值来衡量注释者间的一致性，结果表明有实质性的一致性。 基准准备： 从Bridge和MathDial数据集中编译了MRBench评估基准，并为192个实例生成了基于七个最先进的LLM的响应。 人类标注： 四名训练有素的标注者使用验证过的分类体系对MRBench进行了标注，每个标注者负责48个对话，总共192个实例被标注，其中40个实例由两名标注者独立标注，以计算标注者间的一致性，平均Cohen\u0026rsquo;s kappa得分为0.71，表明有实质性的一致性。 LLM基础标注： 使用Prometheus2 LLM作为评估者对MRBench进行标注，以评估LLM作为评估者的可靠性。 评估指标： 使用期望标注匹配率（DAMR）和标注相关性（AC）两个关键指标来定量评估LLM和人类教师的教学效果，并进行比较分析。 关键发现： 基于人类和LLM对MRBench的评估，总结了不同LLM作为AI助教的教学能力的关键发现，并讨论了LLM作为评估者的可靠性。 这些实验步骤不仅验证了提出的评估分类体系的有效性，而且提供了对不同LLM在教学能力方面的深入分析，并探讨了LLM作为评估者的可行性。通过这些实验，论文展示了当前LLM作为AI助教的能力和局限性，并指出了未来改进的方向。\nQ5: 有什么可以进一步探索的点？\r论文在最后部分提出了一些可以进一步探索的点，这些包括：\n扩展到其他任务和学科领域： 论文提出的评估分类体系主要集中在数学领域的学生错误纠正任务上。未来的研究可以验证并可能适应这个分类体系，以应用于其他任务（如概念学习）和学科（如科学、语言学习等）。 考虑学生视角： 目前的分类体系和标注方案关注于教师响应的适当性，但未考虑辅导对话对学生学习过程和学习成果的整体影响。未来的研究可以将评估扩展到对话层面，以更好地评估AI教师的教学能力对学生学习的影响。 使用其他LLM作为评估者： 论文中限制了LLM基础评估到Prometheus2 LLM作为批评者的使用。未来的研究可以探索更多先进的LLM作为批评者，并尝试使用多种提示模板。 创建和标注数据集： 论文提到，创建和标注可用于强化学习和微调的数据集，可以帮助未来的AI教师与人类和教学价值观保持一致。这是一个重要的研究方向，可以促进AI教师的发展。 改进LLM的教学能力： 论文的研究发现，即使是最先进的LLM也缺乏一些有效的教学能力。未来的研究可以探索如何改进LLMs，使其更符合教学的有效性。 评估LLM生成内容的伦理和风险： 考虑到LLM可能生成事实上不正确、无意义或甚至冒犯的内容，未来的研究需要考虑评估LLM在教育领域使用时的伦理风险，并探索如何减轻这些问题。 这些探索点为AI助教评估和开发的未来研究提供了明确的方向，并强调了需要进一步工作以提高AI教师的教学效果和可靠性。\nQ6: 总结一下论文的主要内容\r这篇论文的主要内容包括以下几个方面：\n研究动机： 论文探讨了当前最先进的大型语言模型（LLMs）是否能够作为有效的AI助教，并展示必要的教学能力，特别是在教育对话中纠正学生错误和困惑的场景。 评估分类体系： 提出了一个统一的评估分类体系，包含八个基于学习科学原则的教学维度，用于评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。 评估基准MRBench： 发布了一个新的评估基准MRBench，包含192个对话和1596个响应，这些响应来自七个最先进的基于LLM的和人类助教，并提供了八个教学维度的黄金标注。 人类和LLM评估： 通过人类和LLM基础评估，分析了不同LLM作为AI助教的教学能力，并讨论了LLM作为评估者的可靠性。 研究发现： 发现尽管最先进的LLMs如GPT-4在回答问题方面表现良好，但作为教师时往往不够胜任，特别是在不立即透露答案和提供指导方面。 Llama-3.1-405B在评估中表现最好，而GPT-4作为教师时太快透露答案，Gemini的响应不够连贯，Sonnet虽然注重人类化和鼓励性，但在其他维度上效果较差，Phi3的表现最差。 结论和未来工作： 论文得出结论，当前的LLMs作为AI教师还不够好，有很大的改进空间，并指出了最相关的改进方向。 论文希望发布的资源能够简化评估过程，并帮助跟踪AI教师的发展。 局限性和伦理声明： 论文承认了研究的局限性，并强调了LLM输出可能带来的风险，尤其是在教育领域可能误导学生和传播偏见的问题。 这篇论文为AI助教的评估提供了一个统一的框架和基准，并通过对现有技术的评估，揭示了AI助教在教学能力上的不足和未来改进的方向。\n","date":"2025-09-06T20:45:55+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"大语言模型教育纠错 · 论文笔记（三）"},{"content":"[TOC]\n李宏毅机器学习2025\nTransformer\r","date":"2025-09-06T17:39:34+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02025-transformer/","title":"李宏毅机器学习2025 · Transformer"},{"content":"[TOC]\n李宏毅机器学习2025\nAgent\r传统AI：给定明确步骤、指令，AI完成任务\nAgent：给定目标，由Agent想办法完成\nAgent将通过观察Environment，采取特定的Action\n强化学习：通过强化学习方法得到的Agent是可行的，但是不具备通用能力（围棋Agent不能处理五子棋） LLM：通过文字描述进行交互，具备通用能力 Goal、Environment1、Action1、Environment2、Action2……\n本质上也是在接龙\n回合制的交互会比较好做，有时候会需要被实时打断\n即：Action执行时，Environment变化，会需要Agent中断Action，进行新的Action\n常见应用：语音聊天\nMemory\r交互的次数足够多时，记忆量过大，会造成Agent性能下降\n因此针对相关经验做一些记忆的检索和筛选是必要的\n可以直接套RAG的技术\n但这里最好不要提供模型过去的错误例子\n这里的情景似乎没有做一些纠错任务，给了错误的例子性能会发生下降\n因此设计Agent时需要考虑一下哪些内容是应该提供或筛除的\n模型的使用技巧：\n告诉模型应该做什么比告诉模型不要做什么效果更好 从存储角度出发\n有些记忆没有存储的必要，因此可以引入一个Write模型去分类筛选\n有些记忆可以被格式化、转化成更好、更通用的内容，可以引入Reflection模块做转化，存储到合适的载体中，方便Read去做RAG\nFunction Call\r通常会把调用方法、工具列表放在System Prompt中\n让用户通过User Prompt进行交互\nFunction过多时，可以参考上述的Memory方法去做选择 Agent也可以自己做一个Function，放入Memory中 Agent有时候会过度相信工具\n因此需要看看模型自己是否有辨别的能力（室温10000°？不对，这里是工具出错了）\n但其实加一个Reflection也不错\n课堂中探索了哪些信息是容易被模型采纳的\n当原始上下文逐渐被不切实际的值修改时，LLM（大语言模型）会越来越多地回归到其先验知识 LLM坚持遵循上下文中检索到的信息的可能性，与其在没有上下文时对自身回复的信心呈负相关。 省流：\n外部知识如果和模型知识差距越大，模型会对模型知识更有信心；差距越小，模型更愿意相信外部知识 模型对模型知识的likelihood越大，对外部知识的likelihood越小 AI和人类分别给出两个意见不同的文章，AI倾向于相信AI\nEx单独抽取了AI回答错误的例子（排除AI与AI回答类似，造成偏好的情况），但仍然是AI更相信AI 具体原因未知，猜测是AI的文章结构、表达上比人类更好\n2401.11911\n（这里首先都是用了AI生成的文章做实验，避免偏好问题，并且文章都是假的）\nMeta Data会影响模型的采纳 其中时间影响较大 资料来源写Wikipedia还是其他来源，似乎没有什么影响（这里比较反直觉） 但是实验做得似乎比较粗糙，看看就好\n总结：\n模型总会犯错 Function Call要不要采用取决模型本身能力，如果模型可以自己解决没必要Call Plan\r目前的Agent都喜欢做一个Plan，再开始Action\n但是Plan不能定太死\n操作浏览器时突然出现一个广告弹窗\n因此Plan需要灵活，一种方案是：每次思考一下Plan要不要重新制定\n如何强化模型的规划能力？\n让模型实际去探索一下（本质是搜索） 可以剪枝（自问自答：当前还有机会完成任务吗？） （不适合不容易回溯状态的任务，例如：订餐）\n（但是可以引入一个World Model，让模型扮演环境本身去做反馈，模拟）\n从Agent的角度去看待模型Thinking Mode：\n一些杂谈：\n做benchmark或一些实验的时候，思考一下这个任务LLM会不会在互联网数据中提前得到 ","date":"2025-09-03T17:13:34+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02025-agent/","title":"李宏毅机器学习2025 · Agent"},{"content":"[TOC]\n李宏毅机器学习2025\n被手搓大模型橄榄了，写BPE写的心态爆了\n不如我们先停下来推一下一些比较有趣的课程\n前言\rThose token could be anything.\n解析任何事物为若干有限的基本单位（token），你就可以使用生成式AI做任何事情\nAuto Regressive Generation\r自回归生成（其实就是词语接龙） $$\rx_1,x_2,...,x_j \\to y_1\\\\\rx_1,x_2,...,x_j,y_1 \\to y_2\\\\\rx_1,x_2,...,x_j,y_1,y_2 \\to ...\\\\\rx_1,x_2,...,x_j,y_1,y_2,... \\to \\text{end token}\\\\\r$$token作为文字时：语言模型\n（但是不管是什么都会被称为语言模型，因为热度太大了）\n本质上都是在有限的选择中做出选择完成输出\n通过Neural Network，得到的是各个token的概率分布 模型架构（超参数）：由人类确定 模型参数：由数据决定 Thinking Mode\r对于现实中的问题，往往足够复杂，哪怕模型足够巨大，层数足够多，可能也无法处理\n而带有思考能力的LLM表现良好，可以从模型层数的角度进行解释\n每次给定输入token集合，产生一个token的输出，模型会过一遍所有的Layer\n所以只要不断思考，本质上是一直在重复这个模型Layer的堆积\n因此思考长度足够，似乎是在使用一个巨深的模型进行推理\n训练时缩放（Training Time Scaling）：通过训练来让模型变得更强大。这需要巨大的成本重新训练模型。\n增加模型参数量（scale up）\n增加训练数据\n延长训练时间（增加计算量）\n测试时缩放（Testing Time Scaling）：模型已经训练好了，参数固定不变。 我们通过一些“技巧”，在使用这个模型的时候投入更多的计算资源\n生成多个答案然后挑最好的\n更仔细地推理\nTesting Time Scaling：在不改变模型本身weights的情况下，仅通过改变inference或testing时的方法和计算量，就能显著提升模型性能的一种现象或技术集合。 本质上是在叠加模型层数，如图，思考的token开销越多，性能确实越好\n如何控制token？一个粗暴的方法，在结束的时候把end变成wait\nDevelopment\r模型的演变经历了专用模型到通用模型的趋势\n而通用模型的演变也非常迅速\nEncoder：将文本encode成向量 配套专用模型完成输出 架构不同 参数不同 Fine-Tune：通过微调适配不同任务 架构相同 参数不同 Prompt：直接给指令做不同任务 架构相同 参数相同 Homework1就不做了，一个RAG任务，之前做过类似的\n","date":"2025-09-02T20:46:34+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02025-%E5%89%8D%E8%A8%80/","title":"李宏毅机器学习2025 · 前言"},{"content":"2406.19949 Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring\n[TOC]\nIntro\r存在的问题：\n基于分类器的黑盒方法虽然准确，但无法提供解释性。\n现有生成评分理由的方法（如AERA框架(2305.12962 Distilling ChatGPT for Explainable Automated Student Answer Assessment）存在以下问题：\n评分准确性不如分类器方法。 生成的评分理由可能不忠实于学生答案或评分标准。 paper贡献\n提出一种新框架，通过模仿人类评分过程生成更忠实的评分理由，同时匹配或超越分类器方法的评分性能。 paper方法\n模仿人类评分过程\n使用大语言模型（LLM）生成“思维树”（Thought Tree），将评分任务分解为中间决策步骤。 每条树路径代表一个评分决策序列，最终汇总为评分理由。 合成数据生成\n从思维树路径中提取合成评分理由和偏好数据。 通过两阶段训练校准LLM： 监督微调（SFT）：使用合成的评分理由数据。 偏好优化（DPO）：使用合成的偏好数据，提升评分理由的准确性和忠实性。 paper贡献\n提出通过思维树生成更忠实的评分理由的方法。 开发基于思维树路径正确性的合成偏好数据生成技术。 实验表明，框架在QWK分数上比现有方法提升38%，同时生成更高质量的评分理由。 Framework\rProblem Set Up\r$$\rD = \\left \\{ (x_i, y_i)\\right \\}\r$$ 表示学生$i$对某道题目的答案与得分\n对于一道题目，可以划分出$M$个关键得分点$K = \\left {k_j\\right }$\n$$\rv(x_i, K)\r$$ 该向量的第$j$维度若为1，则表示$x_i$成功回答了$k_j$，否则没有成功回答\n独热向量 (One-hot Vector)：用于表示一个样本只属于一个类别的情况。例如，如果一个动物只能是“- 猫”或“狗”中的一种，那么“猫”可能表示为 [1, 0]，“狗”表示为 [0, 1]。向量中只有一个 1。\n多热向量 (Multi-hot Vector)：用于表示一个样本可以同时属于多个类别的情况。例如，一个人既是“学生”又是“运动员”，那么可能表示为 [1, 1, 0]（假设第一个位置是学生，第二个是运动员）。向量中可以有多个 1\n$$\ry_i = f_r(v(x_i,K))\r$$ 省流：问题的关键在如何判断$x_i$正确回答了$k_j$，记为$1_{x_i}(k_j)$，是一个二分类任务\nStage 1: Imitate Human Assessment Process via Thought Trees\r$$\rz_j^{(t)} = \\text{LLM}_\\theta(x_i, k_j), \\quad t=1,2,...,n,\\ \\forall k_j \\in K\r$$ $z_j^{(t)}$ 表示第 $t$ 次采样的决策\n我们将这些判定结果汇总，得到如下的平均决策概率：\n$$\rP(z_j^{\\text{Yes}}) = \\frac{|\\{t : z_j^{(t)} = 1\\}|}{n},\\quad\rP(z_j^{\\text{No}}) = \\frac{|\\{t : z_j^{(t)} = 0\\}|}{n}\r$$ 为了后续聚合，我们为每个关键要素生成简洁的解释性理由 $r_j$，例如：\n$$\rr_j = \\text{LLM}_\\theta(x_i, k_j, z_j)\r$$ 一旦所有关键要素评估完成，我们便能根据每一组决策 $\\mathbf{Z}$ 构造路径。假设总共 $d$ 条路径（最多 $2^{M-1}$ 条），每条路径表示一种判定组合：\n$$\r\\text{path}_l = \\hat{\\mathbf{v}}(\\mathbf{Z}),\\quad l=1,2,...,d\r$$其中，$\\hat{\\mathbf{v}}$ 是对向量 $\\mathbf{v}$ 的估计，表示关键要素是否被覆盖；$\\mathbf{Z}$ 是判定集合（由上面的判定概率组成）。路径的概率等于该路径上每个判定概率的乘积：\n$$\rP(\\text{path}_l) = \\prod_{j=1}^{M} P(z_j)\r$$ 我们利用打分函数 $f_r$（例如程序化的rubric规则）对每条路径打分，获得预测得分：\n$$\r\\hat{y}_{\\text{path}_l} = f_r(\\text{path}_l)\r$$ 省流：蒙特卡洛树，从中抽取所有路径，计算概率和对应的分数，选择概率最高的\n最后，选取概率最高的路径作为最终思维树输出结果：\n$$\r\\hat{y}_{\\text{tree}} = \\hat{y}_{\\text{path}_{l^*}} \\quad l^* = \\arg\\max_l P(\\text{path}_l)\r$$ \u0026ldquo;在实际操作中，我们使用LLM动态地将rubric文本转换为可执行Python代码，该代码以关键要素评估决策为输入，输出最终分数。\u0026rdquo;\n人话：LLM会将打分标准生成Python代码，直接传入关键要素的多热向量就可以直接算分\n","date":"2025-07-16T02:45:55+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"大语言模型教育纠错 · 论文笔记（二）"},{"content":"[TOC]\nEssential-Web v1.0 24T tokens of organized web data\rPreview\r构建了多维度的分类体系，适合通过SQL等方式进行数据筛选出新的数据集\n使用开源模型进行数据标签的标注，得到了EAI-Distill-0.5b 推理清洗了23.6B的数据，花费了90000 AMD MI300x GPU-hours The inference job ran on 512 AMD MI300x for about 1 week.\n分类体系：\n一个有限的类别集合 $T=\\left { C_1, C_2, \u0026hellip;, C_k \\right } $。 每个类别$C_i$都有一个非空、有限的标签集$L_i$。 标注形式为$T(d) = \\left { (\\lambda_1, \\mu_1), \u0026hellip;\\right}$\n其中，$λ_i\\in L_i$ 是类别$C_i$的主要标签 $\\mu_i \\in (L_i \\setminus {\\lambda_i}) \\cup {\\bot}$ 是一个可选的次要标签，必须与$\\lambda_i$不同 当文档适合两个标签时非常有用 $\\bot$表示弃权（abstention）。 所有类别和标签集都是预先固定的，这允许训练一个单一的静态分类器 实验设置\rChinchilla最优计算比例\rChinchilla缩放定律发现了一个最优比例：大约每个参数需要20个训练token Epoch AIAnalytics Vidhya。这个比例是DeepMind通过训练400多个语言模型得出的计算最优配置。\n具体来说：\nChinchilla模型有70B参数，在1.4万亿tokens上训练，达到20 tokens per parameter的比例 Chinchilla Scaling: A Replication Attempt | Epoch AI 这个20:1的比例被认为是在给定计算预算下实现最佳性能的理想配置 所有数据集在训练前均使用了 13-gram Bloom Filter\n选用了两个2.3B模型对数据进行评估\n预训练（3200亿Token）：该阶段帮助模型学到广泛的语言知识\nGeneral-base：仅使用网络数据（DCLM-baseline）做预训练\nCode-base：使用网络数据（DCLM-baseline）+代码数据（Stack v2 Dedup中的Python），各占50%做预训练\n退火（800亿token）：为了评估特定领域数据集的性能，采用需要评估的新数据集\n学习率接近零的目的是在新的领域数据上进行“微调”，而不是进行大规模的“重新训练” 每个模型总计处理 4000 亿 token 数据量，是Chinchilla的10倍数据\n蒸馏\r蒸馏方案\r数据来源与规模\r标注数据：使用Qwen2.5-32B-Instruct对104.6M文档共82Btoken进行两轮标注，生成合成标签用于蒸馏训练。 第一轮：标注8个分类类别（如FDC、Document Type V1/V2等）。 第二轮：扩展至12个类别（新增Bloom、Technical Correctness等）。 数据预处理\r子采样：对超过30,000字符的文档，截取开头、随机中间段和结尾（Algorithm 12），避免长文本影响推理速度。 质量过滤：通过统计和模型信号（如DCLM分类器）过滤低质量文档（Algorithm 1）。 模型架构\r基础模型：Qwen2.5-0.5b-Instruct（5亿参数），基于Gemma 3架构，使用QK-norm稳定注意力。 序列长度：16,384 tokens，支持长上下文。 训练参数\r优化器：AdamW（β1=0.9, β2=0.95），权重衰减0.1。 学习率：峰值1e-4，线性预热2B tokens，余弦衰减至1e-5，最后线性退火至0。 批量大小：全局2M tokens，梯度累积实现大批次训练。 训练量：82B tokens（合成标签数据）。 损失计算：仅对教师模型生成的标签token计算损失，输入文档和系统提示被掩码。 教师模型选择\r教师：Qwen2.5-32B-Instruct，因其标注一致性（κ=0.74）与推理速度平衡（1.4 RPS/GPU）。 蒸馏步骤\r标签生成： 教师模型生成多分类标签（如FDC层级、Document Type等），格式为主标签,次标签（Algorithm 13）。 压缩输出：从平均791 tokens缩短至51 tokens，提升推理速度50倍（Table 12）。 上下文蒸馏： 移除教师模型的提示模板（Prompt 1/2），直接训练学生模型生成压缩格式标签。 评估方案\rMetrics\r正确性：多人分类的结果应该类似，验证模型打标签是否标准一致 使用GPT-4o和Claude Sonnet-3.5作为专家模型 使用kappa系数作为指标 取对4o的系数和对claude的系数的均值作为结果 检测方式是验证模型与专家模型的标准是否一致，对于指标paper中进行了变种\n对于某个模型的分类结果$S\\in \\left { \\phi, (\\text{label}), (\\text{label1, label2}) \\right }$​\n最少是一个标签（主标签），有时可以加一个次标签\n标注结果一致的判定：两模型的$S$​有交集 然后套公式\nQwen2.5-32B-Instruct ≈ 0.74\nEAI-Distill-0.5b ≈ 0.71~0.73\n正交性：不同分类体系之间的标签应该是独立的 例如在某分类A下打了a，分类B始终是b，发生了绑定 需计算互信息、香农熵 $$\r\u003e\\text{NMI}(X, Y) = \\frac{2 I(X; Y)}{H(X) + H(Y)}\r\u003e$$其中：\n$p(x)$按$x$出现的频率，$p(x,y)$按$x,y$同时出现的频率计算 $I(X; Y)$：互信息 $$\r\u003e I(X; Y) = \\sum_{x, y} p(x, y) \\cdot \\log \\frac{p(x, y)}{p(x)p(y)}\r\u003e $$ $H(X)$：X 的香农熵 $$\r\u003e H(X) = -\\sum_{x} p(x) \\cdot \\log p(x)\r\u003e $$Qwen2.5-32B 平均 NMI ≈ 0.079\nEAI-Distill-0.5b 平均 NMI ≈ 0.092\nDomain Recall 定了Golden URL（认为arxiv和……30 个 base URL的都是高质量数据） 统计有多少能被模型召回 Dataset\rRandom Set：随机采样（需要避免撞车训练数据） STEM Set：从特定领域集合（科学领域）随机采样 通过Golden URL采样 ","date":"2025-07-10T11:16:32+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/","title":"大语言模型数据清洗 · 论文笔记（五）"},{"content":"2412.16838 Ask-Before-Detection: Identifying and Mitigating Conformity Bias in LLM-Powered Error Detector for Math Word Problem Solutions\n[TOC]\nIntro\rAED（Automatic Error Detection） 本文定义为：给定问题-解答的输入对，识别错误步骤以及错误类型 如图，paper点出传统的方法使用可以对问题的常规解法进行正确错误检测\n但是单个问题的解法可以存在多个，认为之前的做法泛用性较差\n常规解法与非常规解法会产生7%的性能差距，先进的闭源模型也无法避免\nLLM错误检测器表现conformity bias（从众偏差） 倾向“遵循主流答案（训练中经常出现的）”而忽略可能也正确但不常见的其他解法 导致模型对标准答案的识别准确，却对非常规解法的识别薄弱 论文针对缓解模型的conformity bias进行工作\n提出AskBD框架，为每个Solution自适应生成参考答案（合适的参考答案能显著提升性能）\n直接调用模型，无微调，拓展性强 自适应方式高度契合给定Solution，降低Bias 框架可协同CoT技术增强性能 Preliminary Study\rPaper构建了一个Alternative Solution数据集用于充分暴露模型的从众偏差效应，帮助进行后续的探索\nAutomatic Solution Permutation\rpaper希望构建一个高质量的Alternative Solution数据集\n给定问题和Solution，希望替换掉整个解决方案为Alternative Solution\n低质量：对常规Solution只是简单的语义替换，并没有深层的逻辑变换 ASP（自动解法置换），如图，对应了Solution和数学表达式的关系，使用LLM prompt独立执行：\nExtract：常规Solution -\u0026gt; 数学表达式，完成后需要执行运算，检查是否能够得到正确计算结果，否则剔除 Permute：因式分解、分配律，重新排列表达式（同样需要运算检验） Explain：置换后的表达式输入到LLM，引导生成高质量Alternative Solution paper采样GPT-4o，从GSM8K数据集抽取200组问答对，构建常规数据集D\n对D中的每个样本，3次ASP生成3个Alternative Solution，由教育系研究生评审质量，选择三个之中最优的一个\n完成替换数据集D\u0026rsquo;的制作\nErroneous Solution Generation\r需要将错误注入到D和D‘之中，生成测试样本\n主要参考：2406.00755 Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction\npaper引入了四种错误：\n$\\varepsilon_C$：calculation errors Operands in expressions are correct but an error occurs in the calculated results.（表达式正确，计算结果出错） Each gust blows the leaf forward 5 feet, so 11 gusts will blow it forward 5 ×11 = 50 feet. $\\varepsilon_R$：reference errors Expression are incorrectly referencing the question conditions or the results from prior steps.（错误引用了题目条件或之前的计算结果） Each gust blows the leaf forward 5 feet, so 10 gusts will blow it forward 5 ×10 = 50 feet. （题目原条件是11，不是10） $\\varepsilon_M$ ：missing steps Operands or expressions in the step that lack of references or support from the question conditions or prior steps. Each swirl after a gust blows it back 2 feet, so 11 swirls will blow it back 2 ×11 = 22feet. Step 2. After 11 gusts, the leaf has traveled 55 − 22 = 33 feet down the sidewalk.（缺少了得到55这个数字的计算过程） $\\varepsilon_H$：hallucinations（幻觉） Statements or operands in the listed expression are fabricated or inconsistent with the question’s conditions.（虚构或与条件不一致） …… It is worth noting that this study specifically aims to explore conformity bias, and therefore, we do not include all possible error types.\n针对D和D\u0026rsquo;的每个样本，随机错误步骤的位置编号，每个样本生成了四种不同错误类型的样本，总共2000条\nAnalysis and Findings\rConformity Bias Identification\r评估指标：The evaluation metric is the identification accuracy across both correct and erroneous solutions.\n需要识别错误位置、错误类型 Paper使用提示词进行纠错，明确LLM本题存在替代解法，强调所有合理解决方案应该被接受，并且明确定义错误类别\n1 2 3 4 5 6 Given the \u0026lt;question\u0026gt;, please judge whether each step in \u0026lt;solution\u0026gt; is correct. **During the judging process, you should know that the \u0026lt;question\u0026gt; does not always have only one standard solution, and any reasonable \u0026lt;solution\u0026gt; should be accepted. You should pay attention to both the expressions and the statements in each step, and take care about the logic consistency between different steps. Additionally, consider arithmetic expression equivalency and avoid rejecting solutions solely because they use equivalent expressions.** In each step, if no errors are found, respond with Step X: \u0026lt;correct\u0026gt;. If you find that the operands in the listed expressions are correct but an error occurs in the calculated result, respond with Step X: \u0026lt;calculation error\u0026gt;. If you find statements or operands in the listed expression are incorrectly referencing the question conditions or the results from prior steps, respond with Step X: \u0026lt;reference error\u0026gt;. If you find operands or expressions in the step that is lack of references or support from the question conditions or prior steps, respond with Step X: \u0026lt;missing step\u0026gt;. If you find statements or operands in the listed expression are fabricated or inconsistent with the question’s conditions, respond with: Step X: \u0026lt;hallucination\u0026gt;. If an error is a follow-on issue due to mistakes in previous steps rather than an independent error, respond with: Step X: \u0026lt;secondary error\u0026gt;. \u0026lt;question\u0026gt; [Question Text] \u0026lt;solution\u0026gt; [Solution Text] Now, please start to respond. 参与测试的LLM：\n平均错误检测准确率的测试结果：\n证实了LLM在AED任务中存在明显的从众偏差\nSolution Likelihood Score Analysis\r对于当前问题$q$，我们可以计算LLM生成答案$s$的概率$P(s|q)$\n我们可以把$s$拆分成多个token：$s_i$\n$$\rP(s|q) = P(s_1, s_2, ...,s_{|s|} | q)\r$$$$\rP(s|q) = P(s_1|q) \\times P(s_2|q,s_1) \\times P(s_3|q,s_1,s_2) \\times ...\r$$$$\r\\log P(s|q) = \\sum_{i=1}^{|s|} \\log P(s_i|q,s_1:s_{i-1})\r$$ 这个值就是对数似然分数 （Log-Likelihood Score）\n它衡量了模型在给定问题 q 的情况下，对答案 s 的“信任度”或理解程度\n$$\r\\log L_{\\theta}(s|q) = \\frac{\\log L_\\theta(s|q)}{|s|}\r$$ 其中$\\theta$表示LLM的参数（闭源模型的似然分数无法获取，采取了开源模型的似然分数均值做伪指标）\n同时喂入 了常规解法+替代解法，根据似然分数分成四个档进行对比\n显然似然值越高，Acc越高\n对于开源模型，替代解法的似然分数显然低于常规解法\nReference-based Detection Findings\r不太可能直接提高替代Solution的似然值 通过改变数据集进行微调，但是无法解决根本问题，仍有可能碰见其他的未遇见情况 参考：2407.09136 Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors，提出引入参考答案提升了常规解法的检测性能\n将学生解题步骤与标准答案对齐\n本paper尝试推广到替代解法，但是：\n实际情况下，参考答案并不是总能获取 即便能够获取，一般也是常规解法的参考答案 paper采用了两条技术路线进行对比：\n使用常规解法作为参考答案 自适应使用对应解法作为参考答案（？细节不太清楚，后文兴许会说） 根据与前文提示词的测试结果进行对比，引入参考答案对两个数据集的acc都有显著的提升\n但是使用常规解法做参考答案加剧了bias，而自适应选择明显缓解了bias\n因此选择合适的参考答案是能起到关键作用的\nMethod\r提出AskBD（Ask-Before-Detection）框架，在评分过程中为每个待评答案动态生成适配的参考解法\n输入：问题文本$q$，解答文本$s$，LLM$f$，提示词$p$\nCondition and question extractor(CQE) 从问题文本中抽取条件$q_c$和提问文本$q_i$ $(q_c, q_i) = f([p_{cqe}, q])$ Solution Step Inquirer(SSI) 为提高生成结果的稳定性，SSI 会先总结每个步骤的结论再构建对应问题。 将解答文本转化为分步骤问题列表文本$Q$，末尾附加提问文本$q_i$，以确保生成的参考解答能够回应原始问题的核心任务。 $Q = [f([p_{ssi},s]), q_i]$ Step Question Responder (SQR) 通过条件文本总结$Q$中每个问题的答案，生成参考答案$r$ $r = f([p_{sqr},q_c, Q])$ Reference-Enhanced Grader (REG) 根据$q, s, r$生成错误位置$y_s$和错误类型$y_e$ $y=f([p_{reg},q,s,r])$ 输出：$y_s, y_e$\nExperiment\r实验目的是验证核心的三个问题：\n是否缓解从众偏差 是否有额外的性能优势 与CoT等推理技术的兼容性 实验方法：\n采用前文数据集，相同的10个LLM，用于对比前两个问题 整合了CoT技术，评估兼容性 所有实验分别实验三种不同的随机seed，报告平均错误检测准确率 前文测试和CoT方案作为两个baseline CoT的提示词\n1 2 3 Before the \u0026lt;response\u0026gt;, you should provide your step-by-step \u0026lt;thinking\u0026gt; about your judging process. \u0026lt;question\u0026gt; [Question Text] \u0026lt;solution\u0026gt; [Solution Text] Now, please start to think first and then respond. 对于问题1\n重点分析M0和M2在$\\Delta$列的差距 Base版本的优化并不明显，认为是模型推理能力不足，限制了框架效用 对比M1与M2，CoT也有缓解Bias的能力，在多数Advance模型中，框架的优化能力强于CoT 对于问题2\n在D、D\u0026rsquo;列对比M2和M0 框架确实提升了acc性能 对比M1 M2，CoT也体现出了性能提升 在base模型中CoT技术更胜一筹 在Advance模型中框架更强（……） 针对问题3\nM3和M1对比，确实变强了 兼容性好 Limitation\r只考虑了四个错误类型，忽略了学生解答中那些更罕见却更具挑战性的错误类型 仅聚焦于数学应用题 ","date":"2025-06-23T15:13:55+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"大语言模型教育纠错 · 论文笔记（一）"},{"content":"[TOC]\nTemporal Consistency for LLM Reasoning Process Error Identification\r2503.14495 Temporal Consistency for LLM Reasoning Process Error Identification\n无训练过程 纯迭代反思 应用领域主要是大模型自己的解题步骤的错误检测\n算是一个比较通用的做法，数学题之外有分步性质的应该也ok\n可以借鉴一下其Reflection的方法\n输入定义如下：\n$P$：题目原文（例如数学问题）； $S = [s_1, s_2, \u0026hellip;, s_n]$：模型生成的解题步骤，按步分段； $L$：目标是预测哪一段 $s_i$ 是 首个错误步骤（或无错误）； $R_t$：第 $t$ 轮的模型判断（包含错误定位和解释）； 总共准备了K个模型并行进行推理，对于单个模型需要做以下事情：\n给定题目、解题步骤、自己的上轮判断 模型需要结合该信息判断、解释 持续迭代 对K个模型投票，票数最多的即为结果\n设定的终止条件：\n单个模型连续$q$轮给出稳定结论 K个模型的过去$q$轮的主体结果投票比例不能下降 或者T轮迭代上限（防止死循环） 如图，三个模型进行迭代，最后得到一致的结果\n下面两个模型一开始不能得到正确答案，但是经过迭代得到正确结果\n实验结果：\nTodo\r2406.00755\n","date":"2025-06-20T14:52:55+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/","title":"大语言模型教育纠错 · 论文笔记（零）"},{"content":"https://arxiv.org/html/2303.16854\nPreview\r解释 - 标注 双阶段方法 LLM生成少量人类标注的解释 自动构建思维链+fewshot提示词 自动标注 Approach\r从人类标注者的培训中可以发现，我们需要提供一定的引导、样例，才能规范人类标注一致性\n模型标注也是同理\n解释\r使用GPT3.5进行生成解释：\n1 2 3 4 5 6 7 8 9 10 Directions: Given a search engine query: \u0026#34;google data studio sharepoint\u0026#34;, first, consider what the user could have in mind when they type in the query and allow for misspellings or other ambiguity, then classify the relevance of keyword: \u0026#34;sharepoint migration tool file share\u0026#34; to the query into one of the following categories: \u0026#34;Not bad\u0026#34;, \u0026#34;Bad\u0026#34;. Definitions of the categories: - **\u0026#34;Not bad\u0026#34;** : The keyword is relevant to the user’s search query. This can include: broader or narrower product selection, competitor or alternative products, accessories, products often purchased together, and related topics as well as direct matches to the user’s search. - **\u0026#34;Bad\u0026#34;**: The keyword is not relevant to the user’s search query. There is no relationship between the query and keyword. This includes but is not limited to: incorrect/unrelated product, unrelated topic, wrong location when location is important, cannot be used in place of query product nor are they commonly purchased together, etc. Briefly explain why the relevance is \u0026#34;Bad\u0026#34;, with a response length not exceeding 100 words. 构建提示词\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Given a search engine query, first, consider what the user could have in mind when they type in the query and allow for misspellings or other ambiguity, then classify the relevance of keyword to the query into one of the following categories: \u0026#34;Not bad\u0026#34;, or \u0026#34;Bad\u0026#34;. The definitions of the categories are: - **\u0026#34;Not bad\u0026#34;**: the keyword is relevant to the user’s search query. This can include: broader or narrower product selection, competitor or alternative products, accessories, products often purchased together and related topics as well as direct matches to the user’s search. - **\u0026#34;Bad\u0026#34;**: the keyword is not relevant to the user’s search query. There is no relationship between the query and keyword. This includes but is not limited to: incorrect/unrelated product, unrelated topic, wrong location when location is important, cannot be used in place of query product nor are they commonly purchased together, etc. Please predict whether the keyword is relevant to the query or not. The answer should be exact \u0026#34;Not bad\u0026#34; or \u0026#34;Bad\u0026#34;. ### Examples: 1. **Query**: google data studio sharepoint **Keyword**: sharepoint migration tool file share **Answer**: The relevance of the keyword \u0026#34;sharepoint migration tool file share\u0026#34; to the search query \u0026#34;google data studio sharepoint\u0026#34; is \u0026#34;Bad\u0026#34;. This keyword is not relevant to the user’s search query as it pertains to a different tool or technology (SharePoint) that is not mentioned in the search query. The keyword focuses on a migration tool for SharePoint file sharing, while the search query is focused on Google Data Studio. There is no relationship between the two, so the keyword is not relevant to the user’s search query. Therefore, the relevance is \u0026#34;Bad\u0026#34;. 2. **Query**: motorhomes sale **Keyword**: rv sale used class c **Answer**: The relevance of keyword \u0026#34;rv sale used class c\u0026#34; is \u0026#34;Not bad\u0026#34;. The keyword \u0026#34;rv sale used class c\u0026#34; contains relevant information to the user’s search query \u0026#34;motorhomes sale\u0026#34;. It includes a specific type of motorhome, \u0026#34;class c\u0026#34;, that is being sold used, which is a relevant product selection for the user’s search. Additionally, the keyword \u0026#34;rv sale used class c\u0026#34; is a direct match to the user’s search and is commonly purchased together with the products in the search query. Therefore, the relevance is \u0026#34;Not bad\u0026#34;. 3. **Query**: southern exposure seed exchange company **Keyword**: uk poppy seeds **Answer**: The relevance of the keyword \u0026#34;uk poppy seeds\u0026#34; to the search query \u0026#34;southern exposure seed exchange company\u0026#34; is \u0026#34;Not bad\u0026#34;. This is because the search query is about a seed exchange company, and the keyword \u0026#34;uk poppy seeds\u0026#34; is a specific type of seed that could be offered by such a company. Even though the company mentioned in the search query is based in the southern hemisphere, they may still offer seeds from other regions, including the UK, which could include poppy seeds. Therefore, the relevance is \u0026#34;Not bad\u0026#34;. 4. **Query**: nissan parts canada **Keyword**: purchase tires **Answer**: The relevance of \u0026#34;purchase tires\u0026#34; to the query \u0026#34;nissan parts canada\u0026#34; is \u0026#34;Bad\u0026#34;. The keyword \u0026#34;purchase tires\u0026#34; is not directly related to the user’s search for Nissan parts in Canada. Tires are not a part of a Nissan car and therefore are not relevant to the search for Nissan parts. Additionally, the keyword does not relate to a common purchase with Nissan parts, and it is not a substitute for the search query. Therefore, the relevance is \u0026#34;Bad\u0026#34;. ### Task: **Query**: {query} **Keyword**: {keyword} **Answer**: ","date":"2025-06-14T16:35:54+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%9B%9B/","title":"大语言模型数据清洗 · 论文笔记（四）"},{"content":"FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering\r2501.07314FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering\nhttps://github.com/TurkuNLP/finerweb-10bt\n[TOC]\n概述\rGPT-4o mini 对 FineWeb 中 20,000 份文档样本进行逐行标注，使模型能够为低质量文本行创建描述性标签 标签被归纳为九大类别，并训练 DeBERTa-v3 分类器将过滤规模扩展至 FineWeb 的 100 亿 token 子集 结果表明：使用过滤数据训练的模型在 HellaSwag 基准测试中准确率更高，且能以最多减少 25%的数据量更快达到性能目标 核心问题：\nHow well can an LLM identify low-quality content missed by heuristic filters? Does LLM-based quality filtering of training datasets improve model performance? paper定义高质量数据为：\nhuman-written, continuous English text from the main content of a website, reflecting natural language use across diverse contexts and domains.\n网站主体内容中人类撰写的连贯英文文本，能反映跨领域自然语言使用。\n典型实例包括访谈核心文本、论坛帖子、新闻文章、博客和食谱。 与之相对，低质量内容则包含导航菜单、版权声明、编程代码和元数据等重复性元素。 过滤分为三个级别：\n文档级：基于简单规则整篇剔除文档 少于三句话的文档 存在过度重复内容的文档 行级： 删除含javascript等术语的行、纯数字行或低于长度阈值的行 字符级： 移除维基百科常见的引用标记如[1]和[citation needed] 现存的过滤方法具有数据集特异性，相关指标与数据集本身有关\n行末标点比例≤0.12的文档（移除10.14% token，相比C4终止标点过滤的30%更高效） 重复行字符比例≥0.1的文档（移除12.47% token） 短行（\u0026lt;30字符）比例≥0.67的文档（移除3.73% token） Method\r数据来源：Fineweb，构建来自 FineWeb 的 100 亿 token（约 1500 万文档）样本，称为 FineWeb-10BT 抽样20,000份文档进行GPT-4o mini 标注 - 为每行生成描述性标签，分为高质量或低质量类别 O1-preview将生成的大量标签归类为更小、更方便管理的集合 训练基于encoder的分类器，scale到Fineweb10BT 使用清洗前后的Fineweb10BT训练GPT-2，在HellaSwag上benchmark 全过程是数据驱动的，不依赖于固定的类别\nExperiments\rGPT-4o mini 标签标注\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 # 系统提示词 system = \u0026#34;You are an expert text classifier specializing in LLM training data. Your task is to classify each line of text based on its suitability for inclusion in a language model training dataset. High-quality content is clean, meaningful, well-structured, and useful for training language models. Low-quality content includes boilerplate elements (e.g., navigation menus, footers), non-linguistic symbols, formatting tags, placeholders like \u0026#39;Lorem ipsum\u0026#39;, and spammy, irrelevant, or toxic language.\u0026#34; # 用户提示词 prompt = f\u0026#34;\u0026#34;\u0026#34; **Instructions:** 1. **Line Identification and Separation**: - Each line starts with \u0026#34;Line X:\u0026#34; where X is the line number. Treat each \u0026#34;Line X:\u0026#34; as a single unit, regardless of length; do not split lines. - Lines are separated by newline characters (`\\\\n`) and dashes (`------`). If there\u0026#39;s no newline character, treat the entire text as a single line. 2. **Contextual Classification**: - Use the context of all lines when classifying each one, as they are sequential and from the same document. - For example, a line starting with a hyphen might be part of a list and should be classified as \u0026#34;Clean.\u0026#34; 3. **Assigning Labels**: - Assign **exactly one label** to each line. - If the line is suitable for inclusion, label it **\u0026#34;Clean\u0026#34;**. - If not, assign a specific and descriptive label explaining why it\u0026#39;s unsuitable. - **Prefer labels from the provided list**. Only create a new label (max three words) if absolutely necessary. - **Do not use vague labels** like \u0026#34;Low-Quality,\u0026#34; \u0026#34;Bad,\u0026#34; \u0026#34;Unsuitable,\u0026#34; etc. Labels must be specific and descriptive. 4. **Focus on Linguistic Content**: - Retain valuable and diverse linguistic content suitable for language model pre-training, including natural language patterns, standard advertising copy, commercial language, and promotional content written in natural language. 5. **Tolerance for Minor Errors and Toxic Language**: - Minor grammatical errors, typos, or small mistakes do not disqualify a line from being \u0026#34;Clean.\u0026#34; Only exclude lines with pervasive errors that significantly hinder understanding. - Mild expletives and controversial opinions do not disqualify a line from being \u0026#34;Clean.\u0026#34; Only exclude lines with blatantly hateful, harmful or toxic content. 6. **Output Format**: - Your output must have exactly the same number of lines as the input, matching each line number correctly. - Output only the line number followed by the label, separated by a colon. - Do not include any additional text or explanations. - Do not output dashes between the lines. **Guidelines for \u0026#34;Clean\u0026#34; Lines**: Assign \u0026#34;Clean\u0026#34; to lines that: - Represent natural language suitable for training language models. - Include informal internet language, grammatical errors, questions, partial sentences, and common online expressions. - Contain standard advertising or commercial language in natural sentences. - Have properly formatted titles, headings, and readable content, even with stylistic elements. - Include minor in-text elements like email addresses, dates, or URLs within natural sentences. - Are general promotional content written in natural language. **Guidelines for Non-\u0026#34;Clean\u0026#34; Lines**: Lines not classified as \u0026#34;Clean\u0026#34; need a specific and descriptive label. Examples include lines that: - Contain blatantly hateful or harmful language. - Are long passages of non-English text (excluding common foreign phrases used in English). - Include disclaimers, copyright notices, terms, and conditions. - Consist of menu items, login links, buttons, or navigation menus. - Contain random characters, garbled text, or excessive symbols. - Include programming code, HTML tags, or markup languages (when actual code or markup appears). - Present keywords, tags, or similar data without sufficient context. - Are irrelevant or spam-like content not suitable for training. - Are **excessively** promotional without natural language structure (e.g., a list of product names and prices without sentences). **Possible Labels for Non-\u0026#34;Clean\u0026#34; Lines**: {non_quality_labels} **Example Input:** Line 1: Welcome to our website! ------ Line 2: Contact us at support@example.com. ------ Line 3: ***** $$$$$ ------ Line 4: \u0026lt;div\u0026gt;Content\u0026lt;/div\u0026gt; ------ **Example Output:** Line 1: Clean Line 2: Clean Line 3: Encoding Errors Line 4: HTML Tags **Now, classify the following lines:** {input} \u0026#34;\u0026#34;\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 **指令：** 1. **行标识与分隔**： - 每行以“Line X:”开头，X为行号。将每个“Line X:”视为一个独立单元，无论长度如何；请勿拆分。 - 行之间用换行符（`\\n`）和短横线（`------`）分隔。若无换行符，则将整个文本视为单行。 2. **上下文分类**： - 分类时需考虑所有行的上下文，因为它们来自同一文档且顺序相关。 - 例如，以连字符开头的行可能是列表的一部分，应标记为“Clean”。 3. **标签分配**： - 每行**必须分配一个标签**。 - 若适合纳入训练数据，标记为**“Clean”**。 - 若不适合，需提供具体描述性标签说明原因。 - **优先使用提供的标签列表**。仅在必要时创建新标签（最多三个单词）。 - **禁止使用模糊标签**，如“低质量”、“差”、“不合适”等。标签必须具体明确。 4. **语言内容聚焦**： - 保留对语言模型预训练有价值的多样化语言内容，包括自然语言模式、标准广告文案、商业用语和自然语言编写的推广内容。 5. **对轻微错误和毒性内容的容忍**： - 轻微语法错误、拼写问题或小错误不影响标记为“Clean”。仅当错误严重影响理解时才排除。 - 轻度脏话或有争议的观点不影响标记为“Clean”。仅排除明显仇恨、有害或毒性内容。 6. **输出格式**： - 输出行数必须与输入完全一致，且行号对应正确。 - 每行输出格式为“Line X: 标签”，仅包含行号和标签。 - 禁止额外解释或文本。 - 行间禁止输出短横线。 **“Clean”行标准**： 符合以下条件的行标记为“Clean”： - 代表适合训练的自然语言。 - 包含网络用语、语法错误、问题、不完整句子或常见网络表达。 - 含有自然句式中的标准广告或商业用语。 - 格式正确的标题、题头或可读内容（即使包含样式元素）。 - 自然句子中的邮箱、日期或URL等次要元素。 - 以自然语言编写的常规推广内容。 **非“Clean”行标准**： 需提供具体描述性标签，例如： - 包含明显仇恨或有害内容。 - 大段非英语文本（英语中常用的外语短语除外）。 - 免责声明、版权声明、条款协议。 - 菜单项、登录链接、按钮或导航菜单。 - 随机字符、乱码或过多符号。 - 编程代码、HTML标签或标记语言（实际代码或标签出现时）。 - 缺乏上下文的关键词或标签。 - 与训练无关的垃圾内容。 - **过度推广**且无自然语言结构（如纯产品名和价格列表）。 **非“Clean”行标签示例**： {non_quality_labels} **输入示例**： Line 1: 欢迎访问我们的网站！ ------ Line 2: 联系支持邮箱：support@example.com。 ------ Line 3: ***** $$$$$ ------ Line 4: \u0026lt;div\u0026gt;内容\u0026lt;/div\u0026gt; ------ **输出示例**： Line 1: Clean Line 2: Clean Line 3: 编码错误 Line 4: HTML标签 **请对以下行进行分类**： {input} 一开始并不提供任何的非Clean标签，由模型逐渐生成，优先使用已有的标签，否则进行扩充\n未避免顺序带来的影响，每次迭代后随即打乱标签列表\n文档最多被分割为多个chunk，每个chunk最多15行，方便结合上下文\n单行不能超过200字符，否则按照标点进行切割为新的行\npaper提到：超长行会导致模型的错误输出 其中每个圆点的大小对应相应类别的相对出现频率\n法律文本出现在左上角，成人及有害内容集中于右上方中部，而参考文献则靠近底部。联系方式（如时间、日期和电话号码）松散分布在左侧，技术类内容（如编程代码）则位于中部。这些分布模式表明，LLM 生成的标签能够有效区分文本行质量，为我们最终构建分类体系提供了可靠依据。\n83%的数据被标记为清洁 547个生成的标签，其中部分只出现了一次 人工复查，直接标记为清洁 标签分组\r对于实现剩下的382个标签，通过O1-preview（推理模型）归类为更简洁、更易管理的宽泛类别\n指导该模型创建清晰、明确的分类 每个标签只能属于一个组别 Categories Lines % Clean 283,267 86.24 Formatting, Style \u0026amp; Errors 格式、风格与错误 13,150 4.00 Bibliographical \u0026amp; Citation References 参考文献与引用规范 8,768 2.67 Promotional \u0026amp; Spam Content 促销与垃圾内容 7,339 2.23 Contact \u0026amp; Identification Information 联系与身份识别信息 3,898 1.19 Navigation \u0026amp; Interface Elements 导航与界面元素 3,327 1.01 Technical Specifications \u0026amp; Metadata 技术规范与元数据 3,298 1.00 Legal \u0026amp; Administrative Content 法律与行政内容 2,992 0.91 Offensive or Inappropriate Content 冒犯性或不当内容 2,433 0.74 Total 总计 328,472 100 模型可能会发生错误，例如未能分配全部标签、标签归入多个类别……\n人工修正一下即可\nInter-Annotator Agreement 人工标注者一致性（IAA）实验\r抽取50篇文档的726行，人工独立分类到九个标签之内 $$\r\\kappa = \\frac{p_o - p_e}{1 - p_e}\r$$ 假设两位标注员（A 和 B）对 100 条文本进行情感分类，标签为 正面（Positive） 或 负面（Negative）。他们的标注结果如下表：\nB: Positive B: Negative 总计 A: Positive 50 10 60 A: Negative 20 20 40 总计 70 30 100 $p_o$是两位标注员实际一致的比例，即对角线单元格的和除以总数。\n两位标注员在 70 条样本上达成一致（50 条 Positive + 20 条 Negative），因此$p_o = 0.7$ $p_e$ 是假设两位标注员随机标注时预期的一致比例。需分别计算每个类别随机一致的联合概率，再求和。\nA 标注 Positive 的概率：$P_{\\text{A+}} = \\frac{60}{100} = 0.6$ A 标注 Negative 的概率：$P_{\\text{A-}} = \\frac{40}{100} = 0.4$ B 标注 Positive 的概率：$P_{\\text{B+}} = \\frac{70}{100} = 0.7$ B 标注 Negative 的概率：$P_{\\text{B-}} = \\frac{30}{100} = 0.3$ 已知以上概率，接下来计算在随机标注的情况下，两人同时一致的概率：\n随机都标为 Positive 的概率：$P_{\\text{A+}} \\times P_{\\text{B+}} = 0.6 \\times 0.7 = 0.42$ 随机都标为 Negative 的概率：$P_{\\text{A-}} \\times P_{\\text{B-}} = 0.4 \\times 0.3 = 0.12$ 因此：$p_e = 0.42 + 0.12 = 0.54$\n解释：\n如果两位标注员完全随机标注，预计会有 54% 的样本因巧合而一致。\n$$\r\u003e \\kappa = \\frac{p_o - p_e}{1 - p_e} = \\frac{0.7 - 0.54}{1 - 0.54} = \\frac{0.16}{0.46} \\approx 0.348\r\u003e $$ κ ≈ 0.35：介于 0.2~0.4 之间，说明两位标注员的一致性为“一般”（仅略高于随机水平）。 对比简单一致率 70%：若直接用 70% 会高估一致性，而 Cohen\u0026rsquo;s Kappa 通过剔除随机影响，给出了更严格的评估。 \\( p_o \\)：直接观察到的对角线比例。 \\( p_e \\)：基于边际分布的“随机一致”概率，反映巧合带来的虚假一致性。 Kappa 的意义：量化了超越随机水平的一致性，避免高估可靠性。 κ值范围 一致性强度 解释 κ ≤ 0 比随机还差 一致性低于随机猜测（罕见，可能表示系统性分歧或标注错误）。 0 \u0026lt; κ ≤ 0.2 轻微一致（可忽略） 一致性极低，几乎无实际意义。 0.2 \u0026lt; κ ≤ 0.4 一般一致（弱） 一致性较弱，但高于随机水平（需谨慎对待结果）。 0.4 \u0026lt; κ ≤ 0.6 中等一致 一致性适中，结果有一定可靠性（常见于人工标注任务）。 0.6 \u0026lt; κ ≤ 0.8 高度一致 一致性较强，结果可靠（如专业医生诊断或严格标注流程）。 0.8 \u0026lt; κ ≤ 1 几乎完全一致 一致性极高，接近完美（罕见，通常需检查是否过拟合或标注规则过于简单）。 通过IAA实验，得到：\nA1 A2 Avg. 平均 All labels 所有标签 0.79 0.60 0.70 Clean vs. Non-clean 清洁与非清洁 0.78 0.67 0.73 基于 LLM 的分类方法总体上能为 FineWeb 文本生成可接受的标签。\n分类器训练\rDeBERTa-v3 Stella-en-400M-v5 XLM-RoBERTa-base（支持多语言） 我们首先从文档中提取独立文本行，将每行作为单独样本。随后对数据进行随机打乱，并通过分层抽样划分为训练集（70%）、开发集（10%）和测试集（20%）。我们在每个模型上添加分类头，为每行文本生成 9 个类别的概率分布，同时微调分类头与基础模型。\n我们采用 bfloat16 精度，学习率设为 1e-5，批处理大小为 16。基于评估损失值实施早停机制（耐心值为 5），最大训练轮数设为 5 轮，但模型通常在首轮后即收敛。我们对交叉熵损失函数施加 0.1 的标签平滑处理以提升泛化能力。所有训练均在单块 A100 GPU 上完成。\n大多数误分类样本被归入 Clean 类别，表明其他类别间具有较强区分度 冒犯性或不当内容区分度最低，源于 LLM 训练数据中对冒犯性材料定义边界存在固有困难 参考文献与引用类别因其易于识别的格式和内容特征，成为区分度最高的类别 分类器更倾向于将低质量文本行误标为\u0026quot;清洁\u0026quot;\n而非错误地将高质量行标记为低质量\n这种偏差有助于降低从数据集中丢弃有价值数据的风险\n数据清洗\rClean数据占比86%确实可能会带来模型预测过度自信的问题\n采用 Platt 缩放法 在保留测试集上训练 Platt 逻辑回归模型 在为 FineWeb-10BT 数据集预测质量分数时将其叠加应用于分类器之上 留坑，先不研究 对整个数据集进行分片，每个分片128行为一个批次\n转化为分类问题，只判断是否为Clean 阈值分别设为0.5或0.9 ","date":"2025-06-10T14:54:12+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/","title":"大语言模型数据清洗 · 论文笔记（三）"},{"content":"[TOC]\nThe Pile: An 800GB Dataset of Diverse Text for Language Modeling\rarXiv 2101.00027 The Pile: An 800GB Dataset of Diverse Text for Language Modeling\nGithub EleutherAI/the-pile\n通过合成多个数据集，提升多样性，提升大规模语言模型的跨领域通用知识与下游任务泛化能力 稍微看偏了，paper更多的精华在如何去衡量数据集对模型性能的提升水平\n和清洗关系不大\nThe Pile Datasets\r由22个部分组成 由于不同数据集存在差异（维基百科质量更高），因此进行了加权处理 权重越高，被使用的概率越高（更可能被重复使用次数） 例如维基百科重复采用3次 部分表格：\nDataset Name Raw Size (before sampling) Weight (%) Epochs Effective Size Mean Document Size Pile-CC 227.12 GiB 18.11% 1.0 227.12 GiB 4.33 KiB PubMed Central 90.27 GiB 14.40% 2.0 180.55 GiB 30.55 KiB Books3 100.96 GiB 12.07% 1.5 151.44 GiB 538.36 KiB OpenWebText2 62.77 GiB 10.01% 2.0 125.54 GiB 3.85 KiB ArXiv 56.21 GiB 8.96% 2.0 112.42 GiB 46.61 KiB Github 95.16 GiB 7.59% 1.0 95.16 GiB 5.25 KiB FreeLaw 51.15 GiB 6.12% 1.5 76.73 GiB 15.06 KiB StackExchange 32.20 GiB 5.13% 2.0 64.39 GiB 2.16 KiB USPTO Backgrounds 22.90 GiB 3.65% 2.0 45.81 GiB 4.08 KiB PubMed Abstracts 19.26 GiB 3.07% 2.0 38.53 GiB 1.30 KiB Gutenberg (PG-19) 10.88 GiB 2.17% 2.5 27.19 GiB 398.73 KiB OpenSubtitles 12.98 GiB 1.55% 1.5 19.47 GiB 30.48 KiB Wikipedia (en) 6.38 GiB 1.53% 3.0 19.13 GiB 1.11 KiB Raw Size：采样前的大小\nWeight ：采样后的大小占比\nEpochs：被采样次数\nEffective Size：采样后的有效大小\nMean Document Size：平均文档大小\n部分数据已被发布者清洗的很好，只进行了最小程度的预处理 Pile-CC\r由CC数据集清洗得到 使用justText清洗raw HTTP responses including page HTML，相比于.WET的纯文本效果更好 Others\r分类 来源 学术文献 ArXiv、PubMed Central、NIH ExPorter 图书与出版物 Books3、Project Gutenberg (PG-19)、BookCorpus2 代码与技术文档 GitHub、StackExchange 法律与政府文件 FreeLaw、USPTO Backgrounds 多语言与翻译文本 EuroParl、OpenSubtitles 社交与对话数据 HackerNews、Ubuntu IRC、Enron Emails 特殊领域数据 DeepMind Mathematics、PhilPapers（哲学）、YouTube字幕 网络爬取内容 Pile-CC（新构建的Clean Common Crawl子集） Benchmarking Language Models with the Pile\r可以训练数据，同时因为涉及领域广泛，也可以基准测试\n划分为训练集、验证集、测试集（$0.1%$测试集+验证集，虽然比例很低但是仍各自超过1G）\n尽管去重，但是肯定还是存在重复\npaper中首选了BPB作为评测指标：\n输入：负对数似然损失（Negative Log-Likelihood Loss）\n模型在测试数据上输出一个损失值 $L$，表示其预测能力。 越低的 $L$ 表示模型越能准确预测下一个词。 转换为 BPB：（bits per UTF-8 encoded byte）\n使用公式将损失 $L$ 转换为每字节的比特数 $$\rBPB = \\frac{L_T}{L_B}\\log_2 e^L = \\frac{L_T}{L_B}\\times \\frac{L}{\\ln2}\r$$ 其中： $L_T$：数据集以 token 为单位的长度 $L_B$：数据集以 UTF-8 编码字节为单位的长度 和困惑度有一点相似，用于衡量模型对数据的压缩效率或预测能力\n与Bits per Character (bpc)不同的一点，字符不是一个很好的定义（Unicode 中字符的界定可能复杂（例如组合字符、emoji 等），导致统计不一致。）\n同时bpb不受到分词的影响，UTF-8的字节定义是准确的，适合基于不同模型、分词进行比较\n指标 优点 缺点 适用场景 Bits per Byte 分词无关、字节标准明确 对非字节级任务不直观 跨模型比较、数据压缩评估 Bits per Char 更贴近人类理解 Unicode 字符定义模糊 字符级生成任务（需统一字符定义） Perplexity 直接反映预测不确定性 依赖分词、数值范围不稳定 单一模型调参、生成质量评估 更加完整的解释\r自信息：指的是当我们接收到一个消息时所获得的信息量 在信息论中，自信息衡量一个事件携带的信息量，由概率$p$决定。\n$$\rI(p) = -\\log_2(p)\r$$为了编码这一事件，我们选择霍夫曼编码这类最优编码，同时为了最小化平均码长：\n高频事件：分配短码 低频事件：分配长码 如果事件 $A$ 的概率 $p = 1/2$ ，则 $I(A) = -\\log_2(1/2) = 1$ 比特。这表示需要用 1 位二进制码（如 0 或 1）编码。 - 如果事件 $B$ 的概率 $p = 1/8$ ，则 $I(B) = -\\log_2(1/8) = 3$ 比特。需要用 3 位二进制码（如 000 到 111 之一）编码。 $$\rL = -\\ln p\r$$ 一般使用的是自然对数，同时其恰好表示了概率为$p$的事件的信息量（单位为纳特（底数取e））\n$$\rBits = I(p) = -\\log_2(p) =-\\frac{\\ln p}{\\ln 2} =\\frac{L}{\\ln 2}\r$$ 同时，模型的损失是基于token计算的，即每个token的预测损失\n所以这里的单位是：Bits per token\n$$\rbpb = \\frac{L_T}{L_B}\\times \\frac{L}{\\ln2}\r$$ 这样就得到了：Bits per Byte，消除了分词器、语种编码等其他影响，可以直接衡量模型输出的质量\n评测\r然后paper实验验证了一下用训练集训练过的模型会更nb\n通过分析哪些Pile子数据集的表现最差，就知道模型的训练数据分布在这块比较浅，就可以使用pile这块数据集进行补充 为了探索哪些数据集是模型表现较差的，显然不能直接使用困惑度进行比较（数据集熵值不一样）\n结构化的数据（熵值低）困惑度天然会比非结构化的更低 困惑度可以用于衡量一个数据集是否更接近另一个数据集\n如CCNet，在维基百科内训练一个模型，计算其他数据集的困惑度 所以如果要比较的话，可以通过模型的损失值，拟合得越好，说明训练数据中包含了这部分，否则就是缺失\n如果钱多的话，当然是直接把所有数据集用模型train一下，看看损失值，与没有train过的原模型（GPT-3），在测试集上比一下Loss\npaper这里钱不够，改用了GPT2做了一个trick：\n首先需要知道GPT3比GPT2强多少\n参考数据集：OWT2（与GPT训练数据高度相似的一个数据集） 用原生的GPT3和在Pile训练的GPT2进行比较 得到一个基准差值 $$\rL^{GPT-3}_{OWT2} - L^{GPT-2-Pile}_{OWT2}\r$$ $$\rL^{GPT-3}_{TargetSet} - L^{GPT-2-Pile}_{TargetSet}\r$$ 两个值作差：大概能衡量出在目标数据集上的提升水平\nBooks3等数据集与GPT-3训练数据高度相似，因此不会有过多的提升（0） 清洗\r看不动了，以后再说，整理一下清洗的东西：\nC.1 Pile-CC（Clean Common Crawl）\r来源 ：Common Crawl 的 WARC 文件（2013–2020 年）。 提取工具 使用 jusText 提取网页正文，去除菜单、页脚等模板文本。 对比了 Trafilatura、Newspaper、Goose3、DragNet，最终选择 jusText。 语言过滤 使用 pycld2 检测网页语言，仅保留英文内容。 质量控制 使用 FastText 分类器对 OpenWebText2 和 Common Crawl 进行分类，过滤低质量页面。 参数 α = 3，使用 Pareto 分布阈值进行过滤。 去重 使用 MinHash LSH 算法在内存中进行文档级去重。 其他说明 未使用 WET 文件，因其包含大量模板文本。 与 Brown et al. (2020) 类似，但只处理了部分 WARC 文件。 C.2 PubMed Central（PMC）\r来源 ：美国国家生物技术信息中心（NCBI）提供。 格式转换 使用 Pandoc 将 JATS 格式转为 Markdown。 清理步骤 删除以 ::: 开头的行（Pandoc 添加的 HTML 类标签）。 C.3 Books3\r来源 ：未具体说明，但为高质量书籍数据集。 处理细节 ：无额外处理。 C.4 OpenWebText2（OWT2）\r来源 ：Reddit 提交链接。 处理步骤 提取 URL 及其元数据。 去除得分低于 3 的链接。 使用 Newspaper 抓取网页内容。 去重 使用 DataSketch 库进行文档级 MinHash LSH 去重。 C.5 ArXiv\r来源 ：arXiv.org 学术论文。 处理步骤 转换为纯文本。 去重 使用与验证/测试集对比的方法去重。 C.6 GitHub\r来源 ：GitHub 上的开源项目。 获取方式 收集星标数 \u0026gt; 100 的仓库。 提取内容 提取可用于语言建模的文本（代码、README、注释等）。 限制条件 单个仓库克隆和提取时间不超过 300 秒。 文件大小上限为 100KB（避免大文件中的重复自动生成内容）。 C.7 FreeLaw\r来源 ：法律数据库。 处理方式 未提供详细清洗步骤。 数据来自已有结构化格式，可能已做过预处理。 C.8 Stack Exchange\r来源 ：Stack Overflow 等问答网站。 处理方式 提取问题、回答、评论。 按照层级结构组织。 保留 /me 类型的动作描述，删除系统消息。 C.9 USPTO Backgrounds\r来源 ：美国专利商标局（USPTO）公开数据。 处理方式 处理 XML 格式的专利文件。 提取“Background”部分内容。 处理不同格式变化（APS → XML）。 C.10 PubMed Abstracts\r来源 ：PubMed 数据库摘要。 处理方式 排除缺失或格式错误的条目。 合并标题和摘要，去除版权信息。 排除已在 PMC 中出现的内容。 C.11 Project Gutenberg（PG-19）\r来源 ：古登堡计划电子书。 处理方式 ：无额外处理。 C.12 OpenSubtitles\r来源 ：Tiedemann (2016) 提供的英文字幕数据。 处理方式 提取 XML 文件中的字幕文本。 忽略元数据。 C.13 Wikipedia (en)\r来源 ：Wikipedia English dataset from TensorFlow Datasets。 处理方式 使用 wikipedia/20200301.en 数据集。 在每篇文章开头添加标题。 C.14 DeepMind Mathematics（DM Math）\r来源 ：DeepMind 数学数据集。 处理方式 包含 Easy、Medium、Hard 难度。 将每个题目拆分为 8 KiB 块。 C.15 Ubuntu IRC\r来源 ：Ubuntu IRC 日志（2004–2020）。 处理方式 删除系统消息（如加入、离开频道）。 保留 /me 动作。 去除时间戳。 每周日志合并为一个文档，按日期分隔。 C.16 BookCorpus2\r来源 ：基于 Kobayashi (2018) 方法重新构建。 处理方式 收集更多书籍（共 17,868 本，原版为 11,038 本）。 使用修改后的 EPUB 解析器提取文本。 C.17 EuroParl\r来源 ：欧洲议会会议记录。 处理方式 已经是干净文本，无需额外清洗。 C.18 HackerNews\r来源 ：Hacker News 提交链接。 处理方式 提取文章标题、URL、子标题、作者。 按照评论层级组织内容。 使用 html2text 提取 HTML 文本。 C.19 YouTube Subtitles\r来源 ：YouTube 视频字幕。 处理方式 三阶段构建： GPT-3 生成搜索关键词。 下载相关视频。 提取字幕并按时间对齐。 多语言字幕按分钟段落对齐，并标注语言。 C.20 PhilPapers\r来源 ：PhilPapers 数据库（哲学论文）。 处理方式 使用 OAI-MPH 协议抓取元数据。 转换为纯文本。 C.21 NIH ExPorter\r来源 ：NIH Grant Application 数据。 处理方式 合并 ExPORTER 和 CRISP 数据。 按申请 ID 去重。 删除空或太短的摘要。 去除行政模板内容。 C.22 Enron Emails\r来源 ：Enron 公司邮件存档。 处理方式 使用 mailparser 提取邮件正文作为文档。 ","date":"2025-06-06T13:12:32+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/","title":"大语言模型数据清洗 · 论文笔记（二）"},{"content":"CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data\rArXiv1911.00359 CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data\nGithub facebookresearch/cc_net: Tools to download and cleanup Common Crawl data\n概述\r为保证数据质量，倾向于通过维基百科等高质量数据来源合成ad-hoc datasets（专门构建的数据集），但是低资源语言不好做 paper从CC数据集出发，执行了FastText所提出的pipeline，但不同之处： 保留文档级别的结构，支持Bert等需要段落级别的模型训练 之前的方法切成单个句子，只关心局部上下文，切分成了n-gram 增加一个可选的单语言过滤 针对目标语言进行筛选 筛选出接近维基百科风格的文档 在目标语言的维基百科等语料上训练一个语言模型 通过困惑度进行文档打分，只保留那些 perplexity 较低的文档 与传统方法相比： 传统方法：多数只适用于英语的特殊方法，手动设置规则 paper：通用性强，适用于多种语言 Method\r下载网页快照的.WET文件，为每个段落生成哈希值，转化为.bin的二进制文件 独立处理每个WET下的文档，通过哈希进行去重，识别语言，计算困惑度 按照语言和困惑度得分重新分组，保存为 JSON 格式的文件 预处理\r每个快照包含大约 20 到 30TB 的未压缩纯文本 将 WET 文件分组为每个 5GB 的分片（shard），转化为json格式 json中的每一条entry：记录了url、文本等信息，代表了一个网页的内容 文本中含有段落 所以这里的逻辑是：快照（.WET） \u0026gt; shard \u0026gt; entry \u0026gt; 段落 去重\r需要删除不同网页之间的重复段落（占了70%），为方便去重：\n标准化\n字符全部小写\n所有数字变成0\n删除所有Unicode的Punctuation Marks（标点符号）、Accent Marks（重音符号），完成段落标准化\n标点\n英文标点：.,!?;:\u0026quot;'()[]{}-–—…@#$%^\u0026amp;* 中文标点：，。！？；：“”‘’（）【】《》…… 法语标点：«» 阿拉伯语标点：،؛؟ 日文标点：、。，・「」『』 Accent：表示发音变化或区分拼写\n法语：à, é, ô, ù, ç 西班牙语：ñ, á, é 德语：ä, ö, ü 波兰语：ą, ę, ś, ź 希腊语：ά, έ, ό 计算哈希 对每个shard的每个段落计算SHA哈希值（160位），保存为二进制文件.bin 每个段落的前64位作为其id便于维护 对每个段落，查询处理过的**一些（见后文）**shard的二进制文件，若出现过则舍弃，否则保存在本shard二进制文件中 由于很多步骤都是独立的，因此支持并行\n对于网页数据，需要去掉导航栏、cookie、联系信息等 语言识别\rFastText模型，基于 Wikipedia、Tatoeba 和 SETimes 数据集进行训练 支持176种语言，为每一种语言输出0-1的置信度（总和为1） 若某语言得分超过0.5则进行确认，否则舍弃（无法识别语言） 语言模型过滤\r对每种语言，训练了一个tokenizer和语言模型 KenLM 库实现的5-gram模型（处理大量数据效率高） 使用tokenizer对每一个entry进行分词，使用语言模型计算每个段落的困惑度 消融实验\r先去重再语言识别 可以去除一些英文的Cookie警告，防止误识别为英文 去重跨越的shard越多去除内容越多，去重效果越好，但是自然开销变大 选择50均衡了资源与性能 最终数据集的指标\r使用训练好的语言模型对段落进行困惑度（perplexity）评分，作为衡量文本质量的代理指标。 结果发现： 高质量内容（如新闻、写作规范的内容）通常位于数据集的“头部”（head） 含有关键词列表或与 Wikipedia 差异较大的口语化内容会落在“尾部”（tail） 不同语言的困惑度分布差异较大，这主要是由于训练语言模型所使用的 Wikipedia 数据大小不同，而不是某些语言本身缺乏高质量内容。 因此，为每种语言设置了不同的困惑度阈值，将语料库划分为三个部分： Head（头部） ：高质量段落 Middle（中部） Tail（尾部） ：较低质量段落 为了验证数据集的质量，作者使用 fastText 和 BERT 模型进行实验：\nfastText 实验\r对英语和波兰语的不同质量子集（head、mid、tail）训练词向量 在标准的类比任务数据集（Mikolov et al., 2013）上评估性能 结果表明 ：随着从 tail 到 head 的变化，模型性能逐步提升，说明基于困惑度的过滤方法能有效提升数据质量 子集 英语总分 波兰语总分 head 77.9 65.3 mid 74.2 62.8 tail 62.0 59.9 BERT 实验\r分别使用 Wikipedia 和 CCNet 提取的 head 数据训练 BERT-BASE 模型 训练语言包括：英语（en）、俄语（ru）、中文（zh）、乌尔都语（ur） 使用 XNLI 任务评估模型表现 语言 Wikipedia 准确率 CCNet 准确率 提升幅度 en 82.8 85.0 +2.2 ru 73.3 76.4 +3.1 zh 77.0 77.9 +0.9 ur 57.3 64.3 +7.0 ✅（显著提升） 特别是对于低资源语言乌尔都语（ur），Wikipedia 数据太小导致模型几乎无效，而使用 CCNet 提取的数据训练后，准确率提升了 7 个百分点 ，证明了该数据集对低资源语言预训练的重要性。\n维基百科的数据不足，CCNet从CC中提取了高质量语言专用的数据，效果显著\n","date":"2025-06-06T01:38:32+08:00","permalink":"https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/","title":"大语言模型数据清洗 · 论文笔记（一）"},{"content":"[TOC]\n面向对象\r类：抽象概念定义 属性：用于描述对象 方法：对象具有的行为 对象：类的实例 1 2 3 4 5 6 class Apple: price = 100 # 可以直接定义类变量 print(Apple.price) # 使用类名进行调用 Apple.quality = \u0026#39;good\u0026#39; # # 可以直接定义类变量 print(Apple.quality) 所有实例对象公用一个类变量：\n1 2 3 4 5 6 7 class Apple: price = 100 apple = Apple() print(apple.price) # 100 Apple.price = 200 print(apple.price) # 200 若定义类的方法，则必须要有一个必要参数：self，表示对象本身\n1 2 3 4 5 6 7 class Apple: def eat(self): print(\u0026#39;I am eating an apple\u0026#39;) apple = Apple() apple.eat() # \u0026#39;I am eating an apple\u0026#39; Apple.eat(apple) # \u0026#39;I am eating an apple\u0026#39; 其中实例可以有自己的属性（变量）：\n1 2 3 4 5 6 7 8 class Apple: def eat(self): print(\u0026#39;I am eating an apple\u0026#39;) apple = Apple() apple.size = 16 # 与类变量一致 可以在外部进行定义 print(apple.size) print(Apple.size) # 报错 但是这样的方法显然会让不同实例的属性不统一，需要进行自动化定义：\n构造函数__init__()\r1 2 3 4 5 6 class Apple: def __init__(self): self.eat = \u0026#39;I am an apple!\u0026#39; apple = Apple() print(apple.eat) 同样可以带上参数：\n1 2 3 4 5 6 7 8 9 10 class Apple: def __init__(self,size=12): self.eat = \u0026#39;I am an apple!\u0026#39; self.size = size apple = Apple() print(apple.size) apple2 = Apple(20) print(apple2.size) 析构函数__del__()\r1 2 3 4 5 6 7 8 9 class Apple: def __init__(self,size=12): self.eat = \u0026#39;I am an apple!\u0026#39; self.size = size def __del__(self): print(\u0026#39;Apple is being deleted!\u0026#39;) apple = Apple() del apple 垃圾回收机制：\nPython通过统计对象被引用次数（引用计数器），从而判断是否回收垃圾 但是回收不是\u0026quot;立即\u0026quot;的， 由解释器在适当的时机，将垃圾对象占用的内存空间回收 但是针对循环引用，会有额外的循环垃圾收集器 封装\r隐藏对象中不希望被外部访问的属性或方法 隐藏属性也称：魔术方法、特殊方法，通过开头结尾加上两个下划线：__\n构造和析构都是一种隐藏属性\n对于变量，可以在开头加个__，实现私有效果\n1 2 3 4 5 6 class Apple: public_age = 10 __private_age = 20 print(Apple.public_age) print(Apple.__private_age) # 报错 但是似乎留了一个后门：_类名__隐藏属性\n1 2 3 4 5 6 class Apple: public_age = 10 __private_age = 20 print(Apple.public_age) print(Apple._Apple__private_age) 标准的方式还是需要通过类的内部进行访问\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Apple: public_age = 10 __private_age = 20 def grow(self): self.__private_age += 1 def print(self): print(self.__private_age) apple = Apple() apple.print() apple.grow() apple.print() 有了私有，当然还有保护变量：单个下划线\n1 2 3 4 5 6 7 8 9 class Fruit: apple = True _banana = True __cherry = True fruit = Fruit() print(fruit.apple) # True print(fruit._banana) # True print(fruit.__cherry) # AttributeError: \u0026#39;Fruit\u0026#39; object has no attribute \u0026#39;__cherry\u0026#39; 私有变量只能被类内部的方法进行使用 建议别用，容易和python自带的属性冲突 主要用于防止子类意外覆盖父类属性。 保护变量可以被外部使用 建议只在内部使用，约定习惯，但是不禁止 公开变量无限制 类型 示例 特点 公开变量 var 可以直接访问，无任何限制 保护变量 _var 本身与子类可以访问，不能被其他文件from import 私有变量 __var 只允许本身访问 魔术方法 __var__ 双下划线开头和结尾，Python的特殊方法 方法\r静态方法 1 2 3 4 5 6 7 8 9 class Animal: def speak(self, times): print(\u0026#34;Animal speaks\u0026#34; * times) @staticmethod def eat(times): print(\u0026#34;Animal eats\u0026#34; * times) Animal.eat(3) 这样可以不创建实例直接用，同时不传self，减少内存\n类方法 1 2 3 4 5 6 7 class Animal: name = \u0026#39;Animal\u0026#39; @classmethod def eat(cls): print(cls.name) Animal.eat() 两种方法都可以用于规避实例的创建，静态方法不能访问类和实例，类方法可以访问类本身\n方法 参数 自动传参 访问 被调用 用途 实例方法 self 是 ✅ 类属性 / ✅ 实例属性 ✅ 实例 操作对象自身的状态 类方法 cls 是 ✅ 类属性 / ❌ 实例属性 ✅ 类 / ✅ 实例 工厂方法、操作类级别的数据 静态方法 无 否 ❌ 类属性 / ❌ 实例属性 ✅ 类 / ✅ 实例 与类相关的工具函数，不依赖类或实例 这里的访问其实不是说无法访问，事实上类名.属性都可以访问\n所以其实都是约定俗成\n继承\r1 2 3 4 5 6 7 8 class Fruit: # 其实默认继承object类（不然你以为默认自带的方法属性都是哪来的） size = 10 class Apple(Fruit): color = \u0026#39;red\u0026#39; # 子类如果拓展了新的属性 ——\u0026gt; 派生类 apple = Apple() print(apple.size) 重写\r子类中定义与父类相同名称的方法（覆盖） 1 2 3 4 5 6 7 8 9 10 class Fruit: def print(self): print(\u0026#39;I am a fruit\u0026#39;) class Apple(Fruit): def print(self): print(\u0026#39;I am a apple\u0026#39;) apple = Apple() apple.print() 重写可以用于拓展父类方法的功能：\n1 2 3 4 5 6 7 8 9 10 11 12 13 class Fruit: def print(self): print(\u0026#39;I am a fruit\u0026#39;) class Apple(Fruit): def print(self): # Fruit.print(self) # 调用父类方法（需要传入self） super().print() # 更推荐这种方法 # super(Apple, self).print() 等价 print(\u0026#39;I am a apple\u0026#39;) apple = Apple() apple.print() 理解上把super()当成一个函数，返回的是父类，这样没什么问题\n实际上是创建了一个super类的实例，动态查找MRO链，返回一个代理对象\nMRO = MRO（Method Resolution Order，方法解析顺序）是Python中决定类继承关系中方法调用顺序的规则。它是Python多继承机制的核心组成部分。\n方式 示例 区别 super() super().method() 动态查找 MRO 链，适用于多重继承 直接调用父类 Parent.method(self) 硬编码父类名，不适用于复杂继承 多继承\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 class Father(object): def __init__(self): print(\u0026#34;Father\u0026#39;s __init__\u0026#34;) def __del__(self): print(\u0026#34;Father\u0026#39;s __del__\u0026#34;) def print(self): print(\u0026#34;Father\u0026#39;s print\u0026#34;) class Mother(object): def __init__(self): print(\u0026#34;Mother\u0026#39;s __init__\u0026#34;) def __del__(self): print(\u0026#34;Mother\u0026#39;s __del__\u0026#34;) def print(self): print(\u0026#34;Mother\u0026#39;s print\u0026#34;) class Child(Mother, Father): def __init__(self): super().__init__() print(\u0026#34;Child\u0026#39;s __init__\u0026#34;) def __del__(self): super().__del__() print(\u0026#34;Child\u0026#39;s __del__\u0026#34;) child = Child() child.print() \u0026#39;\u0026#39;\u0026#39; Mother\u0026#39;s __init__ Child\u0026#39;s __init__ Mother\u0026#39;s print Mother\u0026#39;s __del__ Child\u0026#39;s __del__ \u0026#39;\u0026#39;\u0026#39; 构造与析构的顺序其实可以根据super()去控制\n这里print()和super()默认都是就近的Mother（写在第一个）\n调用其他父类需要显式的写出来\n1 2 3 4 def __init__(self): Mother.__init__(self) Father.__init__(self) print(\u0026#34;Child\u0026#39;s __init__\u0026#34;) 目测不实现构造函数应该也会自动实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class Father(object): def __init__(self): print(\u0026#34;Father\u0026#39;s __init__\u0026#34;) def __del__(self): print(\u0026#34;Father\u0026#39;s __del__\u0026#34;) def print(self): print(\u0026#34;Father\u0026#39;s print\u0026#34;) class Mother(object): def __init__(self): print(\u0026#34;Mother\u0026#39;s __init__\u0026#34;) def __del__(self): print(\u0026#34;Mother\u0026#39;s __del__\u0026#34;) def print(self): print(\u0026#34;Mother\u0026#39;s print\u0026#34;) class Child(Mother, Father): pass child = Child() print(Child.__mro__) \u0026#39;\u0026#39;\u0026#39; Mother\u0026#39;s __init__ (\u0026lt;class \u0026#39;__main__.Child\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.Mother\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;__main__.Father\u0026#39;\u0026gt;, \u0026lt;class \u0026#39;object\u0026#39;\u0026gt;) Mother\u0026#39;s __del__ \u0026#39;\u0026#39;\u0026#39; 多态\r同一种行为多个表现形式\n需要基于：\n继承 重写 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class Animal: def speak(self): print(\u0026#34;Animal speaks\u0026#34;) class Dog(Animal): def speak(self): print(\u0026#34;Dog barks\u0026#34;) class Cat(Animal): def speak(self): print(\u0026#34;Cat meows\u0026#34;) dog = Dog() dog.speak() # 重写 cat = Cat() cat.speak() # 重写 这里其实就只是重写\n1 2 3 4 5 def speak_loudly(animal: Animal): animal.speak() # 多态 speak_loudly(Dog()) # 调用 Dog 的 speak 方法 speak_loudly(Cat()) # 调用 Cat 的 speak 方法 这样就可以体现多态\n","date":"2025-06-02T20:15:32+08:00","permalink":"https://example.com/p/python-rookie-%E6%89%AB%E7%9B%B2-%E8%AF%AD%E6%B3%95%E8%BF%9B%E9%98%B6/","title":"Python Rookie 扫盲 · 语法进阶"},{"content":"[TOC]\n深浅拷贝\r1 2 3 4 5 6 7 8 9 10 11 a = [i for i in range(4)] b = a print(a) # 0 1 2 3 print(b) # 0 1 2 3 b[0] = 100 print(a) # 100 1 2 3 print(id(a)) # 1879915237696 print(id(b)) # 1879915237696 Python直接赋值相当于建立一个新的引用\n浅拷贝\r创建新的对象，只复制原对象本身，不复制原对象内部的子对象\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 a = [i for i in range(4)] from copy import copy b = copy(a) print(a) print(b) b[0] = 100 print(a) print(id(a)) print(id(b)) \u0026#39;\u0026#39;\u0026#39; [0, 1, 2, 3] [0, 1, 2, 3] [0, 1, 2, 3] # a未发生改变 2752117477696 # 地址不一样 2752117479552 \u0026#39;\u0026#39;\u0026#39; 之所以是浅拷贝，会体现到含有子对象的情况：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from copy import copy a = [ [i for i in range(5)], [i for i in range(5)], [i for i in range(5)], ] b = copy(a) print(id(a), id(b)) for ia, ib in zip(a, b): print(id(ia), id(ib)) \u0026#39;\u0026#39;\u0026#39; 2955264350656 2955262793600 2956986749248 2956986749248 # 子对象未发生变化 2956986751104 2956986751104 # 子对象未发生变化 2955300137024 2955300137024 # 子对象未发生变化 \u0026#39;\u0026#39;\u0026#39; 拷贝速度、占用空间显然会比深拷贝快 当只是简单创建副本，同时不存在内部资源修改时使用\n深拷贝\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from copy import deepcopy a = [ [i for i in range(5)], [i for i in range(5)], [i for i in range(5)], ] b = deepcopy(a) print(id(a), id(b)) for ia, ib in zip(a, b): print(id(ia), id(ib)) \u0026#39;\u0026#39;\u0026#39; 1640473937792 1640472338688 1640049480000 1640052597184 1640049481856 1640049786560 1640509759936 1640049482304 \u0026#39;\u0026#39;\u0026#39; 所有子对象都会被重新创建\n函数传参\r首先需要分两类：\n可变对象：列表、字典 不可变对象：数值、字符串、元组 可变对象作为引用进行传参，函数修改值，外部会发生改变\n1 2 3 4 5 6 def modify(x: list): x.append(42) x = [1, 2, 3] modify(x) print(x) # Output: [1, 2, 3, 42] 但是如果发生赋值，则也是重新创建一个对象\n1 2 3 4 5 6 def modify(x : list): x = [2, 3, 4] # 新创建 x = [1, 2, 3] modify(x) print(x) # Output: [1, 2, 3] 因此可以利用这个机制，使用.copy一个副本，进行重新赋值\n1 2 3 4 5 6 def modify(x : list): x = x.copy() x = [1, 2, 3] modify(x) print(x) 其中.copy()是内置可变容器类型（如list、dict、set）自带的浅拷贝方法\n不可变对象相当于重新创建一个副本，和原来没有任何关系\n1 2 3 4 5 6 7 8 def modify(x: str, y: int): x = x + \u0026#34;niko\u0026#34; y += 10 x = \u0026#34;hello\u0026#34; y = 5 modify(x, y) print(x, y) # Output: hello 5 函数\r函数参数\r位置参数：传入数量、位置需要一一对应 1 2 3 def fun(a, b, c): return a + b + c print(fun(1, 2, 3)) 默认参数（缺省参数default） 1 2 3 def fun(a, b, c=12): return a + b + c print(fun(1, 2)) 注意默认参数需要按顺序写在最后面\n可变参数 1 2 3 4 def fun(*args): # 加一个*就表示可变参数，args是约定俗成，可以更换其他名字 print(args) # 以元组形式打包 fun(1,2,3) # (1, 2, 3) 关键字参数 1 2 3 4 5 def fun(**kwargs): print(kwargs) fun(name=\u0026#39;hcf\u0026#39;, school=\u0026#39;dhu/ecnu\u0026#39;) # Output: {\u0026#39;name\u0026#39;: \u0026#39;hcf\u0026#39;, \u0026#39;school\u0026#39;: \u0026#39;dhu/ecnu\u0026#39;} 以键值对的形式传入，打包成字典\n1 2 3 4 5 def combined_example(*args, **kwargs): print(\u0026#34;位置参数 (args):\u0026#34;, args) print(\u0026#34;关键字参数 (kwargs):\u0026#34;, kwargs) combined_example(1, 2, 3, name=\u0026#34;Alice\u0026#34;, age=25) 当两者一起使用时，函数可以接受任意数量和类型的参数（位置参数 + 关键字参数）。 规则：*args 必须在 **kwargs 之前。\n作用域\r1 2 3 4 5 6 7 a = 100 print(a) # 100 def fun(): a = 200 print(\u0026#39;fun \u0026#39;, a) # 200 fun() print(a) # 100 函数内部的a是局部变量，与外部的a没有任何关系\n1 2 3 4 5 6 7 8 9 10 def fun(): a = 100 # print(a) error 局部变量超出作用域 if True: b = 10 print(b) # ok 这里b不是全局变量 所以这个和C++不太一样\n1 2 3 4 5 6 7 a = 100 def fun(): global a # 声明这个是全局变量，这样后续赋值不会认为是新的变量 a = 300 print(a) fun() print(a) lambda\r1 2 3 4 5 6 7 8 9 add_fun = lambda a,b : a+b print(add_fun(2,3)) add_fun = lambda a,b=5 : a+b # 带默认参数 print(add_fun(2)) fun = lambda **kwargs: kwargs print(fun(name=\u0026#39;hcf\u0026#39;, age=18, city=\u0026#39;beijing\u0026#39;)) # {\u0026#39;name\u0026#39;: \u0026#39;hcf\u0026#39;, \u0026#39;age\u0026#39;: 18, \u0026#39;city\u0026#39;: \u0026#39;beijing\u0026#39;} 内置函数\rzip之类的就不说了，写点自己不会的\nmap\r1 2 3 4 5 6 7 8 9 10 11 a = [1,2,3,4] fun = lambda a: a**2 b = [fun(i) for i in a] c = list(map(fun, a)) print(b, c) a = [1,2,3,4] b = [5,6,7,8] c = [9,10,11,12] d = list(map(lambda x, y, z: x + y + z, a, b, c)) print(d) map传入一个函数和若干个可迭代对象，把所有对象塞进去跑\n最后返回一个迭代器，可以直接list转换\n解包\r（不要管为什么放在这里）\n1 2 3 4 5 6 7 8 9 10 11 12 13 a = [1,2,3,4] a1, a2, a3, a4 = a print(a1, a2, a3, a4) print(*a) # 1 2 3 4 # 1 2 3 4 *a, = [1, 2, 3] # a = [1, 2, 3] *a, b = [1, 2, 3] # a = [1, 2], b = 3 first, *rest = [1, 2, 3] # first=1, rest=[2, 3] *a = [1, 2, 3] # SyntaxError: starred assignment target must be in a list or tuple # 因为这个写法非常奇怪，为什么不直接 a = [1,2,3] 异常\r1 2 raise Exception(\u0026#34;Error in test.py\u0026#34;) # 主动抛出异常 闭包与装饰器\r闭包\r闭包就是函数及其周边环境的组合\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 def outer_function(msg): # 外部函数的变量 message = msg def inner_function(): # 内部函数访问外部函数的变量 print(message) # 返回内部函数 return inner_function # 创建闭包 my_closure = outer_function(\u0026#34;Hello, 闭包!\u0026#34;) my_closure() # 输出: Hello, 闭包! 本质上记住了message = msg这一函数的中间状态，使得超出作用域时，仍然能正常运行\n1 2 3 4 5 6 7 8 9 10 11 12 def counter(): counter = 0 def increment(): nonlocal counter # 声明为上一级的变量 counter += 1 print(\u0026#39;counter:\u0026#39;, counter) return increment count = counter() count() # 输出: counter: 1 count() # 输出: counter: 2 count() # 输出: counter: 3 我们可以基于这个特性，使用闭包做一个计数器\n1 2 3 4 5 6 7 8 9 def power_factory(exponent): def power(base): return base ** exponent return power square = power_factory(2) cube = power_factory(3) print(square(5)) # 输出 25 print(cube(5)) # 输出 125 同时可以用闭包预设一些参数，批量生成函数\n装饰器\r装饰器可以给函数添加新功能，且满足：\n不修改原函数代码 不改变原函数调用方法 本质是一个闭包函数 首先我希望实现一个功能，在一个函数执行的前后，输出debug信息：\n1 2 3 4 5 6 7 8 9 def debug(fun): print(\u0026#39;debug start\u0026#39;) fun() print(\u0026#39;debug end\u0026#39;) def test(): print(\u0026#39;test function\u0026#39;) debug(test) 如果使用闭包的方法进行实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 def debug(func): def wrapper(): print(\u0026#39;debug start\u0026#39;) func() print(\u0026#39;debug end\u0026#39;) return wrapper def test(): print(\u0026#39;test function\u0026#39;) closure = debug(test) closure() 这样就非常清新自然，把test打包成另外一个带有调试信息的函数\n事实上有时候我们不需要引入额外的函数：\n1 2 test = debug(test) test() 这样我们就可以完成函数功能的动态修改\n此时，还有一种更加简单的语法糖写法——装饰器，与之等效：\n1 2 3 4 5 6 7 8 9 10 11 12 def debug(func): def wrapper(): print(\u0026#39;debug start\u0026#39;) func() print(\u0026#39;debug end\u0026#39;) return wrapper @debug def test(): print(\u0026#39;test function\u0026#39;) test() 后续你只需要通过注释这行代码就能决定修不修改函数功能\n被装饰的函数携带参数并不影响：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 def debug(func): # 所以wrapper就是被装饰后的函数本身 def wrapper(*args, **kwargs): # func需要什么参数全部一样传 print(\u0026#39;debug start\u0026#39;) func(*args, **kwargs) # 传参数 print(\u0026#39;debug end\u0026#39;) return wrapper @debug def test(*args, **kwargs): print(f\u0026#39;test function\u0026#39;) print(args) print(kwargs) test(1, 2, 3, a=4, b=5, c=6) \u0026#39;\u0026#39;\u0026#39; debug start test function (1, 2, 3) {\u0026#39;a\u0026#39;: 4, \u0026#39;b\u0026#39;: 5, \u0026#39;c\u0026#39;: 6} debug end \u0026#39;\u0026#39;\u0026#39; 带参数的装饰器\rdebug作为闭包函数，只能传入一个函数作为参数\n因此无法携带更多参数，所以我们为了能够给装饰器引入其他参数，需要再嵌套一层：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def decorator_with_args(arg1, arg2): def debug(func): def wrapper(*args, **kwargs): print(\u0026#39;debug start\u0026#39;) print(\u0026#39;Decorator arguments:\u0026#39;, arg1, arg2) # 装饰器的参数 func(*args, **kwargs) print(\u0026#39;debug end\u0026#39;) return wrapper return debug @decorator_with_args(\u0026#39;arg1_value\u0026#39;, \u0026#39;arg2_value\u0026#39;) def test(*args, **kwargs): print(f\u0026#39;test function\u0026#39;) print(args) print(kwargs) test(1, 2, 3, a=4, b=5, c=6) 元信息\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def test(): print(\u0026#39;test function\u0026#39;) print(test.__name__) # test def debug(func): def wrapper(): print(\u0026#39;debug start\u0026#39;) func() print(\u0026#39;debug end\u0026#39;) return wrapper @debug def test(): print(\u0026#39;test function\u0026#39;) test() print(test.__name__) # wrapper 使用装饰器后，原函数的__name__、__doc__等元信息会被wrapper函数覆盖\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 from functools import wraps def debug(func): @wraps(func) # 使用wraps（也是一个装饰器），把func作为参数传入 def wrapper(*args, **kwargs): print(\u0026#39;debug start\u0026#39;) func(*args, **kwargs) print(\u0026#39;debug end\u0026#39;) return wrapper @debug def test(*args, **kwargs): print(\u0026#39;function test\u0026#39;) test() print(test.__name__) # test wraps本身也是一个装饰器，功能大概是修改了wrapper的元信息\n应用\r应用上非常广泛\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def timing(func): @wraps(func) def wrapper(*args, **kwargs): import time start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;Function {func.__name__} took {end_time - start_time:.4f} seconds\u0026#34;) return result return wrapper @timing def test(*args, **kwargs): print(\u0026#39;function test\u0026#39;) test(1,2,3,4,5,6,7,8,9) 比如计时功能的添加\n1 2 3 4 5 @debug @timing def test(*args, **kwargs): print(\u0026#39;function test\u0026#39;) test(1,2,3,4,5,6,7,8,9) 可以堆叠\n注意：装饰器的应用顺序是从下往上的，即最靠近函数的装饰器最先应用。\n","date":"2025-05-28T16:38:32+08:00","permalink":"https://example.com/p/python-rookie-%E6%89%AB%E7%9B%B2-%E8%AF%AD%E6%B3%95%E5%9F%BA%E7%A1%80/","title":"Python Rookie 扫盲 · 语法基础"},{"content":"浅谈MinHash\r常用于网页、文本去重\n前置知识\rn-gram\r通过分词将文本分解为连续的n个单词或字符序列 例如：\n1 \u0026#34;This is an example sentence for n-gram extraction.\u0026#34; 表示为3-gram：\n1 2 3 4 5 6 (\u0026#39;This\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;an\u0026#39;) (\u0026#39;is\u0026#39;, \u0026#39;an\u0026#39;, \u0026#39;example\u0026#39;) (\u0026#39;an\u0026#39;, \u0026#39;example\u0026#39;, \u0026#39;sentence\u0026#39;) (\u0026#39;example\u0026#39;, \u0026#39;sentence\u0026#39;, \u0026#39;for\u0026#39;) (\u0026#39;sentence\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;n-gram\u0026#39;) (\u0026#39;for\u0026#39;, \u0026#39;n-gram\u0026#39;, \u0026#39;extraction\u0026#39;) 按照此方式可以将一段文本表示成一个n元组序列，每个元组可以考虑到一定的上下文\nJaccard相似度\r$$\r\\text{Jaccard}(A,B) = \\frac{|A\\cap B|}{|A \\cup B|}\r$$ 假设一共有$N$篇文档，每篇文档大小都是$M$，我们想知道两两文档之间的Jaccard值的开销是$O(N^2M)$\n对于超大规模的文档，一般来说$M$会占大头，文档的大小都非常恐怖\n流程\r因此MinHash的目标是将一篇大文档表示为一个较短的signature（假设大小为$\\text{numHash}$）\n本质上在做数据降维\n重要的前提：两个集合非常相似，那么对两个集合使用相同的变化，得到的变化结果也是相似的\n例如对于$A,B$两个集合，同时使用函数$\\min$，结果$\\min(A),\\min(B)$大概率也是相同的\n基于Hash的方法\r假设哈希就用最简单的\n1 2 def hash(x, a, b, p): return (a*x + b) % p 因为叫MinHash，所以取最小值\n1 2 def MinHash(document, hash): return min(hash(x) for x in document) $$\rP(\\text{minHash}(A) = \\text{minHash}(B)) = \\text{Jaccard}(A,B)\r$$ 为什么这个公式是对的？\n假设有30个小朋友，有的会唱歌、有的会跳舞，有的什么都会\n定义：\nA = 会唱歌的集合 B = 会跳舞的集合 A和B的并集即为全集 我们从A中挑选身高最高的小朋友，B中也挑选一个最高的小朋友\n两个小朋友是同一个小朋友的概率，等价于从全班中挑选一个身高最高的小朋友，他同时属于A和B\n我们还可以使用：\n体重 力气 成绩 等多种方式（本质上是多种哈希），将所有元素投影到一个方向，完成近似\n我们只要使用多个hash函数进行测试，就可以估计出$\\text{Jaccard}(A,B)$\n$$\r\\text{Jaccard}(A,B) \\approx \\frac{\\sum^{numHash}[\\text{minHash}(A,hash_i) = \\text{minHash}(B,hash_i)]}{numHash}\r$$实际操作中略微复杂，会更偏向于使用随机打乱的方法\n基于随机打乱的方法\r将$N$个文档中的所有token抽取出来去重，假设是一个长度为$M$的集合\n那么每个文档自然可以表示为一个$M$维的01向量\n我们把所有文档写在一起就是一个 $M\\times N$的矩阵\n例如：上山打老虎和老虎不在家\n也就是说每个token对应的下标顺序如果不一致，我们表示每个文档的向量也是不一样的\n我们进行一次打乱得到：\n此时我们的Min选择表示向量出现第一个1的idx\n例如第一张图：A是0，B是3；第二张图：A是0，B是1\n我们进行多次这样的打乱排序\n并统计他们相等的次数，除以总次数，就是估计出来的值\n在打乱前只执行一次Hash即可，与前文的方法相比，是避免了多次hash计算，最终效果相似\n","date":"2025-04-07T16:31:31+08:00","permalink":"https://example.com/p/%E6%B5%85%E8%B0%88minhash/","title":"浅谈MinHash"},{"content":"OPENCSG CHINESE CORPUS A SERIES OF HIGHQUALITY CHINESE DATASETS FOR LLM TRAINING\rDataset：https://huggingface.co/collections/opencsg/chinese-fineweb-66cfed105f502ece8f29643e\nCode：https://github.com/yuyijiong/fineweb-edu-chinese\nPaper：[2501.08197] OpenCSG Chinese Corpus: A Series of High-quality Chinese Datasets for LLM Training\n[TOC]\nFineWeb-Edu-Chinese\rHugging Face的数据集主页有一些表述和论文是不同的\nopencsg/chinese-fineweb-edu · Datasets at Hugging Face\nFineWeb-Edu-Chinese 数据集的构建流程在很大程度上遵循了FineWeb-edu的策略 FineWeb-edu从15TB的FineWeb语料库进行筛选 重点关注数据的教育价值和内容质量 中文数据相对匮乏，整合了多个开源中文语料库 These datasets were selected for their diversity and their educational and technical relevance.\n由上述语料库构建了Original Data Pool\n首先从教育相关性的方向进行过滤：\n从CCI2数据集中抽取100万个条目 使用Qwen2-7b-instruct按照附录A.1的提示词，对样本的教育价值打分0-5，完成数据标注 使用这些打分数据，微调bge-rerank-zh，添加一个线性回归层，得到一个filter 利用filter排除所有语料库中分数低于3的数据 opencsg/chinese-fineweb-v2-scorer-train-data · Datasets at Hugging Face\n同时为了去重，采用重叠阈值0.7的Min-Hash算法（平衡计算效率、数据多样性）\n最后得到：\nFineweb-Edu-Chinese数据集包含 8900 万个高质量样本，为教育和技术应用提供了丰富的资源。 附录A.1提示词：\n1 2 3 4 5 6 7 8 9 10 以下是一段网页内容摘录。请使用以下5分制评分系统来评估该网页的写作水平、教育价值和实用性: 0分：如果网页没有提供任何教育价值,完全由无关信息(如广告、宣传材料、少儿不宜内容)组成。 1分：如果网页提供了一些可能有教育价值的基本信息,即使包含一些无关或非学术内容(如广告和宣传材料)。 2分：如果网页涉及某些与教育相关的元素,但与教育标准不太吻合。它可能将教育内容与非教育材料混杂,对潜在的有用的主题进行浅显概述,或以不连贯的写作风格呈现信息。 3分：如果网页适合教育使用,并介绍了与某些学校课程中可能学到的关键概念，或对个人发展有用的实用信息。它的内容连贯但可能不全面,或包含一些无关信息。它可能类似于教科书的一小段节选,可以学习但有明显局限,如涉及过于复杂的概念、过于具体的不重要事件。 4分：如果网页与教育高度相关，对个人学习发展有益,表现出清晰一致的写作风格。它可能类似于教科书的一个章节或教程,提供大量教育内容,极少包含无关信息,且概念对学生来说不会过于深奥。内容连贯、重点突出,对结构化学习有价值。 5分：如果网页摘录在教育价值上表现极好,完全适合小学、中学或大学教学或专业人士学习。它遵循详细的推理过程,写作风格易于理解,对主题提供深刻而全面的见解,不包含任何非教育性或无实用意义内容。 网页内容摘录: {data} 在审查这段网页摘录后：请简要地为您的评分进行合理的解释，最多不超过100字，最后以“教育得分：【分数】”的格式结束。请根据所列出的标准系统地赋予分数。 FineWeb-Edu-Chinese-V2\r进一步扩展了语料库，添加了这个那个和那个……数据集 Qwen2.5-14b-instruct更换了Qwen2-7b-instruct 1 2 3 4 5 6 7 8 9 10 11 12 以下是一段网页内容摘录。请使用以下5分制评分系统来评估该网页的写作水平、教育价值和实用性: 0分：如果网页没有提供任何教育价值,完全由无关信息(如广告、宣传材料、少儿不宜内容)组成。 1分：如果网页提供了一些可能有教育价值的基本信息,但包含较多的无关或非学术内容(如广告和宣传材料)。 2分：如果网页涉及某些与教育相关的元素,但与教育标准不太吻合。它可能将教育内容与非教育材料混杂,对潜在的有用的主题进行浅显概述,或以不连贯的写作风格呈现信息。 3分：如果网页适合教育使用,并介绍了与某些学校课程中可能学到的关键概念，或对个人发展有用的实用信息。它的内容连贯但可能不全面,或包含一些无关信息。它可能类似于教科书的一小段节选,可以学习但有明显局限,如涉及过于复杂的概念、过于具体的不重要事件。 4分：如果网页与教育高度相关，对个人学习发展有益,表现出清晰一致的写作风格。它可能类似于教科书的一个章节或教程,提供大量教育内容,极少包含无关信息,且概念对学生来说不会过于深奥。内容连贯、重点突出,对结构化学习有价值。 5分：如果网页摘录在教育价值上表现极好,完全适合小学、中学或大学教学或专业人士学习。它遵循详细的推理过程,写作风格易于理解,对主题提供深刻而全面的见解,不包含任何非教育性或无实用意义内容。 网页内容摘录: {} 在审查这段网页摘录后：请简要地为您的评分进行合理的解释，最多不超过100字，最后以“教育得分：\u0026lt;分数\u0026gt;”的格式结束。请根据所列出的标准系统地赋予分数。 打分的分布如下，最终选择3以上的数据：\nCosmopedia-Chinese\r种子数据来源：\n560万百度百科条目 100万个知乎问答样本 200万个技术博客条目 丰富领域知识、较高的信息密度\n中文数据池中的网页文本：质量不够高，含有广告 种子数据：for example an extract from a web page\nCosmopedia-v2的实验说明更大的模型生成数据有显著效果\nqwen2-7b-instruct、yi-1.5-9b-chat：倾向于输出简洁、通用的内容，如摘要、大纲（提示词也没救） 最终选择glm4-9b-longwriter：教科书主要内容那样足够详细和具体的内容 生成：\n生成了各种体裁的合成样本，如教科书单元、叙事故事和详细的 “操作指南” 温度0.8保证多样性 对2000万个样本进行Min-Hash去重，保留1500万个 1 2 3 4 5 6 7 8 9 10 11 12 这是一段来自网页的摘录： “{data}” 请编写一个针对大学生的足够详细的教科书课程单元，该单元与给定的摘录中的某个概念或多个概念相关。 不需要包含摘录中的所有内容，只需要发掘其中适合作为教科书内容的部分。你可以自由补充其他相关知识。 不能仅仅列出概念，而是要深入发展和详细探讨每个概念，因为我们优先考虑深入理解主题内容，而不是广度。 要求： 1. 严谨性：确保对概念/章节的深入覆盖。 2. 吸引性：用学术、专业且引人入胜的语气撰写，以吸引兴趣。 3. 应用：融入具体的实践例子，例如微积分中要给出公式、严格证明，历史中要给出关键日期和人物，计算机操作中要给出代码。 4.不需要给出参考文献。内容中不应包含广告或涉及隐私的信息。注重主体内容，不需要其它格式化的内容。 请记住，要针对大学生制作内容，他们可能拥有一些基础知识，但不是该领域的专家。内容应该详细且发人深省。 请立即开始撰写教科书,不要使用图片,不要输出除了教科书以外的内容,不要以“课程单元”作为标题而是要有具体的标题｡ 以及其他写故事、教程、教科书的提示词\nSmoltalk-Chinese\r基于 Magpie-ultra-1M和Smoltalk的方法构建，提升任务多样性和对话深度\n引入了 7 个额外的任务类别：格式约束、总结、改写、文档QA、安全QA、翻译和日常对话。 确保了对与自然语言理解和生成相关的任务有更广泛的覆盖。 使用Deepseek-V2.5、Qwen2.5-72B-Insturct等较为先进的模型 为Magpie-ultra-1M使用的11个任务类别生成3轮对话、新任务类别（除日常对话）1轮对话、日常对话5轮 对于质量筛选，首先要保证用户的第一个命令语句是流畅、连贯、清晰的，使用Qwen2.5-7b-instruct打分，保留超过3分的\n使用gte-zh-large编码的嵌入进行去重\n","date":"2025-04-03T15:10:53+08:00","permalink":"https://example.com/p/fineweb-edu-chinese%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/","title":"FineWeb-Edu-Chinese数据集论文记录"},{"content":"The FineWeb Datasets Decanting the Web for the Finest Text Data at Scale\r[2406.17557] The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale\nCommon Crawl：网站快照集合，自 2007 年开始运行。截至撰写本文时，Common Crawl 已经生成了 100 个网页快照，数据总量达到 PB 级别。 Building FineWeb\rText Extraction\rCommon Crawl 数据有两种不同格式\nWARC WET（ WET 文件保留了过多的模板和菜单文本） 使用开源的trafilatura库从 WARC 文件中提取文本内容\n实验证明WARC明显效果好于WET\nBase Filtering\r采用了 RefinedWeb\nURL Filter：通过url过滤成人内容 fastText 语言分类器：保留得分大于等于0.65的英文文本 MassiveText的质量和重复过滤器 分词器采用GPT-2 分词器\nDeduplication\r去重与提升模型性能、减轻预训练数据记忆负担相关\n常见去重方法：\n哈希技术 后缀数组 去重方法还可以分为：\n模糊：相似性度量 精确：完全匹配 使用英文分词器提取每个文档5-gram，使用112个哈希函数计算MinHash值\n分成14个桶，每个桶8个哈希函数，识别相似度至少75%的文档 任意一个桶全部相同的哈希值，视为彼此的重复文档\nA、B重复，A、C重复，则根据传递性认为B、C重复（哪怕B、C没有哈希冲突）\n所有重复文档归为一个重复文档簇，最终随机保留一个\n按照这个方法从最新的快照开始，直到最旧的快照，对整个数据集（96个快照）进行全局的哈希处理\n但是对1base filter1的提升效果不佳，与RefinedWeb数据集效果落后明显 选择了某个快照进行对比实验：\nA：全局去重后，取该快照进行去重后的数据 B：这个快照被去除的数据，进行独立去重（不考虑其他快照） B效果明显比A好：原始保留数据包含更多广告、无序关键词列表和格式混乱的文本。\n因此方法更换为：对每个快照单独进行MinHash去重\n此时效果与RefinedWeb相当\n假设：\n去重的主要改进是移除了包含10w以上规模的文档的大型重复文档 对于少量重复文档（小于100），过度去重损害质量 C4 Filter\rBase Filter + Deduplication得到的数据集已经与RefinedWeb相当\n但是C4数据集虽然更小，但是性能还是更强\nC4的过滤规则：\n删除不以终止标点符号结尾的行 删除提及\u0026quot;javascript\u0026quot;的行 删除包含\u0026quot;使用条款\u0026quot;/\u0026ldquo;cookie政策\u0026quot;声明的行 删除过短的文档 删除包含\u0026quot;lorem ipsum\u0026quot;或花括号（{）的文档 对这些规则分别进行消融实验，发现删除不以终止标点符号结尾的行会导致过多数据损失\n删除此规则，保留其他规则\nDeveloping additional heuristic filters\r除了人工目视方法进行过滤，开发一套系统的启发式过滤器\n收集了50多项高级统计指标，涵盖： 文档级指标（如行数、平均行/词长度等） 文档间重复指标 选择某个快照的两个去重版本：\n高质量：独立去重后的快照 低质量：全局去重的快照 通过分析这些指标的直方图分布，经验性地选择在低质量数据集频率明显高于对应高质量数据集的区间设置阈值\n最终确定了16组候选的指标-阈值组合 “字符重复过滤器”：我们注意到存在一些包含大量无意义的字符重复（例如，连续重复的字母或符号）的文本片段 “短单词过滤器”：非常短的单词（例如，只有一个或两个字符的单词）组成的文本行 “特殊字符过滤器”：一些文本行包含大量的特殊字符 规则：\n行末标点比例≤0.12的文档（移除10.14% token，相比C4终止标点过滤的30%更高效） 重复行字符比例≥0.1的文档（移除12.47% token） 短行（\u0026lt;30字符）比例≥0.67的文档（移除3.73% token） 这个过程避免了主观判断，而是根据数据系统地制定指标及其阈值\nFinal\r通过整合前文各环节的设计决策，我们将完整的处理流程应用于96个Common Crawl快照，最终构建出包含15万亿token的FineWeb数据集。具体处理步骤如下：\n文本提取（3.2节）：从WARC文件提取文本内容 基础过滤（3.3节）：应用URL过滤、语言识别和质量筛选 单快照去重（3.4节）：对每个快照独立进行MinHash去重 C4过滤规则（3.5节）：采用精选的C4启发式过滤规则 定制过滤（3.6节）：应用新开发的高效过滤规则 FineWeb-Edu\r使用LLM进行进一步的构建：\n由 Llama-3 70B-Instruct 生成的合成注释所开发的教育质量分类器对其进行过滤 为了构建合成注释：\n使用LLM对教育质量进行评分（0-5），发现累加评分量表效果最好\n累加评分量表允许大语言模型评估每个标准，并逐步得出评分 为避免大语言模型偏爱诸如 arXiv 摘要和提交内容这类专业性很强的页面，我们提示它专注于小学和中学水平的知识 1 2 3 4 5 6 7 8 9 基础信息相关（+1分）：若摘录包含与教育主题相关的基本信息，即使掺杂广告等无关内容。 部分教育相关但松散（+1分）：内容涉及教育元素但未紧密契合标准，可能混杂非教育材料或呈现零散。 适合教学且引入关键概念（+1分）：内容连贯且符合课程要求，但可能不够全面或含额外信息，类似教科书基础章节。 高度相关且有益（+1分）：内容清晰一致，适合小学至初中，含练习题等实质性材料，无关内容极少。 教育价值突出（+1分）：内容深入透彻，完全适合目标学段，无无关或复杂信息。 选择最低阈值为3分，高于此分数认为属于教育内容\n","date":"2025-04-03T15:10:42+08:00","permalink":"https://example.com/p/fineweb%E6%95%B0%E6%8D%AE%E9%9B%86%E8%AE%BA%E6%96%87%E8%AE%B0%E5%BD%95/","title":"FineWeb数据集论文记录"},{"content":"CritiQ 记录\r[TOC]\n目标\r训练模型，由模型判断语料文本数据的质量（高/低），完成质量检测，帮助后续模型训练 训练数据准备\r至少需要300正例+300负例，希望是人类专家的标注 在训练数据不足1w的情况，使用CritiQ去做数据标注\n传统训练都是训练集\u0026gt;测试集\n这里是通过挖掘人类偏好，测试集合远大于训练集，使用训练集挖掘出来的指标，对测试集进行标注\n当训练数据超过1w的时候，可以直接跳过这个部分去做分类模型训练\nKnowledge Base\r对于数据质量评估，如果完全交由模型生成评估指标，从各个角度来说都很水\n参考：2502.19279] CritiQ: Mining Data Quality Criteria from Human Preferences\nworkflow：\n从huggingface的数据集中寻找参考论文列表 从arxiv能找到的论文中爬取摘要 通过制作agent分析摘要，判断论文中是否有关于数据集评价等内容 提取评估指标 最后论文作者得到了一个370+数量的指标\n根据不同的待测数据集，跑一遍，选择准确度较高的\n数量不够让模型生成\nTraining\rgithub\n环境\rcuda 12.4 + torch 2.6 + Python3.10\n1 2 3 conda create -n critiq_env python=3.10 -y conda activate critiq_env pip3 install torch torchvision torchaudio 1 python -c \u0026#34;import torch; print(\u0026#39;PyTorch版本:\u0026#39;, torch.__version__); print(\u0026#39;GPU可用:\u0026#39;, torch.cuda.is_available()); print(\u0026#39;GPU名称:\u0026#39;, torch.cuda.get_device_name(0) if torch.cuda.is_available() else \u0026#39;无可用GPU\u0026#39;)\u0026#34; 1 2 3 git clone https://github.com/KYLN24/CritiQ cd CritiQ pip install -e \u0026#34;.[vllm,train]\u0026#34; 之后如果网络不太行，换一个源：\n1 2 import os os.environ[\u0026#34;HF_ENDPOINT\u0026#34;] = \u0026#34;https://hf-mirror.com\u0026#34; 数据集\rPair Data格式如下：\n1 2 3 4 class PairData(TypedDict): A: str B: str answer: Literal[\u0026#34;A\u0026#34;, \u0026#34;B\u0026#34;] 训练数据格式（保存为train_reward.jsonl）：\n1 2 {\u0026#34;A\u0026#34;: \u0026#34;金属的导热系数显著高于木材（铁约80 W/m·K vs 木材约0.1 W/m·K），根据傅里叶定律q=-k∇T，更高的导热系数导致更快的热传导速率，使手部热量迅速流失。\u0026#34;,\u0026#34;B\u0026#34;: \u0026#34;金属传热特别快，能迅速把你手上的热量带走，所以摸起来凉；木头传热慢，热量不容易流失，感觉就没那么凉。就像夏天坐金属凳子比坐木凳子感觉更凉快。\u0026#34;,\u0026#34;answer\u0026#34;: \u0026#34;B\u0026#34;} {\u0026#34;A\u0026#34;: \u0026#34;由牛顿第一定律可知，物体保持原有运动状态。当汽车以加速度a减速时，乘客因惯性保持原速v，直到受到座椅摩擦力f=μN的作用才减速。\u0026#34;,\u0026#34;B\u0026#34;: \u0026#34;因为你的身体想保持原来的运动状态。车停了，但你的身体还在往前，所以会往前倾。就像跑步时突然停下，身体还会往前冲一样。\u0026#34;,\u0026#34;answer\u0026#34;: \u0026#34;B\u0026#34;} Reward模型输入格式（保存为predict_reward.jsonl）：\n1 2 3 4 5 6 {\u0026#34;text\u0026#34;: \u0026#34;金属的导热系数显著高于木材（铁约80 W/m·K vs 木材约0.1 W/m·K），根据傅里叶定律q=-k∇T，更高的导热系数导致更快的热传导速率，使手部热量迅速流失。\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;金属传热特别快，能迅速把你手上的热量带走，所以摸起来凉；木头传热慢，热量不容易流失，感觉就没那么凉。就像夏天坐金属凳子比坐木凳子感觉更凉快。\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;由牛顿第一定律可知，物体保持原有运动状态。当汽车以加速度a减速时，乘客因惯性保持原速v，直到受到座椅摩擦力f=μN的作用才减速。\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;因为你的身体想保持原来的运动状态。车停了，但你的身体还在往前，所以会往前倾。就像跑步时突然停下，身体还会往前冲一样。\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;光从光密介质（水，n=1.33）进入光疏介质（空气，n≈1）时发生折射，根据斯涅尔定律n₁sinθ₁=n₂sinθ₂，折射角大于入射角导致视觉偏移。\u0026#34;} {\u0026#34;text\u0026#34;: \u0026#34;因为光从水进入空气时会拐弯，让我们看到的位置和实际位置不一样，所以筷子看起来像是弯的。就像把吸管插进水里也会看起来弯折一样。\u0026#34;} 一共准备了137条数据：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 请生成100个初中物理现象问题与解答，要求： 1. 每个问题包含两种回答： - **A**：专业严谨的物理解释（含公式、原理名称、数据等） - **B**：通俗易懂的讲解（用比喻、生活实例，语言口语化） - **answer**：固定标注\u0026#34;B\u0026#34; 2. 数据格式：每行一个完整JSON对象，严格遵循以下结构： {\u0026#34;A\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;B\u0026#34;} 3. 内容要求： - 物理现象需贴近日常生活（如热学、光学、力学等） - B回答必须正确且易于理解，避免专业术语 - 示例类比需直观（如\u0026#34;像磁铁吸住\u0026#34;\u0026#34;像气球爆炸\u0026#34;） 4. 输出：直接给出结果，无需注释，禁止使用```包裹 示例格式： {\u0026#34;A\u0026#34;: \u0026#34;金属导热系数较高（铁80 W/m·K），根据傅里叶定律q=-k∇T...\u0026#34;, \u0026#34;B\u0026#34;: \u0026#34;金属像‘传热快手’，迅速带走热量，所以摸起来更凉\u0026#34;, \u0026#34;answer\u0026#34;: \u0026#34;B\u0026#34;} 由模型生成\n先做一个预测：\n1 python ./critiq/scripts/reward_predict.py --model Qwen/Qwen2.5-1.5B-Instruct --data ./data/predict_reward.jsonl --output_dir ./output 输出结果在output\n1 2 3 4 5 6 {\u0026#34;text\u0026#34;: \u0026#34;金属的导热系数显著高于木材（铁约80 W/m·K vs 木材约0.1 W/m·K），根据傅里叶定律q=-k∇T，更高的导热系数导致更快的热传导速率，使手部热量迅速流失。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9465966820716858} {\u0026#34;text\u0026#34;: \u0026#34;金属传热特别快，能迅速把你手上的热量带走，所以摸起来凉；木头传热慢，热量不容易流失，感觉就没那么凉。就像夏天坐金属凳子比坐木凳子感觉更凉快。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9046503901481628} {\u0026#34;text\u0026#34;: \u0026#34;由牛顿第一定律可知，物体保持原有运动状态。当汽车以加速度a减速时，乘客因惯性保持原速v，直到受到座椅摩擦力f=μN的作用才减速。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9736446738243103} {\u0026#34;text\u0026#34;: \u0026#34;因为你的身体想保持原来的运动状态。车停了，但你的身体还在往前，所以会往前倾。就像跑步时突然停下，身体还会往前冲一样。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9664104580879211} {\u0026#34;text\u0026#34;: \u0026#34;光从光密介质（水，n=1.33）进入光疏介质（空气，n≈1）时发生折射，根据斯涅尔定律n₁sinθ₁=n₂sinθ₂，折射角大于入射角导致视觉偏移。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9532749652862549} {\u0026#34;text\u0026#34;: \u0026#34;因为光从水进入空气时会拐弯，让我们看到的位置和实际位置不一样，所以筷子看起来像是弯的。就像把吸管插进水里也会看起来弯折一样。\u0026#34;, \u0026#34;reward\u0026#34;: 0.9845753908157349} 给的分数都很高，因为两种风格的文本质量都不低\n为了让模型出现人类的偏好，我们开始训练一下：\n1 python ./critiq/scripts/train_reward.py --data ./data/train_reward.jsonl --eval_steps 40 eval_steps一定要开大一点……原代码是1，每一步保存一个模型，磁盘炸的很快\n训练的模型保存路径：\u0026lt;output_dir\u0026gt;/\u0026lt;job_name\u0026gt;\n我这里是./output/tmp\n再跑一下预测：\n1 python ./critiq/scripts/reward_predict.py --model ./output/tmp/checkpoint-109 --data ./data/predict_reward.jsonl --output_dir ./output 还没fixed的bug：\npredict不知道为什么给的结果永远是：0.9以上\n我直接修改了train.py，训练完直接跑预测，结果是对的（\n1 2 3 4 5 6 7 {\u0026#34;text\u0026#34;: \u0026#34;突然关水龙头像‘管道撞车’，水流‘刹车不及’撞出巨响\u0026#34;, \u0026#34;reward\u0026#34;: 0.9999999586005831} {\u0026#34;text\u0026#34;: \u0026#34;磁致伸缩ΔL/L≈10^-5（镍），逆效应用于超声换能器\u0026#34;, \u0026#34;reward\u0026#34;: 2.2827974365423323e-05} {\u0026#34;text\u0026#34;: \u0026#34;磁致伸缩材料像‘会呼吸的磁铁’，磁场一变就‘伸缩’\u0026#34;, \u0026#34;reward\u0026#34;: 0.9999999123574378} {\u0026#34;text\u0026#34;: \u0026#34;防晒霜氧化锌散射UV，粒径d≈20 nm最优\u0026#34;, \u0026#34;reward\u0026#34;: 1.568953403843032e-05} {\u0026#34;text\u0026#34;: \u0026#34;物理防晒像‘纳米镜子’，把紫外线‘弹弹球’般反射\u0026#34;, \u0026#34;reward\u0026#34;: 0.9999999918479713} {\u0026#34;text\u0026#34;: \u0026#34;橡皮船浮力F=ρgV，PVC材料ρ≈1.4 g/cm³\u0026#34;, \u0026#34;reward\u0026#34;: 1.0129980850990629e-05} {\u0026#34;text\u0026#34;: \u0026#34;充气船像‘塑料泡泡’，空气‘内胆’让它漂水面\u0026#34;, \u0026#34;reward\u0026#34;: 0.9999985406223194} ","date":"2025-04-03T15:06:24+08:00","permalink":"https://example.com/p/critiq-%E5%B7%A5%E4%BD%9C%E6%96%87%E6%A1%A3/","title":"CritiQ 工作文档"},{"content":"Hugo + Github 笔记\r【教程】Hugo+Github博客部署\n文章管理\r放在最前面方便看\n创建文章：\n1 hugo new post/一级分类/二级分类……/post_name/index.md 在post/一级分类/二级分类……/post_name/下新建一个文件夹用于存图（typora自己建好了）\n注意：\n1 ![image-20250403143026963](./assets/image-20250403143026963.png) 要改成：\n1 ![image-20250403143026963](assets/image-20250403143026963.png) 否则识别不到\n只有封面图片可以直接放在直接文件夹下\n定义分类、标签：\n1 2 3 4 5 6 7 8 9 10 tags : [ \u0026#34;markdown\u0026#34;, \u0026#34;css\u0026#34;, \u0026#34;html\u0026#34;, \u0026#34;themes\u0026#34;, ] categories : [ \u0026#34;themes\u0026#34;, \u0026#34;syntax\u0026#34;, ] 放在最前面的模板\ntag可以随便取，分类建议大一点\n不需要其他指令去单独建立tag或分类，直接文章里用到就会自动建立\n相册语法\rImages in Markdown - Typlog Docs\n1 2 3 ![Photo by Florian Klauer on Unsplash](florian-klauer-nptLmg6jqDo-unsplash.jpg) ![Photo by Luca Bravo on Unsplash](luca-bravo-alS7ewQ41M8-unsplash.jpg) ![Photo by Helena Hertz on Unsplash](helena-hertz-wWZzXlDpMog-unsplash.jpg) ![Photo by Hudai Gayiran on Unsplash](hudai-gayiran-3Od_VKcDEAA-unsplash.jpg) 效果： Install\r先装一下git\n下载hugo的zip：Releases · gohugoio/hugo\n教程选的是hugo_extended_xxx_windows版本\n把压缩包里的hugo.exe拖到自己的某个文件夹内\n1 hugo new site sci-biribiri 创建文件夹\n然后cd进去，并且把hugo.exe也拖进去这个文件夹方便使用\n1 cd sci-biribiri 运行服务：\n1 hugo server -D 这个时候某个端口就会开始运行hugo，不出意外显示一个Page Not Found\n主题部署\rComplete List | Hugo Themes\n我选的这个主题：CaiJimmy/hugo-theme-stack: Card-style Hugo theme designed for bloggers\n在releases下载source code进行解压\n解压到sci-biribiri/themes目录下\n把主题文件内的exampleSite/content和exampleSite/hugo.yaml直接复制到主文件夹sci-biribiri\n如果你是stack主题，请删掉content/post/rich-content/，否则会无法启动\n删掉主文件夹sci-biribiri的hugo.toml，到此完成配置文件的替换\n注意到此时hugo.yaml是这样的：\n1 2 3 4 5 baseurl: https://example.com/ languageCode: en-us theme: hugo-theme-stack title: Example Site copyright: Example Person 识别的主题是：hugo-theme-stack\n因此要确保sci-biribiri/themes下的主题文件夹的名字要对应上\n我这里修改sci-biribiri/themes/hugo-theme-stack-3.30.0为sci-biribiri/themes/hugo-theme-stack\n1 hugo server -D 后面就是慢慢自己调内容\n记得修改archetypes\\default.md，每次创建文章的模板是按照这个模板做的\n1 2 3 4 5 6 7 8 9 title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; # 标题，创建时自动填充 description: # 文章简介 date: {{ .Date }} # 日期，创建时自动填充，格式同 2023-01-15T12:00:00+08:00 image: # 文章的封面，留空就是没有，填文章所在位置的相对地址，通常放在同目录下， math: # 是否启用 KaTex，填 true 启用 license: # 文章尾部显示的协议，false 为隐藏，其他作为内容，留空就是使用 config.yaml 里默认的 hidden: false # 是否隐藏，一般用不到 comments: true # 因为 bug 所以这个属性只要存在，不管是 true 还是 false 都会导致回复无法显示，需要删掉 draft: true # 是否为草稿，建议改为 false 或者删掉这个属性以防止忘记修改，毕竟我们一般都是写好了才部署到服务器上 推荐：\n1 2 3 4 5 6 7 8 title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; description: date: {{ .Date }} image: math: true draft: false tags: [] # 双引号 categories: [] # 双引号 ","date":"2025-04-03T13:57:51+08:00","image":"https://example.com/p/read-me/bg_hu_c910bc4cbedd0ff0.jpg","permalink":"https://example.com/p/read-me/","title":"Read Me"},{"content":"L5. Sequence to Sequence\rML 2022 Spring (ntu.edu.tw)\nhttps://www.bilibili.com/video/BV1VN4y1P7Zj\n[TOC]\n输出的长度取决于model\nSpeech Recognition：语音到文本\nMachine Translation：文本到文本\nSpeech Translation：语音到文本\nSeq2Seq = encoder + decoder\nencoder：输入-\u0026gt;向量 decoder：向量-\u0026gt;输出 接受encoder的输入，每次输出一个结果，输出的结果会影响下一个输出 加入\u0026lt;BOS\u0026gt;表示开始解码，\u0026lt;EOS\u0026gt;表示结束 Transformer - Encoder\rBlock\rEncoder内以block为单位\n每个block的设计：\nEncoder\r输入需要Position Embedding 重复N次block 这是原始的Transformer结构\n后续发现把LayerNorm放在Attention前和FC前，效果会更好\nTransformer - Decoder\rBOS：Begin of Sentence EOS：End of Sentence 输入输出\r结构\rTransformer的block中，相当于添加了一段新的，其他部分相似 Masked Multi-Head Attention：与encoder部分相比，有所改变 encoder可以拿到整个sequence，因此可以考虑全局 而decoder只有前文，因此只会考虑出现过的向量（因为根本不存在） EOS同时会作为预测内容之一，当输出一个结束符出现时，结束预测，生成的文本即为结果 Autoregressive vs. Non-Autoregressive\r上面的内容被称为自回归 非自回归：一次性喂入多个begin进行预测 方便控制长短 使用另一个分类器进行长度预测（对输出长度再做一次乘法、乘法，能直接调整） 或生成足够数量的token，查看第一个end 方便并行 效果会更差 Encoder - Decoder\rTransformer中多出来的一个block，起到了连接encoder的作用 堆叠Encoder和Decoder，我们就得到了Transformer 原始论文中，decoder只拿了encoder最后一层的输出 但事实上可以拿非常多 Training\r看作一个分类问题，使用交叉熵损失函数 但是我们需要强制喂入Decoder正确的输入，只有这样才能进行比较 即我们不会每次把Decoder的输出作为新的输入，而是直接设定好 这个过程叫做Teacher Forcing 在训练的时候进行Teacher Forcing，最后的误差自然会偏低 而在做测试时是无法做到的，造成mismatching EOS也需要被当作预测字符进行预测 Copy Mechanism\rCopy Mechanism指在文本生成领域，生成的输出是输入序列元素的复制或者指向。\n有时候不需要自己创造输出 直接从输入中进行复制 例如：\n对模型进行询问时含有一些“人名”（模型从未见过），此时需要直接复述这个人名作为指代 文章摘要 Guided Attention\r有时候需要对输入输出进行引导 例如语音合成等相关内容，Attention会产生一些奇怪的错误，而你又对整个流程有明确的认识，就需要对Attention加入限制进行引导 Monotonic Attention Location-aware Attention （没有详讲） Beam Search\r每次都贪心选择概率最高的输出不一定是一件好事 我们无法穷举每一种可能，可以考虑使用Beam Search 基础版本： 对于答案明确的任务，Beam Search会有帮助（需要尽力找到较优的结果） 对于发挥创造力（编故事），反而不是一件好事 BLEU Score\r有时候会需要一些奇奇怪怪的NLP的评价指标去评估训练\n但是这些指标是很难进行微分的，所以很难用于训练\n对于很难微分的函数计算，我们可以考虑使用强化学习\nExposure Bias\r回到前文，我们在训练时每次喂给decoder的都是绝对正确的\n但是在测试时只能看自己的输出，这个不一致的现象叫做Exposure Bias\n解决方法：Scheduled Sampling：我们在喂入decoder的时候，故意弄错一点东西（引入噪声） ","date":"2024-08-11T18:19:55+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021-l5/","title":"李宏毅机器学习2021 · L5"},{"content":"L4. Sequence as input\rML 2022 Spring (ntu.edu.tw)\nhttps://www.bilibili.com/video/BV1VN4y1P7Zj\n[TOC]\nSequence\r对于Sequence Labeling任务，我们需要对一段文本中的所有单词标注词性。\n显然不能单独考虑一个单词，需要结合上下文 同一个单词在不同的上下文中会有不同的词性 我们考虑定义一个window： 每个Fully-Connected需要连接前后若干个单词作为输入 但是我们有时需要考虑整个Sequence去获得信息，我们很难通过扩大Window进行操作，复杂度会非常高 Self-Attention\r概述\r整个Sequence作为输入 输出向量数量等于输入向量 每次完成Self-Attention后，喂入FC进行操作 可以多次叠加 Dot-Product\r我们会对多个输入向量之间的相关性感兴趣\nSelf-Attention常使用Dot-Product求出两个向量之间的相关性\n两个输入$\\text{query},\\text{key}$向量分别乘上$W_q,W_k$，得到向量$q,k$ 对向量$q,k$进行点积，得到$\\alpha$，同时也被称为attention-score Forward\r对于其中一个向量$i$，计算出自己作为查询向量的$q_i$值\n所有向量都需要计算自己作为被查询向量的$k_j$值\n通过Dot-Product，得到$\\alpha_{i,j}$，即向量$i$对所有向量$j$的相关性\n做一下Softmax，得到$\\alpha\u0026rsquo;_{i,j}$\n每个向量都需要通过$W_v$矩阵计算出向量$v$\n$v_j$乘上标量$\\alpha_{i,j}$，求和\n$$\rb_i = \\sum_{j} \\alpha'_{i,j}v_j\r$$ 关联性越高的向量，它会$b$占有很大的成分 每个向量$a_i$得到$b_i$的计算过程是并行进行的\nForward（矩阵形式）\r$W_q,W_k,W_v$都是通用的，每个向量$a_i$都需要使用，因此很容易就能表示成矩阵形式 对于$q_1$，我需要让它与所有$k_j$​进行点积运算 实质上就是与$k_j^T$进行相乘 对于其他$q_i$也是同理，计算得到矩阵$A$ 通过矩阵$A$，每一列过一遍激活函数（softmax）得到$A'$ 计算输出矩阵 整理一下： $$\rO = VA' = (W_vI)\\text{softmax}(A) \\\\\rO =(W_vI)\\text{softmax}( K^TQ ) = (W_vI)\\text{softmax}( I^TW_k^TW_qI ) \\\\\r$$ $W_q,W_k,W_v$则是我们需要学习的参数矩阵 Multi-head Self-Attention\r一般的Self-Attention只会有一种$W_q$得到的一组$q$向量，作为相关性的度量\n但是有时候需要丰富多个相关性指标\n得到的多类$b_i$输出，拼起来乘一个矩阵，得到最后的输出 Position Encoding\r对于$a_1, a_2, a_3, a_4$，其对于上文算法来说，并没有距离的概念（交换位置后没什么差别）\n但对于实际文本来说，$a_1,a_4$是距离较远的向量，$a_2,a_3$​是距离较近的向量\n（需要分析一下位置对实际要的输出是有影响，才会考虑引入Position Encoding）\n在求解前，每个$a_i$需要加入一个位置向量$e_i$即可\nhand-crafted（人为设置）：会使用一些三角函数进行组合 方式非常多，暂时没有最好的 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 // 好像自己写过这个东西 __global__ void cu_matrixPositionalEmbedding(float* d_C, float* d_A, int M, int N, int level) { int idx = blockIdx.x * blockDim.x + threadIdx.x; if (idx \u0026lt; M * N) { int row = idx / N; int col = idx % N; int pos = row * N + col; d_C[pos] = d_A[idx]; pos += M * N; float powf2 = 1.0; for (int l = 0; l \u0026lt; level; ++l, powf2 *= 2) { float val = d_A[idx] * powf2; d_C[pos] = sinf(val); d_C[pos + M * N] = cosf(val); pos += 2 * M * N; } } } learn from data：直接作为参数进行学习 非文本应用\r省流：只要是一个vector set，就可以进行self-attention\nSelf-attention for Speech 声音讯号转换成向量会更加复杂 导致整体的矩阵会变得非常大 所以需要引入window，考虑一小段话进行识别 Self-attention for Image 每个像素的所有通道值看作一个向量 则我们可以得到分辨率数个向量，构成了一个vector set 相比CNN来说，可以考虑整个图像的所有像素，而不是一个感受野 自动学习出附近哪些像素是相关的，本质上自动学习了感受野 不需要人为设定感受野大小 认为：Self-Attention经过调整可以做到CNN一样的事情，因此对于function set层面上，CNN被Self-Attention所包含，是有所限制的特例 Self-attention for Graph 对于有边的结点对，需要计算attention-score 没有边可以直接认为无关，设为0 改改就是GNN RNN\r似乎被Self-Attention替代了\nRNN没法并行 普通的RNN只会考虑左边序列的输出，而Self-Attention考虑整个序列 双向RNN需要大量memory去存储结果，才能做到考虑整个序列 ","date":"2024-08-10T18:19:55+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021-l4/","title":"李宏毅机器学习2021 · L4"},{"content":"L3. Image as input\rML 2022 Spring (ntu.edu.tw)\nhttps://www.bilibili.com/video/BV1J94y1f7u5\n[TOC]\n原则\r局部性\r图片的识别往往需要注意一小块的特征 就算是人类也会根据局部特征，被误导进行分类（其实这是一只猫而不是一只鸟）\n图像会被切分成多个感受野，感受野之间需要有所重叠（否则会遗失边界上的信息）\n因此我们需要用感受野覆盖整个图像\n用一组神经元，识别同一个感受野，即负责这一块区域的图像识别\n平移不变性\r同一个特征可以出现在不同的位置，因此理论上识别同一个特征的单元应该有相同的参数 对于同一个感受野，它应该携带多个神经元，即多个识别单元filter，以识别不同的特征 不同感受野的同一个filter，需要共享参数 卷积神经网络\r卷积层\r我们从一般的全连接神经网络出发，引入了两个限制\n一组神经元只会接受图片上一块区域的张量（感受野） 断掉了这些神经元与其他位置的连接 不同组神经元的同一类识别单元需要共享参数 此时我们相当于对MLP大砍一刀，变成了一个限制非常大的神经网络，即卷积层\n但实际实现我们其实对于一个filter，就是一个卷积核，直接让它扫一遍整个图像即可 一个卷积核会产生一个新的通道\n如图，同样一个3*3的卷积核，恢复到原图像中，会对应上超过9个以上的像素\n随着深度增加，一个卷积核的真实感受野也会变大\n池化层\r大量的卷积核，产生大量的通道，因此对于原图像的尺寸来说参数会非常多\n我们考虑通过池化缩减原图像尺寸（通道数不变）\nMax、Mean Pooling\n在计算量足够的时候，不一定需要使用Pooling（会丢失很多信息）\n数据增强\r旋转、缩放图像，可能会使得CNN变得很差\n所以推荐在训练前使用数据增强\nSpatial Transformer Layer\r但我们为什么不直接在网络结构中加一个层呢\n详细解读Spatial Transformer Networks（STN）-一篇文章让你完全理解STN了-CSDN博客\n没学太明白，知道个大概\n","date":"2024-07-27T18:19:55+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021-l3/","title":"李宏毅机器学习2021 · L3"},{"content":"L2. What to do if my network fails to train\rML 2022 Spring (ntu.edu.tw)\nhttps://www.bilibili.com/video/BV1J94y1f7u5\n[TOC]\nGeneral Guide\r检查Training Loss Training Loss大 Model Bias 模型不够复杂 提高模型复杂度即可 Optimization 可以尝试使用浅层神经网络等容易优化的模型进行训练，若Train Loss比复杂模型更低，说明优化出问题了 解释：复杂模型包含了简单模型（多出来的部分全部不训练，剩下的就是简单模型），因此简单模型能做到的Training Loss，对于复杂模型也需要做到 Training Loss小 检查Testing Loss Testing Loss小（这不结束了吗） Testing Loss大 Overfitting 增加训练数据 数据增强（Data augmentation） 减少模型复杂度 Early Stopping Regularization Dropout Mismatch 与过拟合不同，没法使用增加训练数据避免 训练数据与测试数据有着不同的分布 训练数据少，不足以限制模型 模型太复杂，过于自由 Optimization Failed\rCritical Point\r处于局部极值点（极大值、极小值）或鞍点时，梯度值为0，此时没有办法继续更新梯度\nTayler Series Approximation\r为了判别critical point，我们要是能够知道损失函数$L(\\theta)$的形状就会非常容易\n我们考虑使用泰勒展开，对参数$\\theta\u0026rsquo;$周围的损失函数进行近似\n$$\rL(\\theta) \\approx L(\\theta') + (\\theta-\\theta')g+\\frac{1}{2}(\\theta-\\theta')^T\\times H\\times (\\theta-\\theta')\r$$$$\rL(\\theta) \\approx L(\\theta') + \\frac{1}{2}(\\theta-\\theta')^T\\times H\\times (\\theta-\\theta')\r$$ 此时取决于最后一项：\n永远大于0，则$L(\\theta) \u0026gt; L(\\theta\u0026rsquo;)$，是局部极小值 永远小于0，则$L(\\theta) \u0026lt; L(\\theta\u0026rsquo;)$，是局部极大值 否则：鞍点 判断$\\frac{1}{2}(\\theta-\\theta\u0026rsquo;)^T\\times H\\times (\\theta-\\theta\u0026rsquo;)$的正负是一个经典问题\n即$\\forall v, v^THv \u0026gt; 0$，此时$H$被称为正定矩阵（特征值都是正值）\n反之，负定矩阵所有的特征值均小于0\n但是每次都需要计算矩阵及其特征值，是一件开销很大的事情\n我们考虑从其他方式进行规避鞍点\nBatch Size\r大的Batch会花费更多时间进行计算，但是减少梯度更新次数 小的Batch会花费更少时间进行计算，但是增加梯度更新次数 得益于GPU并行计算，batch在不是非常大的时候，多条数据计算loss计算梯度是并行的，因此时间非常少\n但是小的batch确确实实需要更多次梯度更新，因此小的batch事实上在单个epoch上花费时间更大\n​\t但是似乎小的batch可以对Optimization带来更好的效果\n每次使用不同batch的数据进行更新，某种程度上引入了噪声\n在第一份batch上进入鞍点，但是对于第二份就不一定是鞍点\n甚至你会发现，小batch训练出来的模型，泛化性能更好，在测试集上表现更好\n一种比较玄学的解释：\n极值点有平坦的、陡峭的两种（如图左、右）\n由于测试集多少与训练集有一些mismatch，如果是平坦的极值点，稍微的偏移不会引起损失函数变化太多\n而陡峭的极值点会表现糟糕\n小batch在训练时引入的噪声，非常跳脱，峡谷很难困住其更新方向\n而大batch的梯度下降方向稳定，因此容易进入陡峭的峡谷\nLearning Rate\r非常多时候Loss并不是卡在critical point\n我们观察训练末期的梯度，发现并不是0\n原因是在山谷之间来回跳动，无法下降\n学习率较大，会在山谷来回跳（左图） 学习率较小，在稍微平坦的地方完全走不动（右图） 因此一般的梯度下降很难进入critical point\n我们考虑自适应学习率\n陡峭的地方学习率高 平坦的地方学习率低 梯度的更新应该由\n$$\r\\theta^{t+1} =\\theta^t - \\eta g^t\r$$ 转变为：\n$$\r\\theta^{t+1} =\\theta^t - \\frac{\\eta}{\\sigma^t}g^t\r$$Root Mean Square（Adagrad）\r及对应维度上的梯度均方根作为分母\n但是这样的方法不够灵活\n有时候同一个维度，在不同时间点可能又陡峭又平缓，因此需要更加灵活、动态变化的学习率\nRMSProp+Adam\r定义权重$\\alpha$，每次结合上一次的$\\sigma^{t-1}$与当前梯度进行计算\n$$\r\\sigma^{t} = \\sqrt{\\alpha(\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}\r$$但此时我们仍然无法解决卡在critical point的问题\nAdam = RMSProp + Momentum\nMomentum\r我们考虑在优化时引入动量的概念\n物体在下降时，到达谷底仍拥有一定的动量，还会沿之前的方向继续冲一会\n这样有机会冲到更低的极值点\n并且：每次移动的$m_i$都可以表示为之前所有梯度的综合（一个关于$g_0,g_1,\u0026hellip;$的式子）\nAdam\r结合 RMSProp + Momentum，我们就得到了Adam\nMomentum：\n$$\r\\theta^t = \\theta^{t-1} - \\eta m^t\\\\\rm^t = \\beta m^{t-1} + (1-\\beta)g^{t-1}\r$$ RMSProp:\n$$\r\\theta^t = \\theta^{t-1} - \\frac{\\eta}{\\sigma^t}g^{t-1}\\\\\r\\sigma^{t} = \\sqrt{\\alpha(\\sigma^{t-1})^2+(1-\\alpha)(g^t)^2}\r$$ 结合一下，Adam：\n$$\r\\hat m^{t} = \\frac{m^t}{1-\\beta}\\\\\r\\hat \\sigma^t = \\sqrt{\\frac{(\\sigma^t)^2}{1-\\alpha}}\\\\\r\\epsilon = 10^{-8}\\\\\r\\theta^t = \\theta^{t-1} - \\frac{\\eta}{\\hat \\sigma^t + \\epsilon}\\hat m^{t}\r$$ 同时兼顾方向与步长：\nMomentum 负责确定更新方向（加速收敛）：在陡峭的地方始终保持总体方向，防止偏航 RMSProp 负责调整步长（自适应学习率）：调整每一步的大小，走的更稳 Learning Rate Scheduling\r我们采用Adagrad，此时就可以进行正常的下降\n但是会出现奇怪的波动\n$$\r\\sigma^{t} = \\sqrt{\\frac{1}{t+1}\\sum_k (g^k)^2}\r$$ 刚进入平坦区时，得益于一开始积累了下降的较高梯度\n$\\sigma_y$仍然可以维持在一个比较大的范围\n但随着迭代次数增加，$\\sigma_y$每次只能加上非常小的梯度，平均值不断变小，最终引起了学习率爆炸\n如何解决这个问题呢\n$$\r\\theta^{t+1} =\\theta^t - \\frac{\\eta}{\\sigma^t}g^t\r$$ 我们除了对$\\sigma$进行变化，我们可以本身对$\\eta$​进行变化\n即随着时间，$\\eta$慢慢变小\n时间越长，本身肯定也已经离终点越来越近，因此就会抵消之前的梯度积累\n或者变大再变小\nLearning Rate Decay Warm Up Warm Up貌似更黑科技一点\nWarm Up可能的解释：\n一开始希望多收集周围的梯度信息，因此不希望走太快 信息足够后，开始正式的大踏步前进 ","date":"2024-07-26T20:45:55+08:00","permalink":"https://example.com/p/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021-l2/","title":"李宏毅机器学习2021 · L2"},{"content":"Pytorch入门\r[TOC]\n环境配置\r1 conda create -n pytorch-learn python=3.8 查看cuda版本：\n1 nvcc --version 根据cuda版本进行选择：Pytorch本地安装\n1 conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia 安装检测\n1 2 3 4 5 6 7 (pytorch-learn) aoijays@aoijays-ubuntu:~/Desktop/note$ python Python 3.8.19 (default, Mar 20 2024, 19:58:24) [GCC 11.2.0] :: Anaconda, Inc. on linux Type \u0026#34;help\u0026#34;, \u0026#34;copyright\u0026#34;, \u0026#34;credits\u0026#34; or \u0026#34;license\u0026#34; for more information. \u0026gt;\u0026gt;\u0026gt; import torch \u0026gt;\u0026gt;\u0026gt; torch.cuda.is_available() True 此时说明安装成功\n前言\r法宝函数\r1 2 3 dir( ... ) # 显示下属对象列表 help( ... ) # 显示当前对象的说明 ## 但我更喜欢加两个问号?? Dataset\r以蜜蜂蚂蚁数据集进行说明\n其文件目录：\n1 2 3 4 5 6 7 │ └── hymenoptera_data │ ├── train │ │ ├── ants │ │ └── bees │ └── val │ ├── ants │ └── bees 子文件夹小附有若干张jpg\n接下来我们需要使用torch的Dataset去加载数据集\n| An abstract class representing a :class:Dataset. |\n| All datasets that represent a map from keys to data samples should subclass | it. All subclasses should overwrite :meth:__getitem__, supporting fetching a | data sample for a given key. Subclasses could also optionally overwrite | :meth:__len__, which is expected to return the size of the dataset by many | :class:~torch.utils.data.Sampler implementations and the default options | of :class:~torch.utils.data.DataLoader. Subclasses could also | optionally implement :meth:__getitems__, for speedup batched samples | loading. This method accepts list of indices of samples of batch and returns | list of samples.\n省流：\n需要继承 必须重写__getitem__ 可以重写__len__ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 from torch.utils.data import Dataset from PIL import Image import os class MyDataet(Dataset): def __init__(self, root_dir, label): # 自定义 怎么方便怎么来 self.root_dir = root_dir # 记录数据地址以及对应的标签 self.label = label self.imglist = os.listdir(self.root_dir) # 以列表形式展示文件夹内所有文件 def __getitem__(self, idx): img_name= self.imglist[idx] img_path = os.path.join( self.root_dir, img_name ) img = Image.open(img_path) # 打开图片 label = self.label return img, label # 返回数据与标签 def __len__(self): return len(self.imglist) train_ants = MyDataet(\u0026#39;../Dataset/hymenoptera_data/train/ants\u0026#39;, \u0026#39;ants\u0026#39;) train_bees = MyDataet(\u0026#39;../Dataset/hymenoptera_data/train/bees\u0026#39;, \u0026#39;bees\u0026#39;) train_data = train_ants + train_bees # 拼接数据集 print( train_ants.__len__(), train_bees.__len__(), train_data.__len__()) print(train_data[123], train_data[124], sep=\u0026#39;\\n\u0026#39;) # 124 121 245 # (\u0026lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375 at 0x775FB149ADF0\u0026gt;, \u0026#39;ants\u0026#39;) # (\u0026lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=311x387 at 0x775FB0EB29A0\u0026gt;, \u0026#39;bees\u0026#39;) 写法非常自由，你只要保证重写的函数返回正确结果即可\nTensorboard\r1 conda install tensorboard Scalars\r绘制一些图表，观察训练的loss变化\n1 2 3 4 5 6 7 8 9 from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(\u0026#34;logs\u0026#34;) # 创建对象 在logs文件夹下保存文件 for i in range(100): writer.add_scalar(\u0026#34;y = x\u0026#34;, i, i) # 图像名 y值 x值 for i in range(100): writer.add_scalar(\u0026#34;y = x^2\u0026#34;, i*i, i) # 图像名 y值 x值 writer.close() 在终端中：\n1 tensorboard --logdir=logs --port=6007 # 默认6006 Images\r可视化实际的训练效果，上传图片进行展示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 img = Image.open( \u0026#39;../Dataset/hymenoptera_data/train/ants/0013035.jpg\u0026#39; ) print(type(img)) # \u0026lt;class \u0026#39;PIL.JpegImagePlugin.JpegImageFile\u0026#39;\u0026gt; # 不支持此类型 需要转化为numpy对象 img_array = np.array(img) print(img_array.shape) # (512, 768, 3) # 高 宽 通道数 -\u0026gt; HWC # 标题 添加的图像 步 格式 writer.add_image(\u0026#34;test\u0026#34;, img_array, 1, dataformats=\u0026#34;HWC\u0026#34;) # 我们可以第1步展示一张图，第2步展示一张图…… # 即可观察随着训练步数，图片发生变化 Transforms\r留个印象就好，能通过这个方法对数据、图片等进行互相转换、变化\n常见的有：\nToTensor：转化为Tensor数据 Normalize：归一化 Resize：对图片数据进行缩放 Compose：用列表记录多个变化，进行一次性操作 可能用到的时候查一下就行\n适合对多个数据同时进行相同的处理\nTorchvision数据集的下载与使用\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 import torchvision train_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=True, download=True) # 训练集 test_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=False, download=True) # 测试集 print(train_set[0]) print(train_set.classes) img, target = test_set[0] print(img, target) \u0026#39;\u0026#39;\u0026#39; (\u0026lt;PIL.Image.Image image mode=RGB size=32x32 at 0x7202D3007670\u0026gt;, 6) [\u0026#39;airplane\u0026#39;, \u0026#39;automobile\u0026#39;, \u0026#39;bird\u0026#39;, \u0026#39;cat\u0026#39;, \u0026#39;deer\u0026#39;, \u0026#39;dog\u0026#39;, \u0026#39;frog\u0026#39;, \u0026#39;horse\u0026#39;, \u0026#39;ship\u0026#39;, \u0026#39;truck\u0026#39;] \u0026lt;PIL.Image.Image image mode=RGB size=32x32 at 0x7202D3007310\u0026gt; 3 \u0026#39;\u0026#39;\u0026#39; 此时我们得到的数据集都是(PIL对象，标签索引)\n我们可以使用Transform统一把图片转化为Tensor，方便Pytorch使用\n最简单的方法：\n1 2 3 4 5 6 7 dataset_transform = torchvision.transforms.Compose([ # 封装所有需要进行的转换列表 torchvision.transforms.ToTensor() ]) # 在加载数据时直接进行参数化修改 train_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=True, transform=dataset_transform, download=True) # 训练集 test_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=False, transform=dataset_transform, download=True) # 测试集 DataLoader\r1 2 3 4 5 6 7 8 9 10 11 import torchvision from torch.utils.data import DataLoader test_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=False, transform=torchvision.transforms.ToTensor(), download=True) # 测试集 # 除了官方数据集 也可以选择之前自己实例化的Dataset test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=True, drop_last=False) for data in test_loader: # 按batch_size数量进行遍历 imgs, targets = data print(targets) 网络\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import torch from torch import nn # 需要从nn.Module进行继承 class Mynn(nn.Module): def __init__(self): super().__init__() # 调用父类的初始化函数 def forward(self, input): # 前向传播 output = input + 1 return output mynn = Mynn() x = torch.tensor(1.0) output = mynn(x) print(output) 卷积层\r对于一张H*W的RGB图片，其通道数channel为3\n我们使用多少个卷积核，就会产生多少个out_channels\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 class Mycnn(nn.Module): def __init__(self): super().__init__() # 输入3通道 用6个3*3的卷积核得到6通道输出 步长为1 不填充 self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0) def forward(self, input): # 前向传播 output = self.conv1(input) return output mycnn = Mycnn() x = torch.FloatTensor([ [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]], [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]], [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]] ]) mycnn(x) 池化层\r卷积后的图像依旧比较大，可以通过池化层进行压缩\n避免过拟合、去除冗余\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 池化 class Mycnn(nn.Module): def __init__(self): super().__init__() # ceil_mode 若有无法整除kernel_size 多余的部分按照ceil_mode决定是否保留 self.maxpool1 = nn.MaxPool2d( kernel_size=3, ceil_mode=True) def forward(self, input): # 前向传播 output = self.maxpool1(input) return output mycnn = Mycnn() x = torch.tensor([ [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]], [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]], [[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]] ], dtype = torch.float32) # x.shape mycnn(x) 我们可以喂入图片，就可以得到压缩画质版本的输出\n激活层\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # ReLU import torch from torch import nn class Mynn(nn.Module): def __init__(self): super().__init__() self.ReLU = nn.ReLU() def forward(self, input): # 前向传播 output = self.ReLU(input) return output mynn = Mynn() input = torch.tensor([ [1., -0.5], [-1., 3] ]) input = torch.reshape(input, (-1, 1, 2, 2)) print(input) output = mynn(input) print(output) 其他\r正则化层 线性层…… Sequential\r我们试图构建一个较大的网络对CIFAR10数据集进行推理\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 # Sequential import torch from torch import nn class Mynn(nn.Module): def __init__(self): super().__init__() self.sequential = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Flatten(), # 将tensor张成一维张量 nn.Linear(1024, 64), nn.Linear(64, 10) ) def forward(self, x): x = self.sequential(x) return x mynn = Mynn() print(mynn) # 模拟一个batch_size = 64中32*32的3通道数据集 input = torch.ones( (64, 3, 32, 32) ) output = mynn(input) print(output.shape) 损失函数\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import torch from torch.nn import * inputs = torch.tensor([1,2,3], dtype=torch.float32).reshape([-1,1,1,3]) targets = torch.tensor([1,2,5], dtype=torch.float32).reshape([-1,1,1,3]) loss1 = L1Loss(reduction=\u0026#39;sum\u0026#39;) loss2 = L1Loss(reduction=\u0026#39;mean\u0026#39;) print( loss1(inputs, targets) ) print( loss2(inputs, targets) ) loss3 = MSELoss() print( loss3(inputs, targets) ) import torch import torch.nn as nn # 交叉熵损失函数 # 假设 outputs 是模型的最后一层输出，shape 为 (batch_size, num_classes)，targets 是 ground truth labels outputs = torch.randn(10, 4) # 对于4分类问题的10个样本的不归一化的预测值 targets = torch.randint(0, 4, (10,)) # 对应的真实类别 print(outputs) print(targets) loss_fn = nn.CrossEntropyLoss() loss = loss_fn(outputs, targets) print(loss.item()) 反向传播\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 # 构建网络 import torch from torch import nn class Mynn(nn.Module): def __init__(self): super().__init__() self.sequential = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Flatten(), # 将tensor张成一维张量 nn.Linear(1024, 64), nn.Linear(64, 10) ) def forward(self, x): x = self.sequential(x) return x mynn = Mynn() print(mynn) # 模拟一个batch_size = 64中32*32的3通道数据集 input = torch.ones( (64, 3, 32, 32) ) output = mynn(input) print(output.shape) # ------------------------------------------- # 构建数据集 import torchvision from torch.utils.data import DataLoader test_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=False, transform=torchvision.transforms.ToTensor(), download=True) # 测试集 test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=True, drop_last=False) # ------------------------------------------- # 反向传播 loss_fn = nn.CrossEntropyLoss() for data in test_loader: # 按batch_size数量进行遍历 imgs, targets = data outputs = mynn(imgs) # 计算得到损失函数 loss = loss_fn(outputs, targets) loss.backward() # 反向传播 沿计算图得到所有参数的梯度 优化器\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 loss_fn = nn.CrossEntropyLoss() loss_sum = 0 for data in test_loader: # 按batch_size数量进行遍历 # 正向推理 imgs, targets = data outputs = mynn(imgs) # 计算得到损失函数 loss = loss_fn(outputs, targets) optim.zero_grad() # 将所有参数进行梯度清零 loss.backward() # 反向传播 沿计算图得到所有参数的梯度 optim.step() # 优化 loss_sum += loss print(loss_sum / len(test_loader)) 我们可以进行多轮学习\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 loss_fn = nn.CrossEntropyLoss() epochs = 20 for epoch in range(epochs): loss_sum = 0 for data in test_loader: # 按batch_size数量进行遍历 # 正向推理 imgs, targets = data outputs = mynn(imgs) # 计算得到损失函数 loss = loss_fn(outputs, targets) optim.zero_grad() # 将所有参数进行梯度清零 loss.backward() # 反向传播 沿计算图得到所有参数的梯度 optim.step() # 优化 loss_sum += loss print(\u0026#39;step\u0026#39;, epoch, \u0026#39; = \u0026#39;, loss_sum / len(test_loader)) 当loss收敛后，就完成了训练\n模型的修改\r除了自己的模型，其实也能修改别人训练完的模型\n以此网络为例：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import torch from torch import nn class Mynn(nn.Module): def __init__(self): super().__init__() self.sequential1 = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Flatten(), # 将tensor张成一维张量 nn.Linear(1024, 64), nn.Linear(64, 10) ) self.sequential2 = nn.Linear(10,10) self.classfication = nn.Sequential( nn.Linear(10,10), nn.Linear(10,10), nn.Linear(10,10) ) mynn = Mynn() print(mynn) 网络结构为：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Mynn( (sequential1): Sequential( (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Flatten(start_dim=1, end_dim=-1) (7): Linear(in_features=1024, out_features=64, bias=True) (8): Linear(in_features=64, out_features=10, bias=True) ) (sequential2): Linear(in_features=10, out_features=10, bias=True) (classfication): Sequential( (0): Linear(in_features=10, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=10, bias=True) ) ) 添加\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 mynn.add_module(\u0026#39;add_ReLU\u0026#39;, nn.ReLU() ) print(mynn) \u0026#39;\u0026#39;\u0026#39; Mynn( (sequential1): Sequential( (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Flatten(start_dim=1, end_dim=-1) (7): Linear(in_features=1024, out_features=64, bias=True) (8): Linear(in_features=64, out_features=10, bias=True) ) (sequential2): Linear(in_features=10, out_features=10, bias=True) (classficatipm): Sequential( (0): Linear(in_features=10, out_features=10, bias=True) (1): Linear(in_features=10, out_features=10, bias=True) (2): Linear(in_features=10, out_features=10, bias=True) ) (add_ReLU): ReLU() ) \u0026#39;\u0026#39;\u0026#39; 修改\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 mynn.classfication[1] = nn.ReLU() print(mynn) \u0026#39;\u0026#39;\u0026#39; Mynn( (sequential1): Sequential( (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (2): Conv2d(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (4): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2)) (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False) (6): Flatten(start_dim=1, end_dim=-1) (7): Linear(in_features=1024, out_features=64, bias=True) (8): Linear(in_features=64, out_features=10, bias=True) ) (sequential2): Linear(in_features=10, out_features=10, bias=True) (classfication): Sequential( (0): Linear(in_features=10, out_features=10, bias=True) (1): ReLU() (2): Linear(in_features=10, out_features=10, bias=True) ) (add_ReLU): ReLU() ) \u0026#39;\u0026#39;\u0026#39; 模型的保存与读取\rsave\r1 2 3 4 5 import torch torch.save(model, \u0026#39;new_model.pth\u0026#39;) model = torch.load(\u0026#39;my_model.pth\u0026#39;) 完整保存了模型的结构与参数，数据量大\nstate_dict（官方推荐）\r1 2 3 4 5 6 7 8 import torch model = MyModel() # 需要保证与读取的模型是同一个类 # 以字典形式进行 更加方便 torch.save(model.state_dict(), \u0026#39;model_state_dict1.pth\u0026#39;) state_dict = torch.load(\u0026#39;model_state_dict.pth\u0026#39;) model.load_state_dict(state_dict) GPU训练\r损失函数 数据 模型 三种对象直接x = x.cuda()即可放入GPU显存\n但是若GPU不存在，此时代码兼容性一般\n1 2 device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) x = x.to(device) 更推荐这种写法\n完整流程\r1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import torchvision from torch.utils.data import DataLoader # 导入数据 train_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=True, transform=torchvision.transforms.ToTensor(), download=True) # 训练集 test_set = torchvision.datasets.CIFAR10(root=\u0026#39;../Dataset\u0026#39;, train=False, transform=torchvision.transforms.ToTensor(), download=True) # 测试集 train_data_size = len(train_set) test_data_size = len(test_set) print(train_data_size, test_data_size) # 生成DataLoader train_loader = DataLoader(dataset=train_set, batch_size=64, shuffle=True, drop_last=False) test_loader = DataLoader(dataset=test_set, batch_size=64, shuffle=False, drop_last=False) 构建网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from torch import nn class Mynn(nn.Module): def __init__(self): super().__init__() self.sequential = nn.Sequential( nn.Conv2d(in_channels=3, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, padding=2), nn.MaxPool2d(2), nn.Flatten(), # 将tensor张成一维张量 nn.Linear(1024, 64), nn.Linear(64, 10) ) def forward(self, x): x = self.sequential(x) return x 初始化\n1 2 3 4 5 from torch import optim mynn = Mynn().to(device) loss_fn = nn.CrossEntropyLoss().to(device) optim = optim.SGD(mynn.parameters(), lr = 1e-2) 训练\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(\u0026#34;logs\u0026#34;) # 创建对象 在logs文件夹下保存文件 epochs = 40 for i in range(epochs): print(\u0026#39;-----------第{}轮训练-------------\u0026#39;.format(i + 1)) train_size = 0\ttrain_step = 0 for imgs, targets in train_loader: imgs = imgs.to(device) targets = targets.to(device) # 正向推理 outputs = mynn(imgs) loss = loss_fn(outputs, targets) # 优化 optim.zero_grad() loss.backward() optim.step() # 观察损失函数\ttrain_size += len(imgs) train_step += 1 writer.add_scalar(\u0026#34;Train Loss\u0026#34;, loss.item(), train_step) if train_step % 250 == 0: print(\u0026#39;{}/{} Loss = {}\u0026#39;.format(train_size, train_data_size, loss.item())) # 测试 total_loss = 0 # 上下文管理器： # with 语句是一个上下文管理器，确保在其内部的代码块在启用了 torch.no_grad() 模式下执行。 # 一旦代码块执行完毕，上下文管理器会恢复之前的状态，如果之前启用了梯度计算，则重新启用。 # torch.no_grad() 纯推理 不需要记录梯度 节省时间 with torch.no_grad(): for imgs, targets in test_loader: imgs = imgs.to(device) targets = targets.to(device) outputs = mynn(imgs) loss = loss_fn(outputs, targets) total_loss += loss.item() writer.add_scalar(\u0026#34;Test Loss\u0026#34;, total_loss/test_data_size, i) print(\u0026#39;测试集Loss = {}\u0026#39;.format(total_loss/test_data_size)) if (i + 1) % 10 == 0: torch.save(mynn, \u0026#39;./model/train_model{}.pth\u0026#39;.format(i)) writer.close()\t​\n","date":"2024-05-29T20:46:34+08:00","permalink":"https://example.com/p/pytorch%E5%85%A5%E9%97%A8/","title":"Pytorch入门"},{"content":"cuda编程 · 零\r蒙特卡洛的树 - Cuda编程Bilibili\nGithub: Cuda_Learning\n[TOC]\n基本步骤\r在进行运行之前，我们可以先查询一下设备中有多少块GPU\n1 2 3 int gpuCount = -1; cudaGetDeviceCount(\u0026amp;gpuCount); printf(\u0026#34;%d \u0026#34;, gpuCount); 然后可以设置成最后一块显卡的ID\ncudaGetDevice可以得到当前正在使用的gpu\n1 2 3 4 5 6 7 8 9 10 11 int gpuCount = -1; cudaGetDeviceCount(\u0026amp;gpuCount); printf(\u0026#34;gpuCount = %d\\n\u0026#34;, gpuCount); // 1. 指定GPU设别 // 单GPU设备其实可以省略此步骤 cudaSetDevice(gpuCount - 1); int devideId = -1; cudaGetDevice(\u0026amp;devideId); printf(\u0026#34;gpu = %d\\n\u0026#34;, devideId); 当设置不存在的设备编号时，默认启动0号gpu\n基本步骤如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;cuda_runtime_api.h\u0026gt; __global__ void kernel(float *a) { a[threadIdx.x] = 1; } int main(int argc, char ** argv) { int gpuCount = -1; cudaGetDeviceCount(\u0026amp;gpuCount); printf(\u0026#34;gpuCount = %d\\n\u0026#34;, gpuCount); // 1. 指定GPU设别 // 单GPU设备其实可以省略此步骤 cudaSetDevice(gpuCount - 1); int devideId = -1; cudaGetDevice(\u0026amp;devideId); printf(\u0026#34;gpu = %d\\n\u0026#34;, devideId); // 2. 分配显存空间 float *aGPU; // cudaError_t cudaMalloc(void **devPtr, size_t size); // void **devPtr 指向待分配内存空间指针的指针 // 指针是通用的设备指针，可以指向任何类型的内存 // size_t size 分配的内存大小 cudaMalloc((void**)\u0026amp;aGPU, 16 * sizeof(float)); // 3. 分配内存空间 float a[16] = {0}; // 4. 内存-\u0026gt;显存 // cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, cudaMemcpyKind kind) // 目的地址, 源地址，需要复制的字节数量， 复制类型 // cudaMemcpyHostToHost：从主机到主机的内存复制。 // cudaMemcpyHostToDevice：从主机到设备的内存复制。 // cudaMemcpyDeviceToHost：从设备到主机的内存复制。 // cudaMemcpyDeviceToDevice：从设备到设备的内存复制。 cudaMemcpy(aGPU, a, 16 * sizeof(float), cudaMemcpyHostToDevice); // 5. 设备代码 kernel\u0026lt;\u0026lt;\u0026lt;1, 16\u0026gt;\u0026gt;\u0026gt;(aGPU); // 6. 显存-\u0026gt;内存 cudaMemcpy(a, aGPU, 16 * sizeof(float), cudaMemcpyDeviceToHost); for (int i=0;i\u0026lt;16;++i) printf(\u0026#34;%.2lf \u0026#34;, a[i]); // 7. 释放 cudaFree(aGPU); // 释放申请的显存 cudaDeviceReset(); // 重置设备 // 如果主机内存也有申请 也需要释放 } GPU详细信息\rcudaDeviceProp是cuda封装的一个显卡信息结构体\n我们可以通过这个结构体查看显卡信息\n1 2 3 4 5 6 7 8 9 10 cudaDeviceProp prop; // 指定0号显卡 cudaGetDeviceProperties(\u0026amp;prop, 0); printf(\u0026#34;maxThreadsPerBLOCK: %d\\n\u0026#34;, prop.maxThreadsPerBlock); printf(\u0026#34;maxThreadsDim: %d\\n\u0026#34;, prop.maxThreadsDim[0]); printf(\u0026#34;maxGridSize: %d\\n\u0026#34;, prop.maxGridSize[0]); printf(\u0026#34;totalConstMem: %d\\n\u0026#34;, prop.totalConstMem); printf(\u0026#34;clockRate: %d\\n\u0026#34;, prop.clockRate); printf(\u0026#34;integrated: %d\\n\u0026#34;, prop.integrated); 还有一些别的东西\n1 2 3 4 5 6 7 // 程序可以在多个 CUDA 设备上运行时，可以使用这个函数来选择一个最合适的设备 device会变成被选中的设备编号 // prop需要填写需求 自动匹配符合要求的设备 cudaError_t cudaChooseDevice(int* device, const cudaDeviceProp* prop) // 传入一个编号数组和数组长度 // 只有编号在其中的设备会是有效设备 cudaError_t cudaSetValidDevices(int *device_arr, int len); Cuda项目建立\r建立项目文件夹，新建CMakeLists.txt\n1 2 3 4 5 cmake_minimum_required(VERSION 3.22) project(app LANGUAGES CUDA CXX) find_package(CUDA REQUIRED) CUDA_ADD_EXECUTABLE(app main.cu) TARGET_LINK_LIBRARIES(app) 在同文件夹下建立一个main.cu\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #include \u0026lt;stdio.h\u0026gt; #include \u0026lt;stdlib.h\u0026gt; #include \u0026lt;cuda_runtime_api.h\u0026gt; __global__ void add(int *a, int *b, int *c, int num) { if ( threadIdx.x \u0026lt; num ) c[threadIdx.x] = a[threadIdx.x] + b[threadIdx.x]; } int main(int argc, char ** argv) { int num = 10; int a[num], b[num], c[num]; for (int i=0;i\u0026lt;num;++i) a[i] = i; for (int i=0;i\u0026lt;num;++i) b[i] = i * i; int *agpu, *bgpu, *cgpu; cudaMalloc((void**)\u0026amp;agpu, num * sizeof(int)); cudaMalloc((void**)\u0026amp;bgpu, num * sizeof(int)); cudaMalloc((void**)\u0026amp;cgpu, num * sizeof(int)); cudaMemcpy(agpu, a, num * sizeof(int), cudaMemcpyHostToDevice); cudaMemcpy(bgpu, b, num * sizeof(int), cudaMemcpyHostToDevice); // 加法 add\u0026lt;\u0026lt;\u0026lt;1, 10\u0026gt;\u0026gt;\u0026gt;(agpu, bgpu, cgpu, num); cudaMemcpy(c, cgpu, num * sizeof(int), cudaMemcpyDeviceToHost); printf(\u0026#34;add:\\n\u0026#34;); for (int i=0;i\u0026lt;num;++i) printf(\u0026#34;%d + %d = %d\\n\u0026#34;, a[i], b[i], c[i]); cudaFree(agpu); cudaFree(bgpu); cudaFree(cgpu); cudaDeviceReset(); } 新建build文件夹\n1 2 3 4 mkdir buid \u0026amp;\u0026amp; cd build cmake .. make -j3 ./app 手写卷积\r什么是卷积？【官方双语】那么……什么是卷积？\n首先需要添加一个新的东西：CUDA_CHECK\n1 2 3 4 5 6 7 8 9 10 11 12 13 #define CUDA_CHECK(call) \\ do { \\ cudaError_t err = call; \\ if (err != cudaSuccess) { \\ fprintf(stderr, \u0026#34;CUDA error at %s:%d code=%d(%s) \\\u0026#34;%s\\\u0026#34;\\n\u0026#34;, \\ __FILE__, __LINE__, err, cudaGetErrorString(err), #call); \\ exit(EXIT_FAILURE); \\ } \\ } while (0) // 后续我们使用Cuda函数时 用宏进行包装 // 即可及时报错 CUDA_CHECK(cudaMalloc(\u0026amp;devPtr, size)); code见code/src/code_2.cu\n并行归约Parallel Reduction\r我们需要对一个数组进行并行算法的求和\n交错寻址\r两两求和，逐渐合并\n但是这样寻址速度较慢\n连续地址\rcode见code/src/code_3.cu\n程序计时\r推荐使用\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 #include \u0026lt;sys/time.h\u0026gt; struct timeval startTime, endTime; // 获取开始时间 gettimeofday(\u0026amp;startTime, NULL); // 执行一些操作 // 获取结束时间 gettimeofday(\u0026amp;endTime, NULL); // 计算时间差 long long elapsedTime = (endTime.tv_sec - startTime.tv_sec) * 1000000LL + (endTime.tv_usec - startTime.tv_usec); printf(\u0026#34;Elapsed time: %lld microseconds\\n\u0026#34;, elapsedTime); 其中，sys/time.h 是一个 C 标准库头文件，通常用于在 POSIX 操作系统中进行时间操作，因此在 POSIX 兼容的操作系统上使用时通常是可用的，比如 Linux 和 macOS 等。\n这里我们对手写卷积进行了测速\ncode见code/src/code_4.cu\n并且我们发现，我们一个block一次计算，和一个thread一次计算\n效率基本一致\n并且放在thread可以共享内存，所以推荐放在thread里\n原子操作\r原子操作是一种不可分割的操作，它要么完全执行，要么完全不执行，没有中间状态。\n在并发编程中，原子操作是一种确保多个线程或进程安全访问共享资源的机制。\n原子操作能够保证在多线程环境下不会出现数据竞争的情况，从而确保数据的一致性和正确性。\n原子操作的特性包括：\n不可分割性：原子操作是一个不可分割的操作，它要么完全执行，要么完全不执行，不会被中断或分割成更小的部分。 独占性：在原子操作执行期间，其他线程或进程无法访问被操作的资源，直到原子操作执行完成。 并发安全性：多个线程或进程可以同时执行原子操作，而不会导致数据竞争或数据不一致的情况。 银行转账，当钱被转出时若发生中断，则此时钱就少了\n因此转出和转入必须完整执行完毕\n实例\r统计每种数字出现多少次\n如果在核函数中\n1 hist[ a[id] ] ++; 由于会有多个核函数并行操作，每次hist的值都不一致\n会造成值操作的覆盖\n因此引入了atomicAdd()，自动为数据上锁，在完成一次加法之间，不允许被其他thread使用\ncode见code/src/code_5.cu\n共享内存\r使用__shared__进行声明\n同属于一个block的thread共享一个共享内存\n静态申请\r如果我们一开始就确定要开多少共享内存数组\n1 2 __shared__ int s[64]; __shared__ int s[N]; // N is constexpr 动态申请\r在核函数指定第三个执行配置参数，数值为需要申请的每个块动态共享内存大小\n1 dynamicReverse\u0026lt;\u0026lt;\u0026lt;1, n, n*sizeof(int)\u0026gt;\u0026gt;\u0026gt;(); 在核函数内\n1 extern __shared__ int s[]; // 此时s即为大小为指定值的数组 如果需要申请多个共享内存数组\n1 Kernel\u0026lt;\u0026lt;\u0026lt;g, b, nI*sizeof(int)+nF*sizeof(float)+nC*sizeof(char)\u0026gt;\u0026gt;\u0026gt;(); 申请的数值即为所有数组大小之和\n1 2 3 4 extern __shared__ int s[]; int *integerData = s; // nI ints float *floatData = (float*)\u0026amp;integerData[nI]; // nF floats char *charData = (char*)\u0026amp;floatData[nF]; // nC chars 手动切割数组即可\n框架thrust\rcuda版本的STL\n官方地址：https://github.com/NVIDIA/thrust\n记录几个比较简单的例子\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;thrust/host_vector.h\u0026gt; #include \u0026lt;thrust/device_vector.h\u0026gt; // 在主机内存中申请 大小为10的vector thrust::host_vector\u0026lt;int \u0026gt; a(10); for (auto \u0026amp; e : a) std::cin \u0026gt;\u0026gt; e; for (auto \u0026amp; e : a) std::cout \u0026lt;\u0026lt; e \u0026lt;\u0026lt;\u0026#34; \u0026#34;; // 基本和std::vector没什么差别 实例：估算圆周率\rcode见code/src/code_6.cu\n多个block的归约\r例如我们要进行一个数组求和，但是数组元素个数远远多于线程\n一个block的线程数量有限，一般是1024（看具体设备）\n因此我们需要让多个block进行归约\n（估算圆周率的实例中，我们使用了1个block进行归约）\n把数据切分为若干段，每段数量为总线程数 第一次先把所有数据读到前总线程数个数字内 再细分成block num段，每段thread_num个 分别归约，得到blockNum个数字 数量不会很多的情况下直接CPU计算，节省硬件传输 code见code/src/code_7.cu\n多维矩阵\r1 2 3 4 5 6 7 8 9 10 11 size_t width = 120, height = 10; float * a_gpu; size_t pitch; cudaMallocPitch((void**)\u0026amp;a_gpu, \u0026amp;pitch, width*sizeof(float), height ); printf(\u0026#34;real = %zu\\npitch = %zu\u0026#34;, width*sizeof(float), pitch); cudaFree(a_gpu); // 当width*sizeof(float)\u0026lt;=512时 pitch=512 // 超过512 pitch取最小的512的倍数 内存对齐，使得访问效率更高\n但是我们一行该放多少放多少，剩下的空间直接忽略\n暂时先不研究这个，感觉用处不大\n实例：手写全加器\r对于串行加法器\n高位计算需要等待低位的进位\n令$A_i+B_i$的进位结果为$C_i$，计算结果为$S_i$\n则$C_i = 1 $的情况有：\n$A_i = 1, B_i = 1$同时为1 $A_i \\otimes B_i = 1, C_{i-1} = 1$只有一个1，存在进位1 因此$C_i = A_iB_i + (A_i\\otimes B_i)C_{i - 1}$\n$+$表示或，$\\cdot$表示并\n因此我们可以进行多次展开，每次的进位都可以由$C_0$直接确定\n一般来说我们每四位进行并行，然后总体串行即可\ncode见code/src/code_8.cu\n","date":"2024-04-02T20:46:34+08:00","permalink":"https://example.com/p/cuda%E7%BC%96%E7%A8%8B-%E9%9B%B6/","title":"cuda编程 · 零"},{"content":"CS231A：Computer Vision, From 3D Reconstruction to Recognition\rhttps://www.bilibili.com/video/BV1LQ4y1r7ps/\n[TOC]\nL2. Camera Models\rpinhole camera（小孔成像 - 摄像机）\r$f$：定义为相机焦距 $$\rx' = \\frac{f}{z}x,y' = \\frac{f}{z}y\r$$ 物理课的小知识\n小孔越小，透光越少，但是画面清晰 小孔越大，透光越多，但是画面模糊 因此为了全部都要，引入了Lens（透镜）\nLenses and Cameras（透镜和相机）\r除了通过中心的光线，其他光线都会被折射 在一定距离，所有入射光线会被折射到图像的一点上 少于或多于这段距离，光无法聚焦在一个点上（Out of focus） 景深（Depth of Field）则是指在摄影或者摄像中，一张图像中能够保持清晰度的距离范围。\n定义参数等效焦距$z\u0026rsquo;=f + z_0$，$z_0$​为像距\n$$\rx' = \\frac{z'}{z}x,y' = \\frac{z'}{z}y\r$$ 但由于工艺问题，透镜成像的边缘经常发生distortion（畸变）\n虚线为理想情况\n图1：聚焦偏外，越角落越边缘的图像越偏外 图2：聚焦偏内，越角落越边缘的图像越偏内 The Geometry of Pinhole Cameras（几何）\r只要距离足够，透镜和小孔成像都是同一个数学模型\n$$\rp=\\begin{bmatrix}x\\\\y\\\\z\\end{bmatrix} \\to p'=\\begin{bmatrix}x'\\\\y'\\end{bmatrix}\r$$我们完成了三维世界到二维平面的投影\nCoordinate systems（坐标系）\rOff Set：$(x,y,z)\\to(\\frac{f}{z}x+c_x,\\frac{f}{z}y+c_y)$​ $(c_x,c_y)$的存在：相机由于工艺问题，无法保证焦点中心一定在图像中心，因此通过引入参数来进行矫正调整 From Metric to Pixels：$(x,y,z)\\to(k\\frac{f}{z}x+c_x,l\\frac{f}{z}y+c_y)$​ 我们更偏向乘上系数，使得长度单位变成像素（不同的系数解决了像素是长方形的情况） 所以$(c_x,c_y)$也是像素单位 当$z$​发生变化时，投影坐标并不是线性变化（倒数），不利于使用线代处理\n同时乘上$z$，会丢失$z$的信息\n因此引入Homogeneous Coordinates（齐次坐标）\nHomogeneous Coordinates（齐次坐标）\r$$\r(x,y)\\to \\begin{bmatrix}x\\\\y\\\\1\\end{bmatrix}\r$$我们对点坐标，额外增加一个新的维度（二维变三维，三维变四维）\n$$\r\\begin{bmatrix}x\\\\y\\\\w\\end{bmatrix}\\to (\\frac{x}{w},\\frac{y}{w})\r$$$$\r(x,y,z)\\to(k\\frac{f}{z}x+c_x,l\\frac{f}{z}y+c_y)\\\\\r(x,y,z)\\to(\\alpha\\frac{x}{z}+c_x,\\beta\\frac{y}{z}+c_y)\r$$$$\r\\begin{bmatrix}\r\\alpha \u0026 0 \u0026 c_x \u0026 0\\\\\r0 \u0026 \\beta \u0026 c_y \u0026 0 \\\\\r0 \u0026 0 \u0026 1 \u0026 0\r\\end{bmatrix}\r\\begin{bmatrix}x\\\\y\\\\z\\\\1\\end{bmatrix}=\\begin{bmatrix}\r\\alpha x+c_xz\\\\\\beta y+c_y \\\\ z\r\\end{bmatrix}\r$$$$\rP_h'=MP_h\r$$ $z$的信息就能得到很好的保存\nCamera Matrix K（相机内参）\r$$\rM = \\begin{bmatrix}\r\\alpha \u0026 0 \u0026 c_x \u0026 0\\\\\r0 \u0026 \\beta \u0026 c_y \u0026 0 \\\\\r0 \u0026 0 \u0026 1 \u0026 0\r\\end{bmatrix} = K\\begin{bmatrix}I \u0026 0\\end{bmatrix}\r$$ 但由于工艺问题，有时像素平面可能不是一个矩形，而是一个平行四边形，产生了旋转\n$$\rK=\\begin{bmatrix}\r\\alpha \u0026 -\\alpha\\cot\\theta \u0026 c_x \\\\\r0 \u0026 \\frac{\\beta}{\\sin\\theta} \u0026 c_y \\\\\r0 \u0026 0 \u0026 1 \\end{bmatrix}\r$$ 懒得推导了\n$\\alpha,\\beta,\\theta,c_x,c_y$共五个自由度 上三角矩阵 World Reference System（世界坐标系）\r我们希望世界坐标系转化为相机坐标系，这里我们依旧使用齐次坐标\n我们先处理二维平面的情况\n平移Translation\n$$\rP'\\to\\begin{bmatrix}x + t_x\\\\y + t_y\\\\1\\end{bmatrix} = \\begin{bmatrix}\r1 \u0026 0 \u0026 t_x\\\\\r0 \u0026 1 \u0026 t_y \\\\\r0 \u0026 0 \u0026 1\\\\\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\1\\end{bmatrix}\r$$ 缩放Scaling\n注意是围绕原点进行缩放\n当$s_x=s_y$时，称为相似变换\n$$\rP'\\to\\begin{bmatrix}s_xx\\\\s_yy\\\\1\\end{bmatrix} = \\begin{bmatrix}\rs_x \u0026 0 \u0026 0\\\\\r0 \u0026 s_y \u0026 0 \\\\\r0 \u0026 0 \u0026 1\\\\\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\1\\end{bmatrix}\r$$ 旋转Rotation\n$$\rP'\\to\\begin{bmatrix}x'\\\\y'\\\\1\\end{bmatrix} = \\begin{bmatrix}\r\\cos\\theta \u0026 -\\sin\\theta \u0026 0\\\\\r\\sin\\theta \u0026 \\cos\\theta \u0026 0 \\\\\r0 \u0026 0 \u0026 1\\\\\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\1\\end{bmatrix}\r$$ 同样是围绕原点进行旋转 我们可以组合上述的矩阵：同时进行平移缩放旋转\n即对$P$先后进行变换矩阵的左乘即可\n对于三维情况：\n平移Translation\n$$\rP'\\to\\begin{bmatrix}\rI \u0026 T\\\\\r0 \u0026 1\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\z \\\\1\\end{bmatrix}\r$$ 缩放Scaling\n$$\rP'\\to\r\\begin{bmatrix}\rS \u0026 0\\\\\r0 \u0026 1\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\z\\\\1\\end{bmatrix}\r$$ 旋转Rotation\n绕x轴旋转$\\alpha$，绕y轴旋转$\\beta$，绕z轴旋转$\\gamma$ $$\rR_x(\\alpha)=\r\\begin{bmatrix}\r1 \u00260\u00260\\\\\r0 \u0026 \\cos\\alpha \u0026 -\\sin\\alpha \\\\\r0 \u0026 \\sin\\alpha \u0026 \\cos\\alpha \\\\\r\\end{bmatrix} \\\\\rR_y(\\beta)=\\begin{bmatrix}\r\\cos\\beta \u0026 0 \u0026 -\\sin\\beta\\\\\r0 \u00261\u00260\\\\\r\\sin\\beta \u0026 0\u0026 \\cos\\beta \\\\\r\\end{bmatrix} \\\\\rR_z(\\gamma)=\\begin{bmatrix}\r\\cos\\gamma\u0026 -\\sin\\gamma \u0026 0 \\\\\r\\sin\\gamma \u0026 \\cos\\gamma \u0026 0\\\\\r0 \u00260\u00261\\\\\r\\end{bmatrix}\r$$ 任意绕轴旋转都可以进行分解成绕三轴先后旋转\n$R = R_x(\\alpha)R_y(\\beta)R_z(\\gamma)$，合成三个矩阵\n$$\rP'\\to\r\\begin{bmatrix}\rR \u0026 0\\\\\r0 \u0026 1\r\\end{bmatrix} \\begin{bmatrix}x \\\\y \\\\z\\\\1\\end{bmatrix}\r$$ 在这里，我们一般不考虑缩放（刚体是不会缩放的）\n$$\rP' \\to \\begin{bmatrix}R \u0026 T \\\\ 0 \u0026 1\\end{bmatrix}\r$$ 即可完成旋转后，再平移\n从世界坐标系，通过$R,T$转化到相机坐标系 从相机坐标系通过投影，转化为图像坐标系 $$\rP'=K\\begin{bmatrix}I \u0026 0\\end{bmatrix}P=K\\begin{bmatrix}I \u0026 0\\end{bmatrix}\\begin{bmatrix}R \u0026 T \\\\ 0 \u0026 1\\end{bmatrix}P_w\\\\\rP'=K\\begin{bmatrix}R \u0026 T\\end{bmatrix}P_w\r$$ 其中$\\begin{bmatrix}R \u0026amp; T\\end{bmatrix}$被称为外参数\nWeak Perspective Projection（弱透视投影）\r当物体离相机足够远时，深度$z$其实可以近似为一个常数$z_0$，从而简化计算\nL8: Fitting and Matching\rFitting\rChoose a parametric model to fit a certain quantity from data\nEstimate model parameters\nCritical issues\rnoisy data 数据中存在的随机误差或不确定性，测量、记录或传输过程中的各种因素引起 对整体趋势影响较小 处理噪声数据的方法包括平滑技术（如移动平均）、滤波方法、数据清洗等 outliers 数据集中与其他观测值明显不同的值，测量错误、录入错误、实际现象的稀有事件 具有明显的偏离，可能对分析结果产生较大的影响 missing data Techniques\r目标：拟合点集$(x_i,y_i)$\nLeast Square methods（最小二乘法）\r直线模型：$y-mx-b = 0$ 找到$(m,b)$使得最小化误差$E = \\sum(y_i-mx_i-b)^2$ $$\rE = \\sum(y_i-\\begin{bmatrix}\rx_i \u0026 1\r\\end{bmatrix}\r\\begin{bmatrix}\rm\\\\b\r\\end{bmatrix})^2 \\\\\r= \\left \\| \\begin{bmatrix}\ry_1 \\\\ ... \\\\ y_n\r\\end{bmatrix}- \\begin{bmatrix}\rx_1 \u0026 1 \\\\ ... \u0026 1\\\\ x_n \u0026 1\r\\end{bmatrix} \\begin{bmatrix}\rm \\\\ b\r\\end{bmatrix}\\right \\| ^2 \\\\\r= \\left \\| Y - Xh \\right \\| ^2 \\\\\r= (Y-Xh)^T(Y-Xh) \\\\\r= Y^TY-2(Xh)^TY+(Xh)^TXh\r$$对$h$求导\n$$\r\\frac{dE}{dh} = -2X^TY+2X^TXh=0\r$$$$\rh = (X^TX)^{-1}X^TY\r$$ 但是这样是代数意义上的最优解\n考虑几何意义上的最优解：\n$$\rdis=\\frac{|ax+by+d|}{\\sqrt{a^2+b^2}}\r$$ 我们可以$a,b$的存在表示了斜率，因此我们可以通过类似归一化的操作，使得$\\sqrt{a^2+b^2}$为1，或者为一个定值\n因此我们事实上只需要关心$|ax+by+d|$，代表了相对大小，不需要具体的真实值\n故定义：$E = \\sum |ax+by+d|^2$\n我们需要寻找最优的$(a,b,d)$\n令矩阵\n$$\rA = \\begin{bmatrix}\rx_1 \u0026 y_1 \u0026 1 \\\\ x_2 \u0026 y_2 \u0026 1 \\\\ ...\u0026...\u0026...\\\\x_n\u0026y_n\u00261\\end{bmatrix}\\\\\rh = \\begin{bmatrix}\ra\\\\b\\\\d\\end{bmatrix}\r$$则问题转化为：最小化 $||Ah||$，并且限制$||h|| = 1$\n使用SVD分解即可完成优化问题（留坑待填）\n最后：\n噪声：鲁棒的 外点：影响巨大 conclusion: Least Square is not robust w.r.t. outliers.\n\u0026ldquo;w.r.t.\u0026rdquo; 是英文表达中的缩写，意思是 \u0026ldquo;with respect to\u0026rdquo;，翻译成中文是 \u0026ldquo;关于\u0026rdquo;、\u0026ldquo;针对\u0026rdquo;、\u0026ldquo;就\u0026hellip;而言\u0026rdquo; 等等。\nLeast Squares: Robust Estimators （鲁棒估计器）\r令残差$\\mu = ax+by-d$\n$$\r\\rho(u;\\sigma) = \\frac{\\mu^2}{\\sigma^2+\\mu^2}\r$$ $\\mu$越大，函数值接近1 $\\mu$越小，函数是一个关于$\\mu^2$的函数 较大的残差，原本会极大影响损失函数的值\n通过此方法，我们限制了大残差的贡献，从而降低了对损失函数的影响，故能拟合的鲁棒性提升\nRANdom SAmple Consensus(RANSAC)\r假设1：嘈杂的数据不会为任何单一模型投一致的票 假设2：有足够的数据点来商定一个好的模型 我们定义好阈值$\\delta$，与给定直线的距离在阈值范围内的点，被称为内点；否则是外点\n算法流程\n随机选择出需要确定模型的最小数量的点（例如：确定直线需要两个点，因此随机两个点） 对于随机选出的点，计算出模型 计算出内点和外点 重复多次，外点数量最小的模型即为我们需要的\n因此我们比较好奇重复多少次可以基本保证能找到最优解\n设重复次数为$N$，算法成功概率为$p$（一般取0.99），$e$表示内点数量与点数之比，$s$表示采样点的数量\n$$\r1-p = (1-e^s)^N\r$$$$\rN = \\frac{\\log (1-p)}{\\log (1-e^s)}\r$$ 不管是需要采样的点变多，还是内点比例下降，都会使得次数增加\nconclusion: Cannot be used if ratio inliers is too small\n但对于大部分场景，外点是占较大部分的，因此很难使用\nHough transform(霍夫变换)\r设一条直线：$y=mx+n$，其中$(x_i,y_i)$是该直线上一点\n我们考虑将$m,n$看作自变量与因变量：$n = -x_im-y_i$\n因此我们得到了\n经过霍夫变换后得到的被称为是霍夫空间\n笛卡尔坐标系中的一个点，对应霍夫空间中的一条直线\n同理，笛卡尔坐标系中的一条直线，对应霍夫空间中的一个点\n因此在笛卡尔坐标系中，同一直线上的点，其在霍夫空间中将交于同一点\n理论上我们只需要知道哪些点被投票得最多，这条直线就是我们需要的\n但问题还很多：我们无法表示垂直的线\n考虑切换为极坐标系\n$$\rx\\cos\\theta +y\\sin\\theta = \\rho\r$$ 我们将霍夫空间看作一个网格\n其中高度为原图像对角线长度（$\\rho$的最大值），宽度为$\\theta$的最大值$2\\pi$\n枚举网格点，估计一下其中的交点数量\n对于高维数据非常难以处理\nL9. Detectors and descriptors\rDetectors\rEdge detectors\rEdge产生的要素\r深度不连续性 表面方向不连续性（物体表面不同部分的朝向或法线方向发生突然变化） 反射率不连续性（即，表面材料性质的变化、颜色） 光照不连续性（例如，高光; 阴影） 边缘检测的例子\n检测标准\rGood detection accuracy：不误检测噪声，漏检测真实边缘 Good localization：检测边缘应该尽可能接近真实边缘 Single response constraint：单一的回应 Detectors的设计\r使用导数，定义了梯度较高（也就是变化较为激烈）的地方 对图像进行了平滑处理，提取导数之前减少噪音 $$\r\\frac{df}{dx} = f_x - f_{x-1}\r$$ 因为是离散的，所以单位长度是一个像素，我们对一个像素作一个差值就是变化率，即导数\n考虑如上的一个图像，我们若直接求出导数图像，你会发现并没有特别显著的大导数\n原因是本身图像的波动大概就是5左右，而上升部分的差值也差不多是5\n所以你会发现导数基本都一样\n因此我们使用高斯模糊的卷积核进行平滑处理\n$$\rS = \\bigtriangledown(g\\ast I) = (\\bigtriangledown g)\\ast I \\\\ = \\begin{bmatrix}\r\\frac{\\partial g}{\\partial x} \\\\\r\\frac{\\partial g}{\\partial y}\r\\end{bmatrix}\\ast I = \\begin{bmatrix}\rg_x\\ast I \\\\\rg_y\\ast I\r\\end{bmatrix} \\\\\r= \\begin{bmatrix}\rS_x \u0026 S_y\r\\end{bmatrix}\r$$Corner/blob detectors\r可重复性：尽管存在几何和光度变换，但同一特征可以在多幅图像中被找到。 显著性：每个特征都位于图像的“有趣”区域。（反正基本不是空白区域） 局部性：一个特征占据图像的“相对较小”区域。 Harris corner detector\r在窗口位置变化时探索窗口内的强度变化\nflat：在所有方向上都没有变化 edge：沿着边缘方向没有变化 corner：在所有方向上都有显著变化 我们无法知道corner的尺度变化\nBlob detection\r回到边缘探测，我们可以把卷积后的结果的导数\n修改为二阶导\n因此我们的高斯算子可以换成拉普拉斯算子（高斯的导数）\n对原图像使用拉普拉斯算子进行处理即可\n因此对于一个比较宽的图形，两侧边缘会分别导出两个波动\n我们先尝试固定拉普拉斯算子，当图形宽度变化时\n某种情况下，两个波动会融合在一起，并且幅度最大值取在了图形中央\n所以我们调一下参数，就可以找到幅度最大的点，从而估计出尺度大小\n并且对于半径为$r$的圆，取到最大值的参数是可以计算的\nDoG\r高斯差分，你只需知道这个算子会更常用一点\nDescriptors\r描述信息一般需要：\n特征保证\n光照不变性（Invariant w.r.t Illumination）：特征应该不受光照变化的影响。 姿势不变性（Invariant w.r.t Pose）：特征应该不受物体姿势变化的影响。 尺度不变性（Invariant w.r.t Scale）：特征应该不受尺度变化的影响。 类内变异不变性（Invariant w.r.t Intraclass variability）：特征应该能够在相同类别的不同实例之间保持稳定性。 特征要有\n高度独特性（Highly distinctive）：特征应该具有足够的独特性，以便在大型特征数据库中能够以高概率找到其正确匹配的特征。 Simplest Descriptor - Patch\r图像中的一个小区域或局部区域，通常由一组相邻像素组成\n将特征周围的像素$n\\times m$的小图片展开为$[1,nm]$大小的一维向量$w$\n$$\rw = \\frac{w-\\bar{w}}{||w-\\bar{w}||}\r$$ 减去均值除以模长\n无论图像的光照条件如何变化，这种归一化保证了描述符的生成不会受到影响，从而增强了描述符的稳定性和可靠性\n缺点\n对于位置、姿势、尺度和类内变异的小变化敏感 特征区分度较差 Filter\r提供卷积核做点事情，然后提取特征\n只能说鲁棒性有所提升\nSIFT\r大致理解即可，并不准确\n使用DoG确定位置和特征尺度 对于一个$N\\times N$的窗口，我们对每个像素计算梯度\n我们把$[0,2\\pi]$分成若干份，对所有方向进行计数\n数量最多的即为主方向\n那么只需要按照主方向的角度进行旋转即可\n打包后得到的向量即为描述符\n显然\n强度：DoG的归一化、梯度足够处理 姿势：按照主方向把所有箭头旋转成一样的角度，无视了姿势变化 尺度：DoG处理完毕 类内变异：直方图有一定的粗略计算，有一定鲁棒性 L11. Visual recognition\r数学补坑\rSVD分解（未完成）\r卷积\r【官方双语】那么……什么是卷积？\n在这里我们主要理解一下离散的情况即可\n$$\rP(n) = \\sum_{i+j=n}a_ib_j\r$$ 高斯模糊：\n在二维图像上，使用一个$n\\times n$的卷积核，卷积核的值从中心开始符合二维高斯分布，对整个图像的颜色进行加权平均\n","date":"2024-03-26T02:45:55+08:00","permalink":"https://example.com/p/cs231acomputer-vision-from-3d-reconstruction-to-recognition/","title":"CS231A：Computer Vision, From 3D Reconstruction to Recognition"}]