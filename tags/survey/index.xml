<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Survey on BiribiriBird</title>
        <link>https://example.com/tags/survey/</link>
        <description>Recent content in Survey on BiribiriBird</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Fri, 17 Oct 2025 13:39:00 +0800</lastBuildDate><atom:link href="https://example.com/tags/survey/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Multimodal Diffusion Language Model · BirdResearch · 202510</title>
        <link>https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/</link>
        <pubDate>Fri, 17 Oct 2025 13:39:00 +0800</pubDate>
        
        <guid>https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;mmada-multimodal-large-diffusion-language-models&#34;&gt;MMaDA: Multimodal Large Diffusion Language Models
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先前的多模态架构混合，不同模态需要不同组件、不同数据处理方式&lt;/li&gt;
&lt;li&gt;扩散模型后训练策略欠缺研究&lt;/li&gt;
&lt;li&gt;如何文本与视觉模态协同学习、各方面性能超过各领域现有模型&lt;/li&gt;
&lt;li&gt;如何确保模型具有泛化能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915.png&#34;
	width=&#34;1127&#34;
	height=&#34;554&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915_hu_a5cdfff39a893f5f.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915_hu_a6e35a99ffe8ad82.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;compared to other llms&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;488px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心贡献&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一Diffusion架构：消除模态专用组件，保持跨任务性能&lt;/li&gt;
&lt;li&gt;混合Long-CoT的后训练：统一CoT格式，对齐跨模态推理过程，协同训练&lt;/li&gt;
&lt;li&gt;UniGRPO：专用的强化学习方法&lt;/li&gt;
&lt;li&gt;SOTA：文本推理、多模态理解、文生图三方面均是SOTA（AR、混合、扩散）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731.png&#34;
	width=&#34;987&#34;
	height=&#34;772&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731_hu_7714d48219f97e9f.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731_hu_c7254ad29497536a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;TASKs&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;h4 id=&#34;pretrain&#34;&gt;Pretrain
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Data Tokenization
&lt;ul&gt;
&lt;li&gt;文本：采用&lt;strong&gt;LLaDA&lt;/strong&gt;的tokenizer&lt;/li&gt;
&lt;li&gt;图像：采用&lt;strong&gt;Show-o&lt;/strong&gt;所使用的pretrained image quantizer
&lt;ul&gt;
&lt;li&gt;基于&lt;strong&gt;MAGVIT-v2&lt;/strong&gt;架构（一个图像离散化模型）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;MAGVIT-v2的输入与输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：单张静态图片的像素阵列、由多帧图像组成的序列&lt;/li&gt;
&lt;li&gt;输出：一个token序列&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;论文中采用$F=16$的下采样因子&lt;/p&gt;
&lt;p&gt;对于$H\times W$的图像，转化为一维的$\frac{H\times W}{F^2}$长度序列&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一的概率建模与目标
&lt;ul&gt;
&lt;li&gt;定义MMaDA为一个Mask Token Predictor，直接预测文本与图像的&lt;code&gt;[MASK]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;仅在&lt;code&gt;[MASK]&lt;/code&gt;的图像或文本Token上做统一交叉熵损失&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
L_{unity}(\theta) = -E_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L I(x_t^i = [MASK])\log p_\theta (x_0^i|x_t)\right]
$$&lt;h4 id=&#34;post-training-with-mixed-long-cot-finetuning&#34;&gt;Post-Training with Mixed Long-CoT Finetuning
&lt;/h4&gt;&lt;p&gt;MMaDA明确面向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推理密集型任务（例如数学）&lt;/li&gt;
&lt;li&gt;具备World-knowledge-aware的文生图
&lt;ul&gt;
&lt;li&gt;事实一致性非常重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920.png&#34;
	width=&#34;1344&#34;
	height=&#34;583&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920_hu_6214c2797a50c125.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920_hu_4b28f5ab45f1859e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Long-CoT Finetuning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;230&#34;
		data-flex-basis=&#34;553px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为进行稳定的后训练，论文整理了一个包含三类核心任务（文本推理、多模态推理、文本到图像生成）CoT数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;利用这篇数据，在RL之前通过SFT做冷启动&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一的CoT格式：消除不同任务的输出异构性&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;|&amp;lt;special_token&amp;gt;| &amp;lt;reasoning_process&amp;gt; |&amp;lt;special_token&amp;gt;| &amp;lt;result&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;后续证明了有益于跨模态的协同训练与对齐&lt;/p&gt;
&lt;p&gt;希望文本推理逻辑指导图像生成&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;多样性、复杂性、准确性
&lt;ul&gt;
&lt;li&gt;通过已有的LLM、VLM，合成多样化的数据&lt;/li&gt;
&lt;li&gt;使用模型过滤，只保留高质量、长形式的CoT样本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MMaDA进行了混合任务的CoT微调&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留提示词，对response进行加噪&lt;/li&gt;
&lt;li&gt;通过预训练得到的Predictor进行损失计算&lt;/li&gt;
&lt;/ul&gt;
$$
L_{Mixed-SFT}(\theta) = -E_{t,p_0,r_0,r_t}\left[\frac{1}{t}\sum_{i=1}^{L&#39;} I(r_t^i = [MASK])\log p_\theta (r_0^i|p_0,r_t)\right]
$$&lt;h4 id=&#34;post-training-with-unified-rl&#34;&gt;Post-Training with Unified RL
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810.png&#34;
	width=&#34;2085&#34;
	height=&#34;1055&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810_hu_909d17c24076e0b.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810_hu_a2527497ddba309b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Training&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自回归模型：每个Token的条件概率都非常好计算，适合RL&lt;/li&gt;
&lt;li&gt;Diffusion：过程复杂，无法直接使用传统强化学习方法
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;局部掩码依赖&lt;/strong&gt;：只有&lt;code&gt;[MASK]&lt;/code&gt;处有预测概率，其他位置已知&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掩码比例敏感&lt;/strong&gt;：训练必须兼容不同噪声程度的恢复
&lt;ul&gt;
&lt;li&gt;LLaDA采样大量样本，造成RL开销巨大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非自回归序列似然&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;AR模型：句子概率可以通过token概率乘积计算&lt;/li&gt;
&lt;li&gt;Diffusion：很难计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;unigrpo&#34;&gt;UniGRPO
&lt;/h5&gt;&lt;blockquote&gt;
&lt;p&gt;这部分搁置一下 后续补一下RL的知识&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;主要有三个关键点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构化加噪策略&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;序列对数似然近似为：被遮位置对数概率的平均&lt;/li&gt;
&lt;li&gt;用&lt;strong&gt;旧策略&lt;/strong&gt;和&lt;strong&gt;当前策略&lt;/strong&gt;的“近似序列似然”做&lt;strong&gt;比值&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UniGPRO的奖励是多样化的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本推理奖励
&lt;ul&gt;
&lt;li&gt;答案正确奖励&lt;/li&gt;
&lt;li&gt;格式奖励（&lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;think&amp;gt;&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多模态推理奖励
&lt;ul&gt;
&lt;li&gt;同上&lt;/li&gt;
&lt;li&gt;CLIP奖励：使用原始 CLIP 分数衡量文本-图像的语义一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文生图奖励
&lt;ul&gt;
&lt;li&gt;同上&lt;/li&gt;
&lt;li&gt;图像奖励：反映人类偏好得分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;文本生成：采用半自回归采样
&lt;ul&gt;
&lt;li&gt;Masking Schedule采用线性计划，与LLaDA一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;图像生成：采用低置信度重掩码
&lt;ul&gt;
&lt;li&gt;余弦噪声调度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments
&lt;/h3&gt;&lt;p&gt;一般的benchmark跳过&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion Language Model · BirdResearch · 202510</title>
        <link>https://example.com/p/diffusion-language-model-birdresearch-202510/</link>
        <pubDate>Wed, 01 Oct 2025 15:54:00 +0800</pubDate>
        
        <guid>https://example.com/p/diffusion-language-model-birdresearch-202510/</guid>
        <description>&lt;h2 id=&#34;ar-llm&#34;&gt;AR LLM
&lt;/h2&gt;&lt;p&gt;对于自回归模型，假设输入是：&lt;code&gt;[你, 好, ！]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;词汇表是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;0: &amp;lt;pad&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1: 你
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2: 好
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;3: 我
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;4: 很
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;5: 好
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;6: ！
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;7: &amp;lt;eos&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131.png&#34;
	width=&#34;966&#34;
	height=&#34;655&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131_hu_a8cad21da2d3ad7b.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131_hu_7c248c0bc453526d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20251008181120131&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;dream-7b-diffusion-large-language-models&#34;&gt;Dream 7B: Diffusion Large Language Models
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2508.15487&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2508.15487 Dream 7B: Diffusion Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209.png&#34;
	width=&#34;1492&#34;
	height=&#34;465&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209_hu_640220d5a4a7936d.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209_hu_ebb3f34c95380aba.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Performance&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;320&#34;
		data-flex-basis=&#34;770px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：
&lt;ul&gt;
&lt;li&gt;AR模型对于需要整体考虑的任务（长期规划、多约束）场景表现差&lt;/li&gt;
&lt;li&gt;AR模型对于长文本的一致性较差&lt;/li&gt;
&lt;li&gt;在各类通用任务中，要达到与Qwen2.5等顶尖自回归模型相当的性能仍存在显著差距&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;贡献：
&lt;ul&gt;
&lt;li&gt;基于&lt;strong&gt;自回归的LLM 初始化&lt;/strong&gt;和&lt;strong&gt;上下文自适应噪声调度技术&lt;/strong&gt;来实现扩散语言模型的规模化&lt;strong&gt;训练&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Dream 7B Base和Dream 7B Instruct&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;approach&#34;&gt;Approach
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415.png&#34;
	width=&#34;846&#34;
	height=&#34;343&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415_hu_62334379a41411ae.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415_hu_287610e7745136e9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Dream&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用Transformer以偏移方式，预测所有&lt;code&gt;[MASK]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常规的MDM是直接预测对应位置的&lt;code&gt;[MASK]&lt;/code&gt;，需要重新训练一个新的Transformer&lt;/p&gt;
&lt;h4 id=&#34;ar-based-llm-initialization&#34;&gt;AR-based LLM Initialization
&lt;/h4&gt;&lt;p&gt;自回归模型的训练目标就是使用第$i$个隐藏状态预测$i+1$的token&lt;/p&gt;
&lt;p&gt;因此我们以偏移方式进行预测，没有打破这种位置关系&lt;/p&gt;
&lt;p&gt;因此将已有的自回归模型参数作为初始值&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留AR模型的知识&lt;/li&gt;
&lt;li&gt;加速收敛&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;context-adaptive-token-level-noise-rescheduling&#34;&gt;Context-Adaptive Token-Level Noise Rescheduling
&lt;/h4&gt;&lt;p&gt;先前衡量噪声程度一般都是句子级别的：LLaDA衡量某个句子在$t$时刻的权重是$\frac{1}{t}$&lt;/p&gt;
&lt;p&gt;本文发现不同token之间的上下文信息是不同的，因此需要对噪声的衡量更加精细，避免学习的不平衡&lt;/p&gt;
&lt;p&gt;公式化地，定义损失函数：&lt;/p&gt;
$$
L(\theta) = -\mathbb{E}_{x_0,t,x_t}\sum_{i=1}^{L}1\left [x_t^i=M\right] \cdot w(t,x_t,i) \cdot \log p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;
对于LLaDA，其$w(t,x_t,i) = \frac{1}{t}$&lt;/p&gt;
&lt;p&gt;考虑对于某个token的上下文信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;距离越近的&lt;code&gt;unmask&lt;/code&gt;的token提供的信息越丰富&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此论文定义为：&lt;/p&gt;
$$
w(t,x_t,i) = \frac{1}{2}\sum_{j=1}^L\left [x_t^j\neq M\right] Geo(p, |i-j|-1)
$$&lt;p&gt;
其中$Geo$表示几何分布核：&lt;/p&gt;
$$
Geo(p,d) = (1-p)^d\cdot p, \quad d\geq 0
$$&lt;ul&gt;
&lt;li&gt;距离$d$越大，贡献越小&lt;/li&gt;
&lt;li&gt;超参数$p$：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071.png&#34;
	width=&#34;1139&#34;
	height=&#34;602&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071_hu_ad3745440641ea7f.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071_hu_9cf88da2296088af.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;超参数p&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;189&#34;
		data-flex-basis=&#34;454px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;train&#34;&gt;Train
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dream-7B采用了与Qwen2.5-7B完全相同的Transformer架构配置&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pretrain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SFT&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;采用了之前的技巧，训练上与LLaDA没什么不同（注意损失函数）&lt;/p&gt;
&lt;h3 id=&#34;experiment&#34;&gt;Experiment
&lt;/h3&gt;&lt;h4 id=&#34;base模型&#34;&gt;Base模型
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486.png&#34;
	width=&#34;1013&#34;
	height=&#34;712&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486_hu_530b344c72b58167.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486_hu_e41e555b15c86c6f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;推理任务中（ARC-E、ARC-C）表现良好&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;规划任务中领先幅度巨大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;训练数据量非常小&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化策略和上下文自适应噪声调度有效性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dream-instruct&#34;&gt;Dream-Instruct
&lt;/h4&gt;&lt;p&gt;180万条数据，进行3轮微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828.png&#34;
	width=&#34;1224&#34;
	height=&#34;556&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828_hu_215f137c20397b9a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828_hu_de627354d9bd135e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;528px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文中没做分析&lt;/li&gt;
&lt;li&gt;这里和LLaDA一样，SFT之后效果落后，甚至出现了性能下降&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;扩散大语言模型在遵循指令任务中具备与基于自回归的大语言模型相匹敌的潜力，为未来高级扩散大语言模型后训练方案奠定了基础&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;ar-initialization的贡献&#34;&gt;AR Initialization的贡献
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;验证：AR LLM初始化是有效的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验设计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaMA3.2-1B参数初始化的Dream-1B和从头训练的Dream1B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155.png&#34;
	width=&#34;1266&#34;
	height=&#34;825&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155_hu_e7ebe20ad61e1c37.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155_hu_81eed865a386611d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Loss 对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;368px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loss始终更低，证明了初始化是有效的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时在这个实验中，论文说明了学习率的影响非常大：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大的学习率：破坏AR LLM的有益特性&lt;/li&gt;
&lt;li&gt;小的学习率：阻碍学习扩散的过程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（但似乎没写上下文自适应噪声调度机制的消融实验）&lt;/p&gt;
&lt;h4 id=&#34;planning-ability&#34;&gt;Planning Ability
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368.png&#34;
	width=&#34;1535&#34;
	height=&#34;458&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368_hu_87ecdcf0260af463.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368_hu_66b211674994c51b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;规划能力对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;335&#34;
		data-flex-basis=&#34;804px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dream 模型在两项任务中始终优于其他同等规模的基线模型&lt;/li&gt;
&lt;li&gt;扩散语言模型在解决涉及多重约束或特定目标优化的问题时具有天然优势（？）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;trade-off&#34;&gt;Trade-off
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200.png&#34;
	width=&#34;987&#34;
	height=&#34;692&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200_hu_8669f15f499a63cc.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200_hu_34d18c98a9954339.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Trade-off&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;342px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Diffusion language models provide a unique advantage through their adjustable inference process&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;基于时间步长的方法为推理时缩放引入了新的维度，可与现有技术协同工作，例如 OpenAI o1和DeepSeek R1等大型语言模型中使用的思维链推理&lt;/li&gt;
&lt;li&gt;这种可调节的计算质量权衡代表了扩散模型区别于传统自回归模型的关键优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-15-variance-reduced-preference-optimization-for-large-language-diffusion-models&#34;&gt;LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.19223&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2505.19223 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427.png&#34;
	width=&#34;1719&#34;
	height=&#34;730&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427_hu_d7188d0aed1b164.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427_hu_6a39ec4afc25f7a1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA1.5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;565px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;看不懂&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;大概就是通过VRPO这个方法，基于LLaDA的工作，对LLaDA-instruct进行RL&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570.png&#34;
	width=&#34;877&#34;
	height=&#34;636&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570_hu_66310da504c89ddc.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570_hu_5e68522af6e9a73.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA RL&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;330px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;longllada-unlocking-long-context-capabilities-in-diffusion-llms&#34;&gt;LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2506.14429&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2506.14429  LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746.png&#34;
	width=&#34;1304&#34;
	height=&#34;432&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746_hu_cdf13d8671fc9b92.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746_hu_37b525e60acea430.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;长上下文对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;301&#34;
		data-flex-basis=&#34;724px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;核心问题：扩散型LLMs在长文本处理领域的研究空白&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么扩散LLM在直接长度外推时保持稳定的困惑度并呈现&lt;strong&gt;局部感知特性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;针对自回归 LLM 建立的长度扩展技术能否迁移至扩散架构&lt;/li&gt;
&lt;li&gt;自回归基线相比，扩散 LLM 在长上下文基准测试中表现如何？会显现哪些独特能力或局限性？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;揭示了其在上下文外推过程中保持稳定困惑度和局部感知的独特特性，并通过RoPE机制进行了解释&lt;/li&gt;
&lt;li&gt;基于 NTK 的 RoPE 外推法与缩放定律可无缝迁移至扩散 LLMs，实现 6 倍上下文扩展&lt;/li&gt;
&lt;li&gt;benchmark表明：扩散 LLMs 在&lt;strong&gt;检索任务&lt;/strong&gt;中与自回归模型表现相当，在&lt;strong&gt;聚合任务&lt;/strong&gt;中稍显不足，但在&lt;strong&gt;问答任务&lt;/strong&gt;中表现卓越&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;long-context-phenomenology-of-diffusion-llms&#34;&gt;Long-Context Phenomenology of Diffusion LLMs
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;大海捞针测试（Needle-In-A-Haystack, NIAH）&lt;/p&gt;
&lt;p&gt;在一个超长的上下文（haystack，干草堆）里，研究者会插入一小段关键信息（needle，针）&lt;/p&gt;
&lt;p&gt;模型的任务是：在生成或问答过程中，能否准确地“找到”并使用这段信息。&lt;/p&gt;
&lt;p&gt;这类测试会改变针的位置（例如放在靠前、中间或靠后部分）以及上下文的总长度，用来观察模型在不同深度和不同长度下的表现。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实验目的：揭示扩散 LLM 在长上下文中出现的&lt;strong&gt;局部感知 (local perception)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验设计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：在不同长度（最多32k）的长上下文中插入一个needle&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出：限定模型输出最多32个token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验对象&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DLM：block size = 32，采样步数 = 32&lt;/li&gt;
&lt;li&gt;LLM：默认&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;评估指标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找到Needle的成功率&lt;/li&gt;
&lt;li&gt;模型在不同深度（前文、中间、后文）找到Needle的能力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065.png&#34;
	width=&#34;1135&#34;
	height=&#34;686&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065_hu_335374cd5412e855.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065_hu_befee1c766cd6b1e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA与LLaMA系列实验结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;附录中补充了其他DLM模型的实验&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;AR LLM在8K内的上下文表现完美，超过8K长度无法完成任何任务&lt;/li&gt;
&lt;li&gt;DLM出现了类似**滑动窗口（窗口长度为4k）**的表现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DLM受采样步数影响较大，因此定量补充了实验：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059.png&#34;
	width=&#34;1169&#34;
	height=&#34;749&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059_hu_546e20570a7401cb.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059_hu_181f5578c1ab51ce.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Sample Step&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表明扩散 LLMs 的长上下文性能虽受采样步数影响，但仍受限于模型支持的最大上下文长度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;机制分析&#34;&gt;机制分析
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;自回归只能看见后续的：$[0, T_{train} - 1]$（LLaMA的$T_{train} = 8192$​）&lt;/li&gt;
&lt;li&gt;DLM是双向注意力：$[1-T_{train},T_{train}-1]$（LLaDA的$T_{train}=4096$）
&lt;ul&gt;
&lt;li&gt;对于单个token，可以同时出现在左边的上下文窗口，也可以出现在右边的上下文窗口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544.png&#34;
	width=&#34;1890&#34;
	height=&#34;919&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544_hu_57b583127a349bfd.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544_hu_9380a40281cc2024.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;context&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;205&#34;
		data-flex-basis=&#34;493px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;留坑：RoPE&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;LLaMA完全丢失了负相对位置的信息，外推能力受限&lt;/li&gt;
&lt;li&gt;LLaDA虽然$T_{train}$比较小，但是能够接受到一个负正窗口&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LLaMA：只学习了从头往后一个个token读取的能力&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;它可以知道，第2个token是第1个token的后一个……第1000个token是第999个token的后一个……&lt;/p&gt;
&lt;p&gt;（像翻书一样可以一页一页翻）&lt;/p&gt;
&lt;p&gt;但是一旦碰到第10000页，它推理不出这是9999页过来的（超出上下文，没有学习过这种关系）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLaDA：双向上下文&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以推断出9999是10000的前一页&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;论文补充了t-SNE可视化实验&lt;/p&gt;
&lt;p&gt;观察了两个模型最后的Q和K states&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445.png&#34;
	width=&#34;2054&#34;
	height=&#34;955&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445_hu_3136df464a2be127.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445_hu_d78712eecfc63560.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;t-SNE&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;516px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA随着上下文长度增加，仍然保持形状&lt;/li&gt;
&lt;li&gt;LLaMA出现了明显的聚类分离，表示内部出现了&lt;code&gt;distribution shift&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;context-extension&#34;&gt;Context Extension
&lt;/h3&gt;&lt;p&gt;将 &lt;strong&gt;NTK-based RoPE extrapolation&lt;/strong&gt;（一种在自回归 LLM 中已验证的旋转位置嵌入扩展方法）迁移到扩散式 LLM&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;缩放旋转基数 β0&lt;/strong&gt;，让正弦/余弦函数周期变长，相当于“拉伸坐标轴”，从而容纳更长的上下文&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144.png&#34;
	width=&#34;1226&#34;
	height=&#34;718&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144_hu_c1933c2297845371.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144_hu_662f939c281b78cd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;base&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056.png&#34;
	width=&#34;1223&#34;
	height=&#34;702&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056_hu_2f6517f50f9ebb35.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056_hu_70ac19fca8c8e977.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;instruct&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;小幅扩展有效&lt;/strong&gt;： 8k 或 16k，几乎在所有深度下都保持接近 100% 的检索准确率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;中等扩展出现性能下降&lt;/strong&gt;：24k ，出现lost-in-the-middle现象&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自回归模型中同样有的现象&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;大规模扩展失败&lt;/strong&gt;：模型无法再有效外推，说明方法的实际上限已到达。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;附录中对同类的DLM做了相同的实验&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;experiment-1&#34;&gt;Experiment
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;SD、MD、Sum 和 Syn 分别代表单文档问答、多文档问答、摘要和合成任务&lt;/p&gt;
&lt;p&gt;Avg 是所有子任务按评估数据数量加权的平均得分&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304.png&#34;
	width=&#34;2012&#34;
	height=&#34;745&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304_hu_309eb6e92ee1f29a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304_hu_99cab5b931fefe45.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LongBench&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;270&#34;
		data-flex-basis=&#34;648px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均得分媲美AR LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;检索（NIAH）/聚合（AGG）/问答（QA)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639.png&#34;
	width=&#34;1978&#34;
	height=&#34;1185&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639_hu_a472d06a9de10b3a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639_hu_b08d80dfb6e4d0da.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ruler&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;检索任务：相当&lt;/li&gt;
&lt;li&gt;聚合任务：不如AR LLM&lt;/li&gt;
&lt;li&gt;问答任务：超过AR LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-v-large-language-diffusion-models-with-visual-instruction-tuning&#34;&gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2505.16933&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087.png&#34;
	width=&#34;1961&#34;
	height=&#34;949&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087_hu_73052e8f451b3af2.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087_hu_31fdd1ed720a7b25.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;495px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;问题：完全基于扩散机制的多模态大语言模型能否达到与AR LLM相匹敌的性能？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;论文贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首个完全基于扩散模型的多模态大语言模型&lt;/li&gt;
&lt;li&gt;在多个基准测试中展现出卓越的可扩展性&lt;/li&gt;
&lt;li&gt;在混合型及纯扩散式多模态大语言模型中均SOTA&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-instruction-tuning&#34;&gt;Visual Instruction Tuning
&lt;/h3&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vison Tower（CLIP或SigLIP）：图像转视觉表征&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLP connector：嵌入LLM词空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Language Tower：LLM&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主流的多模态大模型架构之一，只需要相对较少的数据（less than 100w 图文数据对）&lt;/p&gt;
&lt;p&gt;本文主要研究如何在DLM中进行Visual Instruction Tuning&lt;/p&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Language Tower：LLaDA 8B（与LLaMA3-8B相当的语言模型）&lt;/li&gt;
&lt;li&gt;Vison Tower：SigLIP&lt;/li&gt;
&lt;li&gt;MLP Connector：a two-layer MLP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;training&#34;&gt;Training
&lt;/h4&gt;&lt;p&gt;训练阶段引入了含有多轮对话的数据&lt;/p&gt;
&lt;p&gt;为了简化描述，文章以2轮对话的数据进行说明，定义符号：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{v}$：Vison Tower和MLP Connector生成的视觉表征向量&lt;/li&gt;
&lt;li&gt;$[M]$​：掩码标记&lt;/li&gt;
&lt;li&gt;数据：$(\mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2)$&lt;/li&gt;
&lt;li&gt;$p_0^1 = [ p_0^{1,i}]$：首轮提示文本&lt;/li&gt;
&lt;li&gt;$p_0^2 = [ p_0^{2,i}]$ ：次轮提示文本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于一个二轮对话，训练目标定义为：&lt;/p&gt;
$$
L(\theta) = -\mathbb{E}_{\mathcal{v},t,p_0^1,r_0^1,r_t^1,p_0^2,r_0^2,r_t^2}\left[\frac{1}{t}\sum_{i=1}^{L_{p_1}}\sum_{j=1}^{L_{p_2}}1\left[r_t^{1,i}=M\wedge r_t^{2,j}=M\right] \cdot \log p_\theta(r_0^{1,i},r_0^{2,j}\mid \mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2 ) \right]
$$&lt;blockquote&gt;
&lt;p&gt;在多轮对话场景下，&lt;strong&gt;不同轮次的响应是强相关的&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户的问题可能在第 1 轮，答案在第 2 轮&lt;/li&gt;
&lt;li&gt;推理链条往往横跨多个回合，不能只看单独的 token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型必须在预测某个 token 时，同时考虑另一轮对话中的掩码 token&lt;/p&gt;
&lt;p&gt;这样就把 &lt;strong&gt;跨轮次的依赖关系&lt;/strong&gt; 学进去，而不是每轮单独学&lt;/p&gt;
&lt;p&gt;联合约束迫使模型去捕捉 &lt;strong&gt;对话轮次之间的因果逻辑&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;理论上这个式子在先前工作中已经被证明为整个任务的负对数似然上界&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在多轮对话中似乎可以采用causal mask，阻止早期对话轮次访问了后期的对话轮次&lt;/li&gt;
&lt;li&gt;后文消融实验证明双向注意力的效果更好（实现对整体对话语境的全面理解）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;该机制在近期&lt;strong&gt;视频扩散模型&lt;/strong&gt;中已证实可有效提升生成视频的时间连贯性&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;本身训练的流程和LLaDA的SFT流程比较相似，加噪只会在Response中，且同时对多轮对话中的Response进行加噪&lt;/p&gt;
&lt;p&gt;一次性让模型恢复所有对话中的MASK&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499.png&#34;
	width=&#34;1569&#34;
	height=&#34;545&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499_hu_14f4dbdd112ab8a5.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499_hu_55f6d0881c6145a4.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;AR &amp;#43; Train &amp;#43; Inference&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;287&#34;
		data-flex-basis=&#34;690px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;training-strategies&#34;&gt;Training Strategies
&lt;/h4&gt;&lt;p&gt;整个训练过程参考了LLaVA的训练策略&lt;/p&gt;
&lt;p&gt;建立语言和视觉对齐关系并培养视觉指令跟随能力&lt;/p&gt;
&lt;p&gt;训练目标函数与上文相同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶段一：语言-图像对齐
&lt;ul&gt;
&lt;li&gt;目的：图像与语言的分布不一致，如果直接做指令调优，模型学习跨模态语义很困难&lt;/li&gt;
&lt;li&gt;方法：&lt;strong&gt;将视觉表征与 LLaDA 的词向量进行对齐&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;冻结Vison Tower和Language Tower（这两个本身进行过预训练），只训练MLP Connector&lt;/li&gt;
&lt;li&gt;数据集：LLaVA-Pretrain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;阶段二：视觉指令调优Visual Instruction Tuning
&lt;ul&gt;
&lt;li&gt;目的：（单图像训练）建立基本的图像理解能力，（多图像训练）扩展到时序和跨图像推理&lt;/li&gt;
&lt;li&gt;方法：（两个阶段）解冻所有层
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;单图像训练Single image&lt;/strong&gt;：在 &lt;strong&gt;1,000 万单图像样本&lt;/strong&gt;上训练，增强对单张图像的理解与响应能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统一视觉训练阶段one vision&lt;/strong&gt;：在 &lt;strong&gt;约 200 万多模态样本&lt;/strong&gt;（包括单图、多图和视频）上训练，使模型具备处理复杂场景的能力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据集：&lt;strong&gt;MAmmoTH-VL&lt;/strong&gt; 数据集&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;阶段三：多模态推理增强 Multimodal Reasoning Enhancement
&lt;ul&gt;
&lt;li&gt;目的：增强模型处理复杂任务的多模态推理能力，加入reasoning data提升数学、跨图像和逻辑推理任务的表现&lt;/li&gt;
&lt;li&gt;方法
&lt;ul&gt;
&lt;li&gt;推理训练：使用来自 VisualWebInstruct聚焦推理的多模态数据对 LLaDA-V 进行训练（90 万个问答对，详尽的推理链和最终答案）&lt;/li&gt;
&lt;li&gt;平衡训练：参考qwen系列，融合VisualWebInstruct（其中50%添加&lt;code&gt;\think&lt;/code&gt;）和MAmmoTH-VL(one vison部分，全部添加&lt;code&gt;\no_think&lt;/code&gt;，鼓励直接回答)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference
&lt;/h4&gt;&lt;p&gt;推理时根据已有的对话记录，对当前的prompt进行单轮的response生成&lt;/p&gt;
&lt;p&gt;重掩码策略采用low-confidence strategy&lt;/p&gt;
&lt;h3 id=&#34;experiment-2&#34;&gt;Experiment
&lt;/h3&gt;&lt;p&gt;可扩展性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA-V 随着&lt;strong&gt;训练数据&lt;/strong&gt;增加性能持续提升&lt;/li&gt;
&lt;li&gt;在 &lt;strong&gt;多学科与数学推理任务&lt;/strong&gt; 上，LLaDA-V 扩展性明显优于 LLaMA3-V&lt;/li&gt;
&lt;li&gt;但在 &lt;strong&gt;图表/文档理解&lt;/strong&gt; 和 &lt;strong&gt;真实场景理解&lt;/strong&gt; 任务上，LLaMA3-V 表现更优&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275.png&#34;
	width=&#34;1402&#34;
	height=&#34;695&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275_hu_2a1887244b431296.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275_hu_3b1ccaa0b03d7012.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;scalability&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;484px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Benchmark&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于已有的混合或扩散模型，LLaDA-V是SOTA&lt;/li&gt;
&lt;li&gt;对比LLaMA3-V：6 个任务上超越&lt;/li&gt;
&lt;li&gt;对比Qwen2-VL：整体仍落后&lt;/li&gt;
&lt;li&gt;图表/文档理解和 RealWorldQA 上表现稍差&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556.png&#34;
	width=&#34;1710&#34;
	height=&#34;757&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556_hu_16fbc19e2c4d96ea.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556_hu_c8031429e440442.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;542px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;消融实验&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对比了Causal Mask和无Mask（多轮对话）&lt;/li&gt;
&lt;li&gt;12个benchmark中7个更优&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983.png&#34;
	width=&#34;953&#34;
	height=&#34;567&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983_hu_d09e07717de8958f.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983_hu_d4cffbcd05dc86ef.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;消融实验&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;403px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图像接入SigLIP的方式比较简单，会丢失分辨率和信息，造成图表问题表现差&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-medv-exploring-large-language-diffusion-models-for-biomedical-image-understanding&#34;&gt;LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2508.01617&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2508.01617 LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;没怎么看，大概是把LLaDA-V的工作调整到了垂类领域&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;一些比较有趣的实验分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DLM在一些垂类领域非常合适，可以&lt;strong&gt;显式地控制&lt;/strong&gt;一个大概的生成长度&lt;/li&gt;
&lt;li&gt;模型可能出现重复 token（如 “the the the …”）的问题，尤其在&lt;strong&gt;采样步数较少&lt;/strong&gt;或&lt;strong&gt;长度设定较大&lt;/strong&gt;时&lt;/li&gt;
&lt;li&gt;直接使用LLaDA-V的参数做微调的性能反而更差，需要从LLaDA-instruct出发，重新走3个步骤&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lavida-a-large-diffusion-language-model-for-multimodal-understanding&#34;&gt;LaViDa: A Large Diffusion Language Model for Multimodal Understanding
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.16839&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2505.16839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AR LLM对强双向上下文要求的任务（文本填充、从图像中提取信息填充到json格式）很弱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉语言场景中对输出模式的要求特别严格&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标准扩散模型训练的数据效率低下，未被遮盖的token不参与损失函数计算，容易遗失关键语义信息（关键词）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现有的推理方式缺少了KV cache的支持（双向上下文固有的缺陷）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;短文本环境中是可容忍的&lt;/li&gt;
&lt;li&gt;对于VLM任务无法接受，常常伴有数百个visual token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固定比例的&lt;code&gt;unmask&lt;/code&gt;在迭代次数较少时效果非常差&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;贡献：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个DLM视觉语言模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一种互补的mask方案，确保每个token都能参与到学习过程中，提高数据效率&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prefix-DLM decoding：缓存多模态的提示词与图像输入，从而加速推理过程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;受文生图技术的启发，采用了时间步偏移策略，自适应调整每次迭代的解码数量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method-1&#34;&gt;Method
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779.png&#34;
	width=&#34;857&#34;
	height=&#34;381&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779_hu_83db3a345a7595c6.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779_hu_e775b30abb7e0603.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20251018171520779&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;224&#34;
		data-flex-basis=&#34;539px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder和DLM通过MLP进行连接&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型输入：图像$I$和文本$P$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;vision-encoder&#34;&gt;Vision Encoder
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$I \to_{resize} 768\times 768$&lt;/li&gt;
&lt;li&gt;切分成四个不重叠的部分（$I_{1:4} = (384,384)$）；直接resize原图为$(384,384)$，得到$I_5$&lt;/li&gt;
&lt;li&gt;每一个子图被独立地通过Vision Encoder（SigLIP-400M ）进行编码
&lt;ul&gt;
&lt;li&gt;$I_i \to V_i, \text{size} = 27 \times 27$，五个子图总共产生了3645个embeddings&lt;/li&gt;
&lt;li&gt;$2\times 2$平均池化（缩短序列，提升训练效率）：$14\times 14$，总共980个embeddings&lt;/li&gt;
&lt;li&gt;经过MLP构成的Projection Network，Flatten到1D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;这里是输出5个一维向量，还是拼在一起的1个一维向量？&lt;/p&gt;
&lt;p&gt;暂时没去研究&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;dlm&#34;&gt;DLM
&lt;/h4&gt;&lt;p&gt;DLM的输入：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉嵌入向量&lt;/li&gt;
&lt;li&gt;文本提示词$P$&lt;/li&gt;
&lt;li&gt;带有掩码的$X_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;概率分布，用于获取$X_0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文采用LLaDA和Dream作为DLM&lt;/p&gt;
&lt;h4 id=&#34;complementary-mask&#34;&gt;Complementary Mask
&lt;/h4&gt;&lt;p&gt;文本中信息量非常密集，一般就是几个词&lt;/p&gt;
&lt;p&gt;对于之前的加噪方法，不一定能恰好遮蔽需要的词&lt;/p&gt;
&lt;p&gt;例如&lt;code&gt;The answer is dog.&lt;/code&gt;，加噪为：&lt;code&gt;The [MASK] [Mask] dog.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;事实上我们只需要引入它的互补形式：&lt;code&gt;[MASK] answer is [MASK]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246.png&#34;
	width=&#34;1264&#34;
	height=&#34;883&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246_hu_6bc6d2d986ad22ba.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246_hu_3cdadd9f2c6fbffc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;complementary&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;343px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;同时，这份数据会直接copy视觉以及提示词部分的嵌入&lt;/p&gt;
&lt;p&gt;因此对实际训练开销的影响较小&lt;/p&gt;
&lt;h4 id=&#34;prefix-dlm&#34;&gt;Prefix-DLM
&lt;/h4&gt;&lt;p&gt;定义序列长度为$L$，推理的迭代次数$K$，我们有NFE（fraction of the number of functional evaluations）：&lt;/p&gt;
$$
\text{NFE} = \frac{K}{L}
$$&lt;ul&gt;
&lt;li&gt;NFE=100%时，每次迭代生成一个Token&lt;/li&gt;
&lt;li&gt;NFE=50%时，每次迭代生成2个Token&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;但实际上由于毫无推理优化，DLM的速度比自回归慢&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365.png&#34;
	width=&#34;1423&#34;
	height=&#34;417&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365_hu_754ba3cf8053a3da.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365_hu_4f0aa1432c96e31f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Attention&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;341&#34;
		data-flex-basis=&#34;818px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Causal Mask：可以不断复用之前的token的kv矩阵（之前的token没有变化，且之前的token对未来的token不感兴趣）&lt;/li&gt;
&lt;li&gt;Full Mask：每个token都会看前后的内容，因此需要不断重新计算&lt;/li&gt;
&lt;li&gt;Prefix-DLM：I和P部分不会发生变化，遮蔽了对未来answer的token&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;schedule-shift&#34;&gt;Schedule Shift
&lt;/h4&gt;&lt;p&gt;喷了一下等步长的解码，提出了非线性的递推：&lt;/p&gt;
$$
t&#39;_i = \frac{\alpha t_i}{1+(\alpha -1)t_i}
$$&lt;p&gt;
设计一个时间重参数化的函数，要求满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_0 = 0,t_1 = 1$，仍然保持边界&lt;/li&gt;
&lt;li&gt;单调递增&lt;/li&gt;
&lt;li&gt;曲率可控，方便控制早晚阶段的速度&lt;/li&gt;
&lt;li&gt;简单，防止数值不稳定或梯度爆炸&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文希望早期降噪快，后期降噪慢&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;希望先快速搭建一个骨架，后面慢慢填充&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对这个式子求导：&lt;/p&gt;
$$
\frac{dt_i&#39;}{dt_i} =   \frac{\alpha}{(1+(\alpha-1)t)^2}
$$&lt;p&gt;
当$t=0$，导数为$\alpha$，$t=1$，导数为$\frac{1}{\alpha}$&lt;/p&gt;
&lt;p&gt;可以通过$\alpha$​和1的大小关系，控制是先快后慢，还是先慢后快&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428.png&#34;
	width=&#34;739&#34;
	height=&#34;681&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428_hu_afa6e273813b0639.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428_hu_ba9745771f5c5b65.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;schedule&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;108&#34;
		data-flex-basis=&#34;260px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments
&lt;/h3&gt;&lt;p&gt;略过benchmark部分&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371.png&#34;
	width=&#34;1530&#34;
	height=&#34;1237&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371_hu_43223d13feada0c0.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371_hu_2a6ba790d8bb9229.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;296px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;reasoning-distillation&#34;&gt;Reasoning Distillation
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087.png&#34;
	width=&#34;519&#34;
	height=&#34;219&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087_hu_73460ab98d2fb011.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087_hu_aa6c53100831621.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;reasoning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;568px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;paper通过蒸馏VL-Rethinker-7B模型（19.2K CoT examples）&lt;/p&gt;
&lt;p&gt;训练得到LaViDa-Reason&lt;/p&gt;
&lt;p&gt;在MathVista、MathVerse和MathVision上均得到提升&lt;/p&gt;
&lt;h4 id=&#34;text-infilling&#34;&gt;Text Infilling
&lt;/h4&gt;&lt;p&gt;对于实际的文本填充任务，不需要生成所有内容，只需要填写需要填写的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;任意时间步长开始生成&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;There is a [M][M][M][M] in the image.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里我们希望是&lt;code&gt;dog&lt;/code&gt;或者&lt;code&gt;traffic light&lt;/code&gt;，也是就是variable-length completions&lt;/p&gt;
&lt;p&gt;因此利用了第二阶段（激活全部参数）的20%数据，进行第三阶段训练，得到LaViDa-FIM&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本中插入&lt;strong&gt;随机长度&lt;/strong&gt;的&lt;code&gt;[S][S]……[S][FIM]&lt;/code&gt;序列&lt;/li&gt;
&lt;li&gt;推理时，在掩码段后面附加&lt;code&gt;[FIM]&lt;/code&gt;，得到&lt;code&gt;There is a [M][M][M][M][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;模型自然可以生成：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;There is a dog[S][S][S][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;There is a traffic light[S][S][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;约束性诗歌生成：模型根据图像生成一首诗，每行以特定音节开头&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;强调了结构性约束和上下文一致性，测试双向生成能力&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484.png&#34;
	width=&#34;886&#34;
	height=&#34;492&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484_hu_7654eb8df0de9790.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484_hu_540b0ba14a6cc84c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;诗歌补全&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081.png&#34;
	width=&#34;506&#34;
	height=&#34;199&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081_hu_6c166c051812bb1a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081_hu_f982c9463f0367e2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;poem completion&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;610px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sentence：满足行级别的约束的比例&lt;/li&gt;
&lt;li&gt;Sample：满足样本级约束的比例&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;speed-vs-quality-trade-off&#34;&gt;Speed vs. Quality Trade off
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620.png&#34;
	width=&#34;557&#34;
	height=&#34;572&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620_hu_994630fc0b694545.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620_hu_878a9a6de5bcea7.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Quality-Speed&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;97&#34;
		data-flex-basis=&#34;233px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;设定长度固定32，通过调整迭代次数K，进行实验&lt;/p&gt;
&lt;p&gt;（数据集COCO2017图像描述 500张，生成图像标题）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NFE=100%，稍慢于AR LLM，但是性能更强&lt;/li&gt;
&lt;li&gt;50-75%的性能和速度都很不错&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247.png&#34;
	width=&#34;560&#34;
	height=&#34;278&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247_hu_10c5f2cf2970a1ff.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247_hu_1bf8e32d51acfc60.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Prefix-DLM&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;483px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有效的加速推理，性能下降少&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258.png&#34;
	width=&#34;541&#34;
	height=&#34;233&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258_hu_fdf1729242ac0fa8.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258_hu_48f7899bb17d908d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Timestep-shifting&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;232&#34;
		data-flex-basis=&#34;557px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先快后慢是对的&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;消融实验部分&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证了互补掩码&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797.png&#34;
	width=&#34;575&#34;
	height=&#34;285&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797_hu_c22d64ad30e12606.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797_hu_a75bfb12c4f27585.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Comp. Mask&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;484px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证了图像分辨率的影响（输入分辨率）
&lt;ul&gt;
&lt;li&gt;OCR（上面四个）的提升更为明显&lt;/li&gt;
&lt;li&gt;一般视觉理解任务（最后一个）提升不多&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771.png&#34;
	width=&#34;409&#34;
	height=&#34;247&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771_hu_240c4baf6b5065ba.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771_hu_69ac30fdc1178978.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image revolution&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;STOP，接下来需要进入any2any的调研&lt;/p&gt;
&lt;p&gt;这部分内容另开一篇&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
