<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>强化学习 on BiribiriBird</title>
        <link>https://example.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link>
        <description>Recent content in 强化学习 on BiribiriBird</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Thu, 30 Oct 2025 23:38:34 +0800</lastBuildDate><atom:link href="https://example.com/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>强化学习的数学原理 · Chap3 · Bellman Optimality Equation</title>
        <link>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap3-bellman-optimality-equation/</link>
        <pubDate>Thu, 30 Oct 2025 23:38:34 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap3-bellman-optimality-equation/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1sd4y167NS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1sd4y167NS&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;笔记丢失了一次&lt;/p&gt;
&lt;p&gt;这是补档版本，稍微粗略一点……心态小炸&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;optimal-policy&#34;&gt;Optimal Policy
&lt;/h2&gt;&lt;p&gt;当一个策略，在任意状态都比另一个策略的state value更好（或者相等）&lt;/p&gt;
&lt;p&gt;我们就认为这个策略优于另一个策略&lt;/p&gt;
&lt;p&gt;当一个策略比所有策略都好，它就是&lt;strong&gt;最优策略&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;bellman-optimality-equation&#34;&gt;Bellman Optimality Equation
&lt;/h2&gt;$$
\begin{split}
v(s) &amp;= {\color{red}\max_\pi} \sum_a \pi(a|s) \left( \sum_r p(r|s,a)r + \gamma \sum_{s&#39;} p(s&#39;|s,a)v(s&#39;)\right), \quad \forall s\in \mathcal{S} \\
&amp;= {\color{red}\max_\pi} \sum_a \pi(a|s) q(s,a), \quad \forall s\in \mathcal{S}
\end{split}
$$&lt;p&gt;
相比于贝尔曼公式，BOE实质上就是多了一个优化问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;已知：$p(r|s,a),p(s&amp;rsquo;|s,a)$&lt;/li&gt;
&lt;li&gt;未知：$v(s),v(s&amp;rsquo;)$，需要我们针对不同的$\pi$​去计算&lt;/li&gt;
&lt;li&gt;求解：$\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自然有向量形式：&lt;/p&gt;
$$
v = \max_\pi (r_\pi + \gamma P_\pi v)
$$&lt;h3 id=&#34;maximization&#34;&gt;Maximization
&lt;/h3&gt;&lt;p&gt;我们先考虑固定$v$，看看能不能求解$\pi$​&lt;/p&gt;
$$
v = \max_\pi\sum_a \pi(a|s)q(s,a)
$$&lt;p&gt;
其中$q(s,a)$自然也是一个已知的量（所有的$v$都可以知道，自然可以算出$q$）&lt;/p&gt;
&lt;p&gt;同时有：&lt;/p&gt;
$$
\sum_a \pi(a|s) = 1
$$&lt;p&gt;
那么本质上$\sum_a \pi(a|s)q(s,a)$是对action value做加权平均&lt;/p&gt;
&lt;p&gt;为了最大化这一项，我们肯定需要给最大的$q$最多的权值，也就是1&lt;/p&gt;
&lt;p&gt;可以得到策略：选择action最大的行动$a^*$&lt;/p&gt;
$$
\pi(a|s) = \begin{cases}
 1 &amp; \text{ if } a= a^*\\
 0 &amp; \text{ if } a\neq a^*
\end{cases}
$$&lt;p&gt;
这样我们就能得到原式：&lt;/p&gt;
$$
v = \max_\pi\sum_a \pi(a|s)q(s,a) = \max_{a\in \mathcal{A}(s)}q(s,a) \\
\text{where }a^* = \arg \max_a q(s,a)
$$&lt;p&gt;
只要我们固定$v$，我们肯定就能得到最优策略，自然计算出右边的值&lt;/p&gt;
&lt;p&gt;因此右边的值可以直接表示成一个只关于$v$的函数：$v=f(v)$&lt;/p&gt;
&lt;p&gt;表达成向量形式：&lt;/p&gt;
$$
f(v):=\max_\pi (r_\pi+\gamma P_\pi v)
$$&lt;blockquote&gt;
&lt;p&gt;搜索一下不动点定理或叫Contraction mapping theorem&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;基于这个定理，可以证明$f(v)$是符合这个定理的条件的&lt;/p&gt;
&lt;p&gt;大概就是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;函数具有压缩性：有$\left|v_1-v_2\right | \leq \gamma\left|f(v_1)-f(v_2)\right |$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也就是满足这个条件，就会有&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;必然存在&lt;strong&gt;唯一的&lt;/strong&gt;一个不动点$f(v) =v$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，我们可以不断套用$v,f(v),f(f(v)),&amp;hellip;$&lt;/p&gt;
&lt;p&gt;最后会&lt;strong&gt;收敛得到不动点&lt;/strong&gt;，具体证明略过&lt;/p&gt;
&lt;p&gt;实际上的操作就是：&lt;/p&gt;
$$
v_{k+1}=f(v_k) =\max_\pi (r_\pi+\gamma P_\pi v_k)
$$&lt;p&gt;
那么如何证明最后收敛到的结果$v^*$就是最优解呢？&lt;/p&gt;
&lt;p&gt;固定$v=v^&lt;em&gt;$，自然可以得到当前的策略$\pi^&lt;/em&gt;$&lt;/p&gt;
$$
\pi^* = \arg \max_\pi (r_\pi+\gamma P_\pi v^*)
$$&lt;p&gt;
将$\pi^*$代入：&lt;/p&gt;
$$
v^* = \max_\pi (r_\pi+\gamma P_\pi v_*) = r_{\pi^*}+\gamma P_{\pi^*} v^*
$$&lt;p&gt;
你会发现变成state value的公式了&lt;/p&gt;
&lt;p&gt;也就是说$v^&lt;em&gt;$，就是策略$\pi^&lt;/em&gt;$​的state value&lt;/p&gt;
&lt;p&gt;我们考虑策略替换成任意其他的策略$\pi$时：&lt;/p&gt;
$$
v^* = r_{\pi^*}+\gamma P_{\pi^*} v^* \geq r_\pi + \gamma P_\pi v^*
$$&lt;p&gt;
令该策略对应state value的贝尔曼公式：&lt;/p&gt;
$$
v = r_{\pi}+\gamma P_{\pi} v
$$&lt;p&gt;
做一个减法则有：&lt;/p&gt;
$$
v^*-v \geq (r_\pi + \gamma P_\pi v^*)-(r_{\pi}+\gamma P_{\pi} v) = \gamma P_\pi(v^*-v)
$$&lt;p&gt;令$\Delta = v^*-v$，则有：&lt;/p&gt;
$$
\Delta \geq \gamma P_\pi\Delta
$$&lt;p&gt;
我们只需要把右边的$\Delta$不停代换为$\gamma P_\pi\Delta$：&lt;/p&gt;
$$
\Delta \geq \gamma P_\pi\Delta \geq \gamma^2 P_\pi^2\Delta \geq ...  \geq \gamma^n P_\pi^n\Delta \geq 0
$$&lt;p&gt;
故对于策略$\pi^*$，其state value始终是最大的&lt;/p&gt;
&lt;p&gt;因此是最优策略&lt;/p&gt;
&lt;h3 id=&#34;analysis&#34;&gt;Analysis
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Change $r\to ar+b$会发生什么？
&lt;ul&gt;
&lt;li&gt;什么都不会发生&lt;/li&gt;
&lt;li&gt;可以证明 $v&amp;rsquo; = av^*+\frac{b}{1-\gamma}I$，所有state value的相对大小没有发生变化&lt;/li&gt;
&lt;li&gt;自然action value不会有其他选择，policy不改变&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;如何鼓励走最短路径？
&lt;ul&gt;
&lt;li&gt;其实不用一直给agent做-1（损失能量），催促agent&lt;/li&gt;
&lt;li&gt;$\gamma$本身就在催促agent尽快到终点，否则终点的贡献变少了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>强化学习的数学原理 · Chap2 · Bellman Equation</title>
        <link>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/</link>
        <pubDate>Thu, 30 Oct 2025 20:25:34 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1sd4y167NS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1sd4y167NS&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;state-value&#34;&gt;State Value
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;return 用来衡量不同的trajectory的好坏&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;首先有如下定义&lt;/p&gt;
$$
S_t \overset{A_t}{\rightarrow}R_{t+1}, S_{t+1}
$$&lt;ul&gt;
&lt;li&gt;$t$表示时间步&lt;/li&gt;
&lt;li&gt;从状态$S_t$出发，采取$A_t$，转移到$S_{t+1}$，获得了$R_{t+1}$的奖励&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;$S,A,R$​这里表示的是一个&lt;strong&gt;随机变量&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;因此上述内容服从以下概率分布：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$S_t \to A_t$ ：$\pi(A_t| S_t)$&lt;/li&gt;
&lt;li&gt;$S_t, A_t \to R_{t+1}$：$p(R_{t+1}| S_t,A_t)$&lt;/li&gt;
&lt;li&gt;$S_t, A_t \to S_{t+1}$：$p(S_{t+1}| S_t,A_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;推广到多步骤之后：&lt;/p&gt;
$$
S_t \overset{A_t}{\rightarrow}R_{t+1}, S_{t+1} \overset{A_{t+1}}{\rightarrow}R_{t+2}, S_{t+2}\overset{A_{t+2}}{\rightarrow}R_{t+3} ...
$$&lt;p&gt;
则可以定义这个trajectory的reward是：&lt;/p&gt;
$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...
$$&lt;blockquote&gt;
&lt;p&gt;$G_t$也表示一个随机变量&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;我们定义所有从$t$出发的trajectory的期望$G_t$，即为&lt;code&gt;state-value function&lt;/code&gt;or&lt;code&gt;state value&lt;/code&gt;&lt;/p&gt;
$$
v_\pi(s) = \mathbb{E}(G_t| S_t = s)
$$&lt;blockquote&gt;
&lt;p&gt;return 针对单个、单次的尝试（确定性的）&lt;/p&gt;
&lt;p&gt;state value是一种平均情况&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;bellman-equation&#34;&gt;Bellman Equation
&lt;/h2&gt;&lt;h3 id=&#34;递推形式&#34;&gt;递推形式
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;描述不同state的state value之间的关系&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对于单个trajectory：&lt;/p&gt;
$$
\begin{split} 
G_t &amp;= R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ...\\
 &amp;= R_{t+1} + \gamma \left(   R_{t+2} + \gamma^2 R_{t+3} + ...\right)\\
&amp;=  R_{t+1} + \gamma G_{t+1}
\end{split}
$$&lt;p&gt;
非常动态规划（或者叫递推）的一个步骤&lt;/p&gt;
&lt;p&gt;因此可以推广到期望的情况（事实上可以直接写）&lt;/p&gt;
$$
\begin{split}
v_\pi(s) &amp;= \mathbb{E}(G_t| S_t = s) \\
&amp;= \mathbb{E}(R_{t+1} + \gamma G_{t+1}| S_t = s) \\
&amp;=  \mathbb{E}(R_{t+1}| S_t = s) + \gamma  \mathbb{E}(G_{t+1}| S_t = s)
\end{split}
$$&lt;p&gt;
我们考虑去代换两个期望符号&lt;/p&gt;
&lt;p&gt;对于第一个期望，其意义是：从$s$出发，得到奖励的均值&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;始终注意：$R_{t+1}$代表是一个随机变量&lt;/p&gt;&lt;/blockquote&gt;
$$
\begin{split}
\mathbb{E}(R_{t+1}| S_t = s) &amp;= \sum_{a} \pi(a|s) \mathbb{E}(R_{t+1}| S_t=s,A_t = a) \\
 &amp;=\sum_a \pi(a|s) \sum_r p(r|s,a) r
\end{split}
$$&lt;p&gt;
对于第二个期望，其意义是：从下一个时刻$t+1$出发可以得到的期望奖励&lt;/p&gt;
$$
\begin{split}
 \mathbb{E}(G_{t+1}| S_t = s) &amp;= \sum_{s&#39;} \mathbb{E}(G_{t+1}| S_{t+1} = s&#39;) p(s&#39;| s) \\
 &amp;=\sum_{s&#39;} \mathbb{E}(G_{t+1}| S_{t+1} = s&#39;) \sum_{a}\pi(a| s)p(s&#39;| s,a)\\
 &amp;=\sum_{s&#39;} v_{\pi}(s&#39;) \sum_{a}\pi(a| s)p(s&#39;| s,a)
\end{split}
$$&lt;p&gt;
综上，整理一下原式子：&lt;/p&gt;
$$
\begin{split}
v_\pi(s) &amp;=  \mathbb{E}(R_{t+1}| S_t = s) + \gamma  \mathbb{E}(G_{t+1}| S_t = s) \\\\
&amp;= \sum_a \pi(a|s) \sum_r p(r|s,a) r+\gamma\sum_{s&#39;} v_{\pi}(s&#39;) \sum_{a}\pi(a| s)p(s&#39;| s,a)\\
&amp;= \sum_a \pi(a| s) \left[ \sum_r p(r| s,a)r + \gamma \sum_{s&#39;}p(s&#39;| s,a){\color{Red} v_\pi(s&#39;)} \right], \forall s\in S
\end{split}
$$&lt;blockquote&gt;
&lt;p&gt;这里的$S$是state space&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;这样我们就得到了贝尔曼公式，由两个部分组成：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;immediate reward&lt;/li&gt;
&lt;li&gt;future reward&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常我们会使用给定的$\pi(a|s)$​，因此这个过程可以被视作&lt;strong&gt;Policy Evaluation&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$p(r|s,a),p(s&amp;rsquo;|s,a)$在这里是已知的&lt;/p&gt;
&lt;p&gt;但其实不知道也有办法去求解&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;向量形式&#34;&gt;向量形式
&lt;/h3&gt;&lt;p&gt;我们先把贝尔曼公式进行重写，用一些其他符号进行替代：&lt;/p&gt;
$$
v_\pi(s) = r_\pi(s) + \gamma \sum_{s&#39;} p_\pi(s&#39;|s)v_\pi(s&#39;)
$$&lt;p&gt;
表示基于策略$\pi$获取的当前奖励，以及之后的期望奖励&lt;/p&gt;
&lt;p&gt;我们令所有的状态依次编号为：$1\to n$&lt;/p&gt;
&lt;p&gt;对于状态$s_i$，其state value表示为：&lt;/p&gt;
$$
v_\pi(s_i) = r_\pi(s_i) + \gamma \sum_{s_j} p_\pi(s_j|s_i)v_\pi(s_j)
$$&lt;p&gt;
把所有$s_i$写在一起，自然就是向量形式：&lt;/p&gt;
$$
v_\pi = r_\pi + \gamma P_\pi v_\pi
$$&lt;ul&gt;
&lt;li&gt;$v_\pi = \left[ v_\pi(s_1), &amp;hellip;,v_\pi(s_n) \right ]^T \in  \mathbb{R}^n$&lt;/li&gt;
&lt;li&gt;$r_\pi = \left[ r_\pi(s_1), &amp;hellip;,r_\pi(s_n) \right ]^T \in  \mathbb{R}^n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;并且有：&lt;/p&gt;
$$
P_\pi \in \mathbb{R}^{n\times n}, \text{where} \space [P_\pi]_{i,j} = p_\pi(s_j|s_i)
$$&lt;p&gt;理解一下图：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/assets/image-20251030221530292.png&#34;
	width=&#34;596&#34;
	height=&#34;430&#34;
	srcset=&#34;https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/assets/image-20251030221530292_hu_52e37a69891d55fb.png 480w, https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap2-bellman-equation/assets/image-20251030221530292_hu_d861de99c469b5a0.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;matrix&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;solve-state-values&#34;&gt;Solve State Values
&lt;/h3&gt;&lt;p&gt;我们需要通过state value衡量当前的policy的表现&lt;/p&gt;
&lt;p&gt;因此需要进行求解&lt;/p&gt;
&lt;h4 id=&#34;method-1--closed-form-solution&#34;&gt;Method 1 · closed-form solution
&lt;/h4&gt;$$
v_\pi = r_\pi + \gamma P_\pi v_\pi\\
(I-\gamma P_\pi)v_\pi = r_\pi\\
v_\pi = (I-\gamma P_\pi)^{-1}r_\pi
$$&lt;p&gt;
但是由于需要求逆矩阵，计算量过高，这个方法一般不会使用&lt;/p&gt;
&lt;h4 id=&#34;method-2--iterative-solution&#34;&gt;Method 2 · iterative solution
&lt;/h4&gt;$$
v_{\pi, k+1} = r_\pi + \gamma P_\pi v_{\pi,k}\\
$$&lt;p&gt;
我们不断迭代这个等式，可以使得最后有：&lt;/p&gt;
$$
v_{\pi,k} \to v_\pi = (I-\gamma P_\pi)^{-1}r_\pi, k \to \infty
$$&lt;h2 id=&#34;action-value&#34;&gt;Action Value
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;与state value相比，不止固定了当前的状态，action value同时固定了当前执行的动作&lt;/li&gt;
&lt;li&gt;action value意义很大，有时候我们倾向于采取能带来最大value的action&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;定义如下：&lt;/p&gt;
$$
q_\pi(s,a) = \mathbb{E}\left[G_t|S_t=s，A_t=a\right]
$$&lt;p&gt;
同时有：&lt;/p&gt;
$$
\underbrace{\mathbb{E}[G_t \mid S_t = s]}_{v_{\pi}(s)} = \sum_{a} \underbrace{\mathbb{E}[G_t \mid S_t = s, A_t = a]}_{q_{\pi}(s,a)} \pi(a \mid s)
$$&lt;p&gt;
因此：&lt;/p&gt;
$$
{\color{Red} v_\pi(s)}  = \sum_a \pi(a|s){\color{Red} q_\pi(s,a)}\\
 q_\pi(s,a) = \sum_r p(r| s,a)r + \gamma \sum_{s&#39;}p(s&#39;| s,a){\color{Red} v_\pi(s&#39;)}
$$&lt;ul&gt;
&lt;li&gt;第一个式子：通过所有action value，求解当前state value&lt;/li&gt;
&lt;li&gt;第二个式子：通过所有state value，求解当前action value&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;求解的方式比较多样&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过state value&lt;/li&gt;
&lt;li&gt;通过数据采样进行求解（后文）&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>强化学习的数学原理 · Chap1 · 基本概念</title>
        <link>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</link>
        <pubDate>Wed, 29 Oct 2025 23:38:34 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1sd4y167NS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1sd4y167NS&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;概念定义&#34;&gt;概念定义
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State&lt;/strong&gt;：$s$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;state space: the set of states $S = \left{ s_i \right }$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Action&lt;/strong&gt;：$a$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;action space of &lt;strong&gt;a state&lt;/strong&gt;: $\mathcal{A}(s_i) = \left{a_i \right}$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;State Transition&lt;/strong&gt;：$s_i \overset{a}{\rightarrow} s_j$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在确定性的情况，可以使用S+A的表格进行标识&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;State Transition Probability&lt;/strong&gt;：表示成条件概率分布&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Policy&lt;/strong&gt;: determine what actions to take at a state&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\pi$：对于不确定的情况，同样是一个条件概率&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reward&lt;/strong&gt;: a real number we get &lt;strong&gt;after taking an action&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;positive/negative: it represents encouragement or punishment&lt;/li&gt;
&lt;li&gt;corner case
&lt;ul&gt;
&lt;li&gt;zero reward: no punishment&lt;/li&gt;
&lt;li&gt;can positive mean punishment: 取决于怎么设计&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reward本质上是一种人机交互的接口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trajectory&lt;/strong&gt;: a &lt;strong&gt;state-action-reward&lt;/strong&gt; chain&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$s_i\xrightarrow[a_i]{r_i}s_{i+1}\xrightarrow[a_{i+1}]{r_{i+1}}s_{i+2}$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;return&lt;/strong&gt;: the sum of all the rewards along a trajectory&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;discounted return&lt;/strong&gt;：trajectory可能会无限长（走到终点之后仍有奇怪多余的操作），导致reward发散到无穷
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;discount rate&lt;/strong&gt;: $\gamma \in [0,1)$&lt;/li&gt;
&lt;li&gt;防止return发散，通过指数级别的系数累乘，保证return是收敛的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Episode&lt;/strong&gt;: the agent may stop at some terminal states, the resulting trajectory is called an episode(or a trial)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Episodic task（有终止任务）&lt;/li&gt;
&lt;li&gt;Continuing task（持续任务）&lt;/li&gt;
&lt;li&gt;在数学上，可以通过把 &lt;strong&gt;episodic task 转换为 continuing task&lt;/strong&gt; 来统一处理两类任务
&lt;ul&gt;
&lt;li&gt;Option 1：吸收状态 (absorbing state)，当智能体到达目标状态后，就&lt;strong&gt;永远停留在那里&lt;/strong&gt;，之后的奖励永远设为0&lt;/li&gt;
&lt;li&gt;Option2：普通状态 (normal state)，智能体可以离开目标状态，任务会继续，每次进入目标状态时都会获得奖励&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数学与建模上使用Option2会更方便、统一，$\gamma$会控制一切&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;markov-decision-process--mdp&#34;&gt;Markov Decision Process · MDP
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sets&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State: the set of states$\mathcal{S}$&lt;/li&gt;
&lt;li&gt;Action: the set of actions $\mathcal{A}(s)$ is associated for state$s \in \mathcal{S}$&lt;/li&gt;
&lt;li&gt;Reward: the set of rewards $\mathcal{R}(s,a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Probability distribution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;State transition probability: at state $s$, taking action $a$, the probability to transit to state $s&amp;rsquo;$ is $p(s&amp;rsquo;|s, a)$&lt;/li&gt;
&lt;li&gt;Reward probability: at state $s$, taking action $a$, the probability to get reward $r$ is $p(r|s, a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Policy: at state $s$, the probability to choose action a is $\pi(a|s)$&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Markov property&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(s_{t+1} | a_{t+1}, s_t, &amp;hellip;, a_1, s_0) = p(s_{t+1} | a_{t+1}, s_t)$&lt;/li&gt;
&lt;li&gt;$p(r_{t+1} | a_{t+1}, s_t, &amp;hellip;, a_1, s_0) = p(r_{t+1} | a_{t+1}, s_t)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Markov decision process&lt;/strong&gt; becomes &lt;strong&gt;Markov process&lt;/strong&gt; once the policy is given.&lt;/p&gt;&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;似乎这里的定义和比较官方的定义有一些差别&lt;/p&gt;
&lt;p&gt;MDP是一个在马尔可夫性质上的时序决策模型，由一个五元组定义：&lt;/p&gt;
$$
S,A,P,R,\gamma
$$&lt;ul&gt;
&lt;li&gt;状态集合State Space&lt;/li&gt;
&lt;li&gt;动作集合Action Space&lt;/li&gt;
&lt;li&gt;状态转移概率Transition Probability&lt;/li&gt;
&lt;li&gt;奖励函数Reward&lt;/li&gt;
&lt;li&gt;折扣因子Discount Factor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个MDP的动态过程可以描述为一个与时间交互的循环：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;处于某个状态&lt;/li&gt;
&lt;li&gt;执行一个动作&lt;/li&gt;
&lt;li&gt;根据状态转移概率转移到某个状态&lt;/li&gt;
&lt;li&gt;获得动作的即时奖励&lt;/li&gt;
&lt;li&gt;重复&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
