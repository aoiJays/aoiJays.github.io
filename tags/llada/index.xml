<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>LLaDA on BiribiriBird</title>
        <link>https://example.com/tags/llada/</link>
        <description>Recent content in LLaDA on BiribiriBird</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Fri, 26 Sep 2025 15:46:00 +0800</lastBuildDate><atom:link href="https://example.com/tags/llada/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Diffusion Language Model · 论文笔记（一）</title>
        <link>https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
        <pubDate>Fri, 26 Sep 2025 15:46:00 +0800</pubDate>
        
        <guid>https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布&lt;/p&gt;
&lt;p&gt;因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布&lt;/p&gt;
&lt;p&gt;因此指令跟随、上下文学习并不是ARM的专利&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach
&lt;/h2&gt;&lt;h3 id=&#34;概率公式&#34;&gt;概率公式
&lt;/h3&gt;&lt;h4 id=&#34;前向过程forward-process&#34;&gt;前向过程Forward Process
&lt;/h4&gt;&lt;p&gt;序列中逐渐添加mask，直到所有序列全部masked&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个标记有一定概率masked，或者保持unmasked状态&lt;/li&gt;
&lt;li&gt;给定原始数据$x_0$，随机采样的一个时间点$t \in [0,1]$
&lt;ul&gt;
&lt;li&gt;$t=0$表示起点，全部token都是unmasked&lt;/li&gt;
&lt;li&gt;$t=1$表示终点，全部token都已经masked&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;序列中每个token的masked概率就是$t$&lt;/li&gt;
&lt;li&gt;该时刻的序列被定义为$x_t$​&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Bert的mask比例是固定的&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;反向过程reverse-process&#34;&gt;反向过程Reverse Process
&lt;/h4&gt;&lt;p&gt;参数为$\theta$的模型，生成序列$x_0$的概率$p_\theta(x_0)$就是我们需要训练的模型&lt;/p&gt;
&lt;p&gt;我们希望其尽可能接近真实数据的分布$p_{data}(x_0)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;反向过程的目标：$x_{t=1}$出发，恢复$x_0$&lt;/li&gt;
&lt;li&gt;方法：通过&lt;code&gt;mask predictor&lt;/code&gt;，逐步填充&lt;code&gt;masked token&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mask-predictor&#34;&gt;Mask Predictor
&lt;/h4&gt;&lt;p&gt;LLaDA的核心是一个&lt;code&gt;mask predictor&lt;/code&gt;&lt;/p&gt;
$$
p_\theta\left (\cdot\mid x_t \right)
$$&lt;p&gt;
其中的&lt;code&gt;·&lt;/code&gt;表示一个占位符&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;I [MASK] cats.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;则$p_\theta(\cdot \mid [I,[MASK],cats])$ 就是一个基于词表的概率分布表&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;token&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;p&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;like&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;eat&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此时&lt;code&gt;Mask Predictor&lt;/code&gt;会对所有&lt;code&gt;[MASK]&lt;/code&gt;进行预测&lt;/p&gt;
&lt;p&gt;不像ARM只预测一个token&lt;/p&gt;
&lt;p&gt;假设序列为：$(x_t^1, x_t^2,&amp;hellip;,x_t^L)$&lt;/p&gt;
&lt;p&gt;我们的目标即为，对于&lt;code&gt;masked&lt;/code&gt;位置$i$，最大化概率：&lt;/p&gt;
$$
p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;其中$x_0^i$就是原序列的ground truth&lt;/p&gt;
&lt;h4 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h4&gt;&lt;p&gt;对于单个&lt;code&gt;masked&lt;/code&gt;位置，我们希望概率尽可能大&lt;/p&gt;
&lt;p&gt;因此需要使用交叉熵损失&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;补习一下交叉熵&lt;/p&gt;
&lt;p&gt;假设真实分布是$q(y)$，模型分布是$p_\theta(y)$&lt;/p&gt;
&lt;p&gt;交叉熵定义为：&lt;/p&gt;
$$
H(q,p_\theta) = -\sum_y  q(y)\log{p_\theta(y)}
$$&lt;p&gt;
在该任务下，$q(y)$​是一个独热分布&lt;/p&gt;
$$
q(y) = \left\{\begin{matrix}
  1&amp;,y=x_0^i \\
  0&amp;,otherwise
\end{matrix}\right.
$$&lt;p&gt;
代入得&lt;/p&gt;
$$
\mathcal{L}(x_0^i,x_t) = -\log p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;
故整体的损失就只需要对所有&lt;code&gt;masked&lt;/code&gt;位置求和&lt;/p&gt;
$$
\mathcal{L}(x_0,x_t) = -\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$&lt;p&gt;
其中$1\left[ x^i_t=M\right] $表示指示函数，确保代入计算的数值是&lt;code&gt;[MASK]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;但是这样是不合理的，序列中&lt;code&gt;[MASK]&lt;/code&gt;越多，损失似乎会越大&lt;/p&gt;
&lt;p&gt;因此需要做一下归一化&lt;/p&gt;
&lt;p&gt;对于$x_t$，其在该时刻会有$tL$个token被&lt;code&gt;masked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;因此需要代入一个$\frac{1}{t}$​（值得一提的是，$\frac{1}{t} \geq 1$）&lt;/p&gt;
$$
\mathcal{L}(x_0,x_t) = -\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$&lt;p&gt;
建立关于模型参数$\theta$的损失函数则有：&lt;/p&gt;
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$&lt;ul&gt;
&lt;li&gt;$\triangleq$代表&lt;code&gt;定义为&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;这里不是在陈述一个“事实”，而是在&lt;strong&gt;引入损失函数的定义&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果写$=$，读者可能会以为“这是某个推导得到的等式”；&lt;/p&gt;
&lt;p&gt;如果写 $\triangleq$，读者一眼就知道：哦，这是“定义”，不是推导。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;从均匀分布$U(0,1)$采样的任意$t$，从数据集中采样的任意数据$x_0$，根据前向传播方法得到的$x_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;负对数似然的上界&#34;&gt;负对数似然的上界
&lt;/h4&gt;&lt;p&gt;我们本身的目标是使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$&lt;/p&gt;
&lt;p&gt;但是我们从未单独定义、训练$p_\theta(x)$这个模型&lt;/p&gt;
&lt;p&gt;而是定义了一个$p_\theta(x_0^i\mid x_t)$，不断重复迭代，起到了$p_\theta(x)$的作用&lt;/p&gt;
&lt;p&gt;因此我们上述内容得到的$L(\theta)$是作为模型$p_\theta(x_0^i\mid x_t)$的损失函数&lt;/p&gt;
&lt;p&gt;我们如何保证训练出这个模型，可以使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$？&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我们定义真实的似然函数是&lt;/p&gt;
$$
\mathcal{L}(\theta) = -\mathbb{E}_{p_{data}(x_0)}\left [\log{p_{\theta}(x_0)}\right]
$$&lt;ul&gt;
&lt;li&gt;按照真实分布，采样数据$x_0$，得到的似然函数期望&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们需要最小化这个式子&lt;/p&gt;
&lt;p&gt;定义前向加噪分布$q$：&lt;/p&gt;
$$
q(x_t\mid x_0) = \prod_{i=1}^L \left [ (1-t)\times 1(x_t^i=x_0^i) + t\times 1(x_t^i=M) \right]
$$&lt;p&gt;
其描述了通过已知的前向过程，从$x_0$得到$x_t$的概率&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$q$是我们引入的噪声分布，并非需要训练的参数模型，不使用$p$定义&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;由于前向过程是已知的，我们考虑使用它表示$p_\theta$&lt;/p&gt;
$$
p_\theta(x_0) = \sum_{x_t}p_\theta(x_0\mid x_t)p_\theta(x_t)= \sum_{x_t}p_\theta(x_0, x_t) 
$$&lt;blockquote&gt;
&lt;p&gt;原始句子出现的总概率就是把所有可能路径的概率加起来。&lt;/p&gt;
&lt;p&gt;（先得到$x_t$，再生成$x_0$）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;引入已知的$q$&lt;/p&gt;
$$
p_\theta(x_0) = \sum_{x_t}q(x_t\mid x_0)\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}
$$&lt;p&gt;
其中$\sum_{x_t}q(x_t\mid x_0)] \times (\cdot)$，可以理解为对$q(x_t\mid x_0)$的期望&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;离散情况下：$\mathbb{E}_{x\sim r}(g(x)) = \sum_x r(x)g(x)$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;此时$x_0$是当作固定值，所有都可以看作关于$x_t$的函数&lt;/p&gt;
&lt;p&gt;因此则有：&lt;/p&gt;
$$
p_\theta(x_0) = \mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]
$$&lt;blockquote&gt;
&lt;p&gt;任意从$q$分布中采样$x_t$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;采样 Jensen不等式：&lt;/p&gt;
$$
\log \mathbb{E}(z) \geq \mathbb{E}(\log{z})
$$&lt;p&gt;
此时则有：&lt;/p&gt;
$$
\log{p_\theta}(x_0) = \log{\mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]} \geq \mathbb{E}_{ q(x_t\mid x_0)}\left[\log{p_\theta(x_0,x_t)} - \log{q(x_t\mid x_0)} \right]
$$$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0,x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$&lt;p&gt;
分解一下联合概率&lt;/p&gt;
$$
\log{p_\theta(x_0,x_t)} = \log{p_\theta(x_0\mid x_t)} + \log{p_\theta(x_t)}
$$&lt;p&gt;
代回则有：&lt;/p&gt;
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}-\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$&lt;ul&gt;
&lt;li&gt;$p_\theta(x_t)$：其中$x_t$是前向过程人为生成的，因此与$\theta$无关&lt;/li&gt;
&lt;li&gt;$q(x_t\mid x_0)$是噪声项，与$\theta$无关&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此可以写成&lt;/p&gt;
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)} + \text{const}
$$&lt;p&gt;
两边同时对真实数据计算期望&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}(\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}) + \text{const}
$$&lt;p&gt;
左边实质上就是我们需要优化的目标，命名为负对数似然$\text{NLL}$&lt;/p&gt;
&lt;p&gt;根据&lt;/p&gt;
$$
\log {p_\theta(x_0\mid x_t)} = \sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)
$$&lt;p&gt;
代入得：&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const}
$$&lt;p&gt;
事实上右边就是：&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] = 
 -\mathbb{E}_{t,x_0,x_t}\left[\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right] = tL(\theta)
$$&lt;p&gt;
上述内容都是正项（负概率对数），$t \in [0,1]$，因此满足&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const} \leq L(\theta) + \text{const}
$$&lt;p&gt;
则有：&lt;/p&gt;
$$
\text{NLL} = -\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq L(\theta) + \text{const}
$$&lt;p&gt;
至此，成功证明了$L(\theta)$决定了$\text{NLL}$​的上界（常数可以忽略）&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-Training
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2.png&#34;
	width=&#34;1628&#34;
	height=&#34;456&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_9a85fb3e2f02f873.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_3f7bfef69b5c6f92.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;A Conceptual Overview of LLaDA.&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;357&#34;
		data-flex-basis=&#34;856px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：&lt;code&gt;mask predictor&lt;/code&gt;$p_\theta$，训练数据$p_{data}$&lt;/li&gt;
&lt;li&gt;输出：$p_{\theta}$（收敛）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976.png&#34;
	width=&#34;1570&#34;
	height=&#34;385&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_f227ce11f9ed9b16.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_7a4e250f2607c545.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pre-Train Algorithm&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;407&#34;
		data-flex-basis=&#34;978px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mask predictor&lt;/code&gt;采用Transformer架构
&lt;ul&gt;
&lt;li&gt;不采用&lt;code&gt;causal mask&lt;/code&gt;，能看见双向上下文&lt;/li&gt;
&lt;li&gt;未使用KV Cache，采用标准的&lt;code&gt;Vanilla Multi-Head Attention&lt;/code&gt;，每个头单独一份&lt;code&gt;k,q,v&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Transformer架构尽量与LLaMA3对齐，从&lt;code&gt;attention&lt;/code&gt;和&lt;code&gt;FFN&lt;/code&gt;两个参数大头中，选择了减少&lt;code&gt;FFN&lt;/code&gt;的参数量，保持参数规模可以比较&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;在自回归 LLM 生成时，生成新 token 时可以复用之前的 K/V 矩阵（不用重新算整个序列的注意力）。&lt;/p&gt;
&lt;p&gt;这是 KV cache 的意义：极大加速推理，节省显存。&lt;/p&gt;
&lt;p&gt;但 LLaDA 每一步预测的是 &lt;strong&gt;全局被 mask 的位置（不是单个 token）&lt;/strong&gt;，所以每一步输入分布会变，全序列 K/V 都要重新计算 → KV cache 无法使用。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;99%的数据固定长度4096&lt;/li&gt;
&lt;li&gt;1%的数据随机采样长度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sft&#34;&gt;SFT
&lt;/h3&gt;&lt;p&gt;对于问答对$(p_0,r_0)$，我们不改变提问部分，只对&lt;strong&gt;回答部分&lt;/strong&gt;进行掩码加噪得到$r_t$&lt;/p&gt;
&lt;p&gt;损失函数设计为：&lt;/p&gt;
$$
-\mathbb{E}_{t,p_0,r_0,r_t}\left[\frac{1}{t}\sum_{i=1}^{L&#39;}1[r^i_t = M]p_\theta(r_0^i\mid p_0,r_t)\right]
$$&lt;ul&gt;
&lt;li&gt;$r_0$的长度是天然动态的，使用&lt;code&gt;EOS&lt;/code&gt;填充&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference
&lt;/h3&gt;&lt;p&gt;给定$p_0$，我们从完全掩码的$r_1$开始&lt;/p&gt;
&lt;p&gt;设定超参数如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;迭代次数（采样步骤总数）：a trade off between efficency and quality&lt;/li&gt;
&lt;li&gt;生成长度：实质上是一个上界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设我们从时间$t\in(0, 1]$转移到$s\in[0,t)$，需要做的事是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_0,r_t$作为模型的输入，预测$r_0$（模型会&lt;code&gt;unmask&lt;/code&gt;所有被掩码的token）&lt;/li&gt;
&lt;li&gt;由于我们只转移到$s$，因此需要保留$sL$个掩码
&lt;ul&gt;
&lt;li&gt;对预测出的$r_0$，从中&lt;strong&gt;随机&lt;/strong&gt;&lt;code&gt;remask&lt;/code&gt;$\frac{s}{t}L$个token&lt;/li&gt;
&lt;li&gt;得到$r_s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$s = t, r_t = r_s$重复迭代&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认将$|t-s|$​是一个定值，以定长的步长进行迭代&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257.png&#34;
	width=&#34;1647&#34;
	height=&#34;627&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_539b4cf9daaad9c4.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_3153327713f2069d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Reverse Process&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;262&#34;
		data-flex-basis=&#34;630px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;理论上remask策略是随机的&lt;/p&gt;
&lt;p&gt;但是论文给出了两种基于&lt;code&gt;退火&lt;/code&gt;的remask策略&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在生成过程中，需要随机性逐步递减，冻结高确定性的部分、把随机性集中在不确定区域&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;low-confidence remasking&lt;/strong&gt;：取置信度最低的$\frac{s}{t}L$个预测token进行remask&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;semi-autoregressive&lt;/strong&gt; remasking：对序列进行分块，从左到右顺序生成&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789.png&#34;
	width=&#34;1473&#34;
	height=&#34;558&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_c595a78f46bb5a31.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_8789f573c459aaef.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;semi-autoregressive&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;633px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;h3 id=&#34;实验1--scalability&#34;&gt;实验1 · Scalability
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;验证：LLaDA是否与自回归模型ARM具有相同的可拓展性&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;目的：证明论文的核心论点：&lt;/p&gt;
&lt;p&gt;​	理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布&lt;/p&gt;
&lt;p&gt;​	因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布&lt;/p&gt;
&lt;p&gt;​	因此指令跟随、上下文学习并不是ARM的专利&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;实验设计&#34;&gt;实验设计
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932.png&#34;
	width=&#34;1568&#34;
	height=&#34;583&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_4f064368af95b6bc.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_8bdc715fd4ddd50c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;架构&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;645px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;针对MDM和ARM两类语言模型，进行如下控制变量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型结构：采样同一套Transformer架构（优化器、参数量……各种机制），只修改了mask&lt;/li&gt;
&lt;li&gt;参数量：在1B规模下完全一致，在7B规模由于资源限制有一些不同
&lt;ul&gt;
&lt;li&gt;Transformer：causal mask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据：预训练语料相同&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;唯一的实验的变量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FLOPs：使用6ND公式作为横轴
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;N&lt;/strong&gt; 是模型的非 embedding 参数量（固定，比如 1B 或 8B）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;D&lt;/strong&gt; 是训练过的 token 数量（数据量，可以变化）&lt;/li&gt;
&lt;li&gt;实验通过改变 D ，计算出 6 × N × D （即训练 FLOPs）作为横轴的计算预算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验指标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MMLU、ARC-C、CMMLU、PIQA、GSM8K、HumanEval&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（多任务、推理、中文、物理、数学、代码）&lt;/p&gt;
&lt;h4 id=&#34;实验结果&#34;&gt;实验结果
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678.png&#34;
	width=&#34;1472&#34;
	height=&#34;707&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_598efe4120660d9c.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_8ec2789b84388b8c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;实验结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;499px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部分任务体现优势&lt;/li&gt;
&lt;li&gt;对于性能稍逊的任务（PIQA），差距也在逐渐缩小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时喷了先前的一篇工作的结论：&lt;code&gt;达到相同的似然需要16倍算力&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;似然是间接指标（LLaDA的lower bound）&lt;/li&gt;
&lt;li&gt;先前的工作只有GPT2的参数量，本文提高到7-8B&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;结论：LLaDA 在相同训练规模与算力条件下，表现出与 ARM 相似甚至更强的可扩展性。&lt;/p&gt;
&lt;h3 id=&#34;实验2--benchmark&#34;&gt;实验2 · Benchmark
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;验证：LLaDA经过预训练和SFT之后是否能够和已有的ARM在&lt;strong&gt;上下文学习&lt;/strong&gt;与&lt;strong&gt;指令遵循能力&lt;/strong&gt;上进行竞争&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验设计-1&#34;&gt;实验设计
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;实验对象：LLaDA、一系列参数相当的模型
&lt;ul&gt;
&lt;li&gt;Base阶段：比较了所有模型的预训练base模型&lt;/li&gt;
&lt;li&gt;instruct阶段：LLaDA只进行了SFT，其他模型均完成了SFT+RL
&lt;ul&gt;
&lt;li&gt;原文：交给未来的工作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;任务：通用、数学科学、代码、中文等常见benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验结果-1&#34;&gt;实验结果
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237.png&#34;
	width=&#34;1208&#34;
	height=&#34;717&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_caa01b5802879200.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_c7856aff322a4d51.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Base&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在所有任务上超过LLaMA2 7B，与LLaMA3 8B相当&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有模型的训练数据存在差异&lt;/li&gt;
&lt;li&gt;作者认为LLaDA的优势区间与劣势区间的主要原因在数据质量与分布上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GSM8K数据集上体现了显著的优势，论文针对这个情况做了补充实验，证明不存在数据泄露&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440.png&#34;
	width=&#34;1205&#34;
	height=&#34;574&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_db0f9cb7c5edf55e.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_5dfaa65484da0854.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;SFT&amp;#43;RL&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;503px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SFT数据质量较差，出现了性能下降（MMLU）&lt;/li&gt;
&lt;li&gt;没有采用RL，因此性能略微落后LLaMA3 8B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在数据集透明度不足的情况下，以丰富的标准化流程、多样化任务，足以证明LLaDA的性能卓越，是唯一具备竞争力的非自回归模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验2--补充实验&#34;&gt;实验2 · 补充实验
&lt;/h3&gt;&lt;p&gt;验证：LLaDA在GSM8K数据集中的优势不来源于数据泄露（data leakage），检测在全新数据集中仍然能保证推理能力&lt;/p&gt;
&lt;p&gt;省流：找了一个2024年的新数据集，模仿GSM8K的形式做一遍实验&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145.png&#34;
	width=&#34;548&#34;
	height=&#34;173&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_ad845088f2d886d0.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_8a04e4c9a021371f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;iGSM Dataset&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;316&#34;
		data-flex-basis=&#34;760px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA在所有难度（解题步骤数）中均显著优势&lt;/li&gt;
&lt;li&gt;两类模型随着难度上升准确度逐渐下降，但是LLaDA下降较慢&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA允许模型在每一步同时考虑全局 token 关系，因此在多变量方程、层次关系推理中优于单向自回归&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验3--reversal-reasoning-and-analyses&#34;&gt;实验3 · Reversal Reasoning and Analyses
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Reversal Curse（反向诅咒）：ARM从左到右生成序列，因此反向生成或逆序推理的表现很差&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;验证：LLaDA是否克服了反向诅咒&lt;/p&gt;
&lt;h4 id=&#34;实验设计-2&#34;&gt;实验设计
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;数据：496对著名中文诗句（上下两句），每一句子（A,B）构成两个任务
&lt;ul&gt;
&lt;li&gt;Forward：给定A预测B&lt;/li&gt;
&lt;li&gt;Backward：给定B预测A&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;窈窕淑女的下一句是什么？直接输出句子即可。 Answer: 君子好逑。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;不拘一格降人才的上一句是什么？直接输出句子即可。 Answer: 我劝天公重抖擞。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318.png&#34;
	width=&#34;990&#34;
	height=&#34;401&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_f6b1b119ec68bdcd.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_9284f82611816e94.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Reversal Curse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;592px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPT-4o 和 Qwen2.5 均有更大数据和RL优化，但仍失败&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LLaDA 虽仅 SFT，无RL，仍在 reversal 上大幅领先&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;附录补充&#34;&gt;附录补充
&lt;/h4&gt;&lt;p&gt;论文从三个角度补充了为什么LLaDA是无方向偏置的模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理论证明：LLaDA本质上等价于在所有生成顺序上做平均，从而消除方向偏置
&lt;ul&gt;
&lt;li&gt;解释为什么 diffusion 结构在数学上是方向对称的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;实现机制：理论正确的情况下，需要确保算法实现不出现&lt;strong&gt;从左到右&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;确保生成算法本身不引入方向信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;超参数层面：通过实验说明采样步数与效率不会干扰方向一致性
&lt;ul&gt;
&lt;li&gt;排除方向性差异由采样精度造成的可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;a2-inference&#34;&gt;A.2 Inference
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;目的：证明LLaDA训练和推理目标等价于&lt;strong&gt;对所有可能生成顺序的平均建模&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于训练时的核心目标函数：&lt;/p&gt;
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$&lt;p&gt;
训练目标是：模型在任意mask模式下，都能预测出原token&lt;/p&gt;
&lt;p&gt;该训练模式不会看到任何固定方向的序列，故天然是双向建模的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推不动了，pass一下&lt;/p&gt;&lt;/blockquote&gt;
&lt;h5 id=&#34;remasking&#34;&gt;Remasking
&lt;/h5&gt;&lt;p&gt;反向过程的核心：预测 - 重新掩码 - 继续预测&lt;/p&gt;
&lt;p&gt;上文提到了三种不同的掩码策略&lt;/p&gt;
&lt;p&gt;论文使用GSM8K进行了消融实验&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成长度固定512&lt;/li&gt;
&lt;li&gt;采样步数固定256（长度的一半）&lt;/li&gt;
&lt;li&gt;block：32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549.png&#34;
	width=&#34;1329&#34;
	height=&#34;223&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_a185194ec8c04b53.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_9209def52fe04210.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;remask&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;595&#34;
		data-flex-basis=&#34;1430px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base模型：最低置信度即可，半自回归是不需要的&lt;/li&gt;
&lt;li&gt;Instruct模型：必须是最低置信度+半自回归
&lt;ul&gt;
&lt;li&gt;单独最低置信度会严重降低性能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;论文解释：SFT阶段引入了大量&lt;code&gt;EOS&lt;/code&gt;，模型一般会给&lt;code&gt;EOS&lt;/code&gt;较大的置信度。因此推理时&lt;code&gt;EOS&lt;/code&gt;会被大量生成，并且几乎不可能被&lt;code&gt;remask&lt;/code&gt;（置信度非常高）&lt;/p&gt;
&lt;p&gt;因此需要引入半自回归，保证每个块内收敛出连续的内容，抑制&lt;code&gt;EOS&lt;/code&gt;早产&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;尽管引入半自回归，但是块内仍然是并行的（？）&lt;/p&gt;
&lt;p&gt;补充一下，模型对生成长度这个超参数非常不敏感&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980.png&#34;
	width=&#34;1607&#34;
	height=&#34;288&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_496709f8b88b2255.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_4144e169c1540880.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Length&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;557&#34;
		data-flex-basis=&#34;1339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;但对采样步数非常敏感（生成长度1024）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929.png&#34;
	width=&#34;1472&#34;
	height=&#34;508&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_afc64e86f9a1bafe.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_6b0c69ba2a7f9c56.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;采样&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;289&#34;
		data-flex-basis=&#34;695px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA不受自回归方向性的约束，具有更平衡的前后向建模能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;case-studies&#34;&gt;Case Studies
&lt;/h3&gt;&lt;p&gt;附录中展示了一些其他例子，说明生成的对话是出色的（单轮、多轮）&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
