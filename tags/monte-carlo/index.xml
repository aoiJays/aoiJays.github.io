<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Monte Carlo on BiribiriBird</title>
        <link>https://example.com/tags/monte-carlo/</link>
        <description>Recent content in Monte Carlo on BiribiriBird</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Mon, 03 Nov 2025 01:10:34 +0800</lastBuildDate><atom:link href="https://example.com/tags/monte-carlo/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>强化学习的数学原理 · Chap5 · Monte Carlo Learning</title>
        <link>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap5-monte-carlo-learning/</link>
        <pubDate>Mon, 03 Nov 2025 01:10:34 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86-chap5-monte-carlo-learning/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.bilibili.com/video/BV1sd4y167NS&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://www.bilibili.com/video/BV1sd4y167NS&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;model-based：概率模型已知&lt;/li&gt;
&lt;li&gt;model-free：概率模型未知&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;抛硬币模型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;model-based: 基于0.5的概率直接把期望、分布全部都算出来&lt;/li&gt;
&lt;li&gt;model-free: 重复采样多次，以样本均值作为期望&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;mc-basic&#34;&gt;MC Basic
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;最简单的基于蒙特卡洛的强化学习算法&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;本质上是转化Policy Iteration为Model-Free&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中Policy Iteration中的一步：&lt;/p&gt;
$$
\pi_{k+1}=\arg \max_\pi \sum_a \pi(a|s){\color{red}q_{\pi_k}(s,a)}, \quad s\in \mathcal{S}
$$&lt;p&gt;
其中这一步的action value可以表示为：&lt;/p&gt;
$$
\begin{split}
q_{\pi_k}(s,a) &amp;=\sum_{r}p(r|s,a)r+\gamma\sum_{s&#39;}p(s&#39;|s,a)v_{\pi_k}(s&#39;) \\ 
&amp;= \mathbb{E}\left[ G_t \mid S_t = s, A_t = a\right]
\end{split}
$$&lt;p&gt;
第一个公式是依赖于模型$p$的，而第二个公式就可以通过采样去近似&lt;/p&gt;
&lt;p&gt;基于状态$s$采取动作$a$，依据策略$\pi_k$，生成一个episode，记return为$g(s,a)$&lt;/p&gt;
&lt;p&gt;则我们可以将$g(s,a)$看作一个$G_t$的采样&lt;/p&gt;
&lt;p&gt;因此我们大量重复采样，得到序列$\left { g^{(j)}(s,a)\right }$&lt;/p&gt;
&lt;p&gt;则可以近似：&lt;/p&gt;
$$
q_{\pi_k}(s,a) = \mathbb{E}\left [ G_t\mid S_t = s,A_t = a \right] \approx\frac{1}{N}\sum_jg^{(j)}(s,a)
$$&lt;blockquote&gt;
&lt;p&gt;没有模型的时候，你最好有数据（统计学叫Sample，强化学习叫Experience）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;因此整个算法的流程：&lt;/p&gt;
&lt;p&gt;从$\pi_0$出发，迭代到$k$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step1：Policy Evaluation
&lt;ul&gt;
&lt;li&gt;本质上就是希望得到所有的$q$&lt;/li&gt;
&lt;li&gt;对&lt;strong&gt;每一个$(s,a)$对&lt;/strong&gt;，生成大量的episodes&lt;/li&gt;
&lt;li&gt;计算均值，作为$q_{\pi_k}(s,a)$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Step2：Policy Improvement
&lt;ul&gt;
&lt;li&gt;贪心策略：$\pi_{k+1}$基于最大的$q$去选择&lt;/li&gt;
&lt;li&gt;这里和model-based是一样的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;因此我们可以直接通过这个方法得到$\pi$，而不是state value&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;但是算法效率比较差 后续有优化&lt;/p&gt;
&lt;p&gt;甚至这个算法也只是一个idea 没有具体的名字&lt;/p&gt;
&lt;p&gt;是作者取的&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Eposide Length是重要的、需要够长：采样的步数太短，会导致均值不够准确&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mc-exploring-starts&#34;&gt;MC Exploring Starts
&lt;/h2&gt;&lt;p&gt;MC Basic的数据采样效率非常低&lt;/p&gt;
&lt;p&gt;需要对每一个$(s,a)$对进行大量采样&lt;/p&gt;
$$
s_1,a_1 \to s_2,a_5 \to {\color{red}s_1,a_2} \to s_2,a_3 \to a_5,a_1 \to ...
$$&lt;p&gt;
在MC Basic中，这一条episode只会用来计算最开始的$s_1,a_1$​的action value&lt;/p&gt;
&lt;p&gt;但其实中间的$s,a$对是可以当成一次采样的&lt;/p&gt;
&lt;p&gt;我们可以从两个角度开始优化：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;复用&lt;/strong&gt;：我们希望一整条episode都能被利用，有两个方法
&lt;ul&gt;
&lt;li&gt;First Visit：只采样每个episode中每个$(s,a)$​第一次出现
&lt;ul&gt;
&lt;li&gt;样本独立&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Every Visit：每次出现$(s,a)$​都当成一次采样
&lt;ul&gt;
&lt;li&gt;样本更多，但是相关性会变强&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;即时更新&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Basic：跑完所有的episode，统一做一次Policy Improvement&lt;/li&gt;
&lt;li&gt;但可以用单个 episode的return来立即更新action value
&lt;ul&gt;
&lt;li&gt;本质上可以看作是Truncated Policy Improvement&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;算法流程：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;初始化策略$\pi_0$​&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;“Exploring starts”&lt;/strong&gt; 意思是：所有 (s, a) 都有可能成为 episode 的起点。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;随机选择一个起点$s_0,a_0$​​——&lt;strong&gt;Exploring Starts&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;需要保证所有的状态都是非0的概率能作为起点&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;按照当前策略$\pi$生成一个episode&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;反向更新&lt;/strong&gt;（相当于后缀和，方便计算return）
&lt;ul&gt;
&lt;li&gt;每个$s,a$​会维护一个列表&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;First Visit&lt;/strong&gt;：如果当前$s,a$对是$(s_0,a_0,s_1,a_1,s_2,a_2,&amp;hellip;)$中没有出现过的（第一次出现）
&lt;ul&gt;
&lt;li&gt;将当前的return加入到对应列表&lt;/li&gt;
&lt;li&gt;计算列表中的均值，更新$q(s,a)$&lt;/li&gt;
&lt;li&gt;根据$q$实时更新$\pi$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mc-epsilon-greedy&#34;&gt;MC Epsilon Greedy
&lt;/h2&gt;&lt;p&gt;但现实中Exploring Starts难以实现，有时候我们很难自由选择一个起点开始采样&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;你总不能每次采样一个起点，就搬动机器人过去一次吧&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;前两个算法之所以要对不同的$s,a$做采样，本质原因是：&lt;/p&gt;
$$
\pi(a|s) = \begin{cases}
 1 &amp; \text{ if } a=a^*(s) \\
 0 &amp; \text{ if } a\neq a^*(s)
\end{cases}
$$&lt;p&gt;
我们的策略是greedy的，因此会导致从真实起点出发，非常多的状态无法被覆盖到&lt;/p&gt;
&lt;p&gt;因此无法针对单一起点做反复采样&lt;/p&gt;
&lt;p&gt;所以我们可以考虑引入一点随机性：&lt;/p&gt;
$$
\pi(a|s) = \begin{cases}
 1-\frac{\varepsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1) &amp; \text{ for the greedy action }\\
 \frac{\varepsilon}{|\mathcal{A}(s)|} &amp; \text{ for the other }|\mathcal{A}(s)|-1 \text{ actions} 
\end{cases} \\
\text{where } \varepsilon \in [0,1] \text{ and } |\mathcal{A}(s)| \text{ is the number of actions.}
$$&lt;p&gt;
假设有5个action，其中greedy action是$a_0$，定义$\varepsilon=0.2$&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;action&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$a_0$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$a_1$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$a_2$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$a_3$&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;$a_4$&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;probability&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.84&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.04&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.04&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.04&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.04&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;同时，我们可以保证任意时刻greedy action的概率都是最大的&lt;/p&gt;
&lt;p&gt;$\varepsilon=1$时，所有action概率相同&lt;/p&gt;
$$
1-\frac{\varepsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1)=1-\varepsilon + \frac{\varepsilon}{|\mathcal{A}(s)|} \geq \frac{\varepsilon}{|\mathcal{A}(s)|}
$$&lt;p&gt;算法维持了一个平衡：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;探索性：越大的$\varepsilon$带来越强的探索性，只要episode足够长，就能探索所有状态&lt;/li&gt;
&lt;li&gt;最优性：越大的$\varepsilon$带来的策略自然是更差的，因为每一步走向greedy的概率变低了&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;可以动态调整，一开始大，逐渐变小&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;算法流程：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;去掉了Exploring Starts的条件：&lt;strong&gt;需要保证所有的状态都是非0的概率能作为起点&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;修改策略更新的分布&lt;/li&gt;
&lt;/ul&gt;
$$
\pi(a|s) = \begin{cases}
 1-\frac{\varepsilon}{|\mathcal{A}(s)|}(|\mathcal{A}(s)|-1) &amp;  a=a^*(s)\\
 \frac{\varepsilon}{|\mathcal{A}(s)|} &amp;  a\neq a^*(s)
\end{cases} \\
$$&lt;p&gt;
其他都是和前文算法一致&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
