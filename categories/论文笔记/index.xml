<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>论文笔记 on BiribiriBird</title>
        <link>https://example.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/</link>
        <description>Recent content in 论文笔记 on BiribiriBird</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <copyright>Example Person</copyright>
        <lastBuildDate>Sat, 13 Dec 2025 21:20:00 +0800</lastBuildDate><atom:link href="https://example.com/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Personalization Survey</title>
        <link>https://example.com/p/personalization-survey/</link>
        <pubDate>Sat, 13 Dec 2025 21:20:00 +0800</pubDate>
        
        <guid>https://example.com/p/personalization-survey/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;rap-retrieval-augmented-personalization-for-multimodal-large--language-models&#34;&gt;RAP Retrieval-Augmented Personalization for Multimodal Large  Language Models
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2410.13360&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RAP Retrieval-Augmented Personalization for Multimodal Large  Language Models&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;intro&#34;&gt;Intro
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对每个concept进行微调是昂贵的&lt;/li&gt;
&lt;li&gt;MyVLM和Yo&amp;rsquo;LLaVA都需要进行额外的训练去添加新的concept从而实现个性化&lt;/li&gt;
&lt;li&gt;都需要一些样本的正例与负例进行学习&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251217212521296.png&#34;
	width=&#34;1485&#34;
	height=&#34;343&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251217212521296_hu_507ae72255eda6a9.png 480w, https://example.com/p/personalization-survey/assets/image-20251217212521296_hu_eca083438ec3aa1c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;432&#34;
		data-flex-basis=&#34;1039px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RAP只需要一张图以及一些相关的信息&lt;/li&gt;
&lt;li&gt;支持用户&lt;strong&gt;实时修改&lt;/strong&gt;database，从而调整模型的个性化输出&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时RAP构建了一个Pipeline，用于收集大量训练数据，帮助模型先学会理解如何使用用户个人concept的范式&lt;/p&gt;
&lt;p&gt;paper的贡献总结为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出RAP-MLLM框架，支持用户在不需要训练的情况下让模型适应新的concept&lt;/li&gt;
&lt;li&gt;开发了一个训练数据收集pipeline，支持训练个性化assistant&lt;/li&gt;
&lt;li&gt;最终模型在image captioning and question answering等个性化任务中表现优异&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;h4 id=&#34;rap-framework&#34;&gt;RAP Framework
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251218223432497.png&#34;
	width=&#34;1205&#34;
	height=&#34;562&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251218223432497_hu_61d054f981efd32d.png 480w, https://example.com/p/personalization-survey/assets/image-20251218223432497_hu_18772be850246792.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;framework&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;214&#34;
		data-flex-basis=&#34;514px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建立一个数据库，记录每个concept的一个头像、描述，使用视觉特征作为检索键
&lt;ul&gt;
&lt;li&gt;视觉特征通过一个现有的已经过训练的Encoder&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;检索
&lt;ul&gt;
&lt;li&gt;对于用户的图片输入
&lt;ul&gt;
&lt;li&gt;基于YOLO系列做detector，根据设定好的参数识别出ROI区域&lt;/li&gt;
&lt;li&gt;将ROI区域送入Encoder，得到视觉特征，进行检索&lt;/li&gt;
&lt;li&gt;计算欧拉距离最近的top-k组concept&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;对于用户文本输入
&lt;ul&gt;
&lt;li&gt;通过文本中的name，检索是否存在于database中，进行提取&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;生成：将多组（图片，描述）以及用户输入图片及描述送入MLM进行回答&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;personalization-dataset&#34;&gt;Personalization Dataset
&lt;/h4&gt;&lt;p&gt;如果只是这样，当时的模型也不一定总能生成准确的回复&lt;/p&gt;
&lt;p&gt;因此希望构建一个数据集，用于提升模型的生成能力&lt;/p&gt;
&lt;h5 id=&#34;视觉定位visual--grounding&#34;&gt;视觉定位Visual  grounding
&lt;/h5&gt;&lt;p&gt;基于RefCOCO（单物体目标检测数据集）、ILSVRC2015-VID（ 视频目标检测数据集）、TAO 以及 CustomConcept101、Object365 （多对象数据集）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：裁剪出物体对应的标注图像，使用Gemini1.5生成描述&lt;/li&gt;
&lt;li&gt;训练输出：对应物体的边界框&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;指令遵循instruction-following&#34;&gt;指令遵循Instruction Following
&lt;/h5&gt;&lt;p&gt;该部分包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;图像标题生成 (Image Captioning)：将目标概念的裁剪图、名称及描述注入模型输入，要求模型生成能反映这些特定概念的标题&lt;/li&gt;
&lt;li&gt;问答 (Question Answering) ：利用种子问题迭代生成多样化的对话，涵盖视觉相关问题和纯文本查询&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;trick&#34;&gt;trick
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;为了让模型对检索器返回的噪声更加鲁棒，在训练输入中加入噪声概念，但是要求模型的输出仍然不变，学会过滤信息&lt;/li&gt;
&lt;li&gt;对裁剪出的图片进行旋转、翻转、3D合成新视角……&lt;/li&gt;
&lt;li&gt;加入LLaVA模型原有的数据集LLaVA-Instruct-665k，防止灾难性遗忘&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;training-free-personalization-via-retrieval-and-reasoning&#34;&gt;Training-Free Personalization via Retrieval and Reasoning
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/html/2503.18623v1&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Training-Free Personalization via Retrieval and Reasoning on Fingerprints&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;intro-1&#34;&gt;Intro
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;现有方法都需要进行训练&lt;/li&gt;
&lt;li&gt;但是VLMs实际上接触的语义概念是足够多的，内部知识丰富&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paper同样构建了一个带有参考图像、描述的数据库&lt;/p&gt;
&lt;p&gt;使用VLM通过&lt;strong&gt;distinctive attributes&lt;/strong&gt;对描述进行enrich&lt;/p&gt;
&lt;h3 id=&#34;method-1&#34;&gt;Method
&lt;/h3&gt;&lt;h4 id=&#34;database&#34;&gt;Database
&lt;/h4&gt;&lt;p&gt;首先构建database&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输入：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;参考图像 ($I_i$)&lt;/strong&gt;：用户提供的包含特定个性化概念图片&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;概念名称 ($c_i$)&lt;/strong&gt;：用户为该概念起的名称&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;类别 ($g_i$)&lt;/strong&gt;：该概念所属的语义类别（如“毛绒玩具”或“猫”）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;中间处理：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;利用 VLM 提取&lt;strong&gt;指纹属性 ($A_i$)&lt;/strong&gt;（即能将该物体与其同类区分开的关键特征，如“粉色领结”、“Nashville 吉他标志”）和&lt;strong&gt;判别性描述 ($d_i$)&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;利用&lt;strong&gt;视觉编码器&lt;/strong&gt;和&lt;strong&gt;文本编码器&lt;/strong&gt;生成图像特征 ($f_i^V$) 和文本特征 ($f_i^T$)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;输出（存入数据库 $\mathcal{D}$）：&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;包含 ${I_i, c_i, g_i, d_i, A_i, f_i^V, f_i^T}$ 的完整条目&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;文本特征由$d_i$作为输入生成&lt;/p&gt;&lt;/blockquote&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-plain&#34; data-lang=&#34;plain&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Describe the &amp;lt; g_i &amp;gt; in the image identified by the
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;concept-identifier &amp;lt; c_i &amp;gt; and highlight what makes it
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;unique.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Your response MUST be in valid JSON format and must
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;follow EXACTLY the format below:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;{
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;general: a brief description of the object in one sentence.,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;category: category of the object,
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;distinct features: [List of distinct features that makes the
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;object unique],
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;IMPORTANT:
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;* List only the most distinguishing features that set this
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  object apart.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;* Avoid generic descriptions that apply to every object in
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  this category.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;* Do not include any extra text or commentary. Any devi-
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  ation from this format will be considered incorrect.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;假设传入一张毛绒玩具的图像，及其名字和物体类别，该阶段返回内容为：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-json&#34; data-lang=&#34;json&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;general&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;A brown and white Shiba-inu dog plush toy in a sleeping posture.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;category&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Stuffed animal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;nt&#34;&gt;&amp;#34;distinct features&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;Brown and white fur color&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;Closed eyes indicating a sleeping posture&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;Small pointed ears&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;s2&#34;&gt;&amp;#34;Soft, plush texture&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;同时保存视觉特征和文本特征向量进入database&lt;/p&gt;
&lt;h4 id=&#34;multimodal-retrieval&#34;&gt;Multimodal Retrieval
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;输入：查询图像
&lt;ul&gt;
&lt;li&gt;将图像的视觉向量，分别与数据库中的视觉特征和文本特征进行相似度计算&lt;/li&gt;
&lt;li&gt;最终的相似度分数由两者均值得到&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;输出：top-k的候选concepts&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;结合两者，避免出现视觉误导&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;attribute-focused-cot-reasoning&#34;&gt;Attribute-focused CoT reasoning
&lt;/h4&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;You&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;helpful&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;AI&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;agent&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;specializing&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;analysis&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;object&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;recognition&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Your&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;task&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;analyze&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;compare&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;three&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;provided&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;descriptions&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Below&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;):&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;Info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;general&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generic&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;about&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;Info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;general&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generic&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;about&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;n&#34;&gt;Info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;general&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generic&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;about&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;category&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ci3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;     &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;features&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;distinct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;feature&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Your&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Task&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Compare&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;each&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;answer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;following&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;question&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Which&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;subject&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;?&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;A&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;B&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;C&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;List&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;shared&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attributes&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;between&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;image&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;each&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;description&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;very&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;concisely&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;If&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;attributes&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;match&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;certain&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;option&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;generate&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Provide&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;brief&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;reasoning&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;your&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;final&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Respond&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;strictly&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;following&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;JSON&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;format&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[Matching attributes for option A]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;B&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[Matching attributes for option B]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;C&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;[Matching attributes for option C]&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;Reasoning&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;lt;Brief justification&amp;gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;  &lt;span class=&#34;s2&#34;&gt;&amp;#34;Answer&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;lt;one of A, B, C&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Any&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;deviation&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;this&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;format&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;will&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;considered&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;incorrect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Do&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;output&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;any&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;additional&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;text&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;输入：查询图像 + &lt;strong&gt;候选concepts的文本描述&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;任务：模型需要列出查询图像与每个候选概念之间&lt;strong&gt;共同拥有&lt;/strong&gt;的属性&lt;/li&gt;
&lt;li&gt;输出：预测最佳匹配概念以及共享属性列表&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这部分需要使用CoT&lt;/p&gt;
&lt;h4 id=&#34;cross-modal-attribute-verification&#34;&gt;Cross-modal Attribute Verification
&lt;/h4&gt;&lt;p&gt;这部分的主要目的是排查模型幻觉&lt;/p&gt;
&lt;p&gt;有些属性可能根本没有出现在图像中，造成了模型误判&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用文本编码器对&lt;strong&gt;每个&lt;/strong&gt;&lt;u&gt;共享的指纹属性&lt;/u&gt;进行编码&lt;/li&gt;
&lt;li&gt;依次计算与查询图像的相似度&lt;/li&gt;
&lt;li&gt;计算得到相似度均值最高的concept（代表共享属性和对应图像最匹配）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果该concept就是CoT推理得到的最优concept，那么就结束算法&lt;/p&gt;
&lt;p&gt;否则我们需要更加精确的CoT&lt;/p&gt;
&lt;h4 id=&#34;pairwise-reasoning&#34;&gt;Pairwise Reasoning
&lt;/h4&gt;&lt;p&gt;只提供了文本描述看来是不够的，我们需要结合所有信息进行昂贵的推理&lt;/p&gt;
&lt;p&gt;保证这步确实完成任务&lt;/p&gt;
&lt;p&gt;转化为多个二分类任务：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入
&lt;ul&gt;
&lt;li&gt;查询图像&lt;/li&gt;
&lt;li&gt;一个候选concept的原始图像+文本描述&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;输出
&lt;ul&gt;
&lt;li&gt;VLM 输出层中 &amp;ldquo;Yes&amp;rdquo; 和 &amp;ldquo;No&amp;rdquo; Token 的置信度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;因此全过程先通过&lt;strong&gt;检索&lt;/strong&gt;缩小范围，再通过&lt;strong&gt;CoT&lt;/strong&gt;快速筛选，最后只在&lt;strong&gt;属性验证&lt;/strong&gt;失败时才调用最重的&lt;strong&gt;配对推理&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;repic-reinforced-post-training-for-personalizing-multi-modallanguagemodels&#34;&gt;RePIC: Reinforced Post-Training for Personalizing Multi ModalLanguageModels
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2506.18369&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;intro-2&#34;&gt;Intro
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在多概念图像描述任务中，SFT模型的效果一般不行，依赖大量高质量数据&lt;/li&gt;
&lt;li&gt;同一对象的不同视觉图片可能差异较大，特别是姿势、位置、光照、背景变化&lt;/li&gt;
&lt;li&gt;整合信息时可能遗漏和不准确&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method-2&#34;&gt;Method
&lt;/h3&gt;&lt;p&gt;通过强化学习GRPO进行训练&lt;/p&gt;
&lt;p&gt;数据集采用Subject200K以及合成数据（扩散模型生成）&lt;/p&gt;
&lt;p&gt;提供了大量同一物体在不同光照、姿态下的多样化视觉特征&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251221154116222.png&#34;
	width=&#34;1680&#34;
	height=&#34;425&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251221154116222_hu_7399cdb17f748286.png 480w, https://example.com/p/personalization-survey/assets/image-20251221154116222_hu_539093feff2888ae.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;395&#34;
		data-flex-basis=&#34;948px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;oct---object-consistency-tuning&#34;&gt;OCT - Object Consistency Tuning
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;构造正负图像对
&lt;ul&gt;
&lt;li&gt;正例：图像对包含相同对象&lt;/li&gt;
&lt;li&gt;负例：图像对包含不同对象&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;模型回答：图像A是否包含图像B的同一个物体？
&lt;ul&gt;
&lt;li&gt;正例需要回答Yes&lt;/li&gt;
&lt;li&gt;负例需要回答No&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;vlt---visual-localization-tuning&#34;&gt;VLT - Visual Localization Tuning
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;模型根据指令，预测指定物体的边界框&lt;/li&gt;
&lt;li&gt;若IoU大于等于0.5，给出对应奖励&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;ict---identity-consistency-tuning&#34;&gt;ICT - Identity Consistency Tuning
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;要求模型生成一段描述，并且需要包含正确的个性化标签，如&lt;code&gt;&amp;lt;name&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;若场景中有m个，模型提到了n个，则获得&lt;code&gt;n/m&lt;/code&gt;的奖励&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;为了避免模型通过生成 &lt;code&gt;This is &amp;lt;name&amp;gt;&lt;/code&gt; 这种无意义的简短废话刷分&lt;/p&gt;
&lt;p&gt;要求模型至少输出100个token&lt;/p&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;preferthinker-reasoning-based-personalized-image-preference-assessment&#34;&gt;PreferThinker: Reasoning-based Personalized Image Preference Assessment
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2511.00609&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;PreferThinker: Reasoning-based Personalized Image Preference Assessment&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;intro-3&#34;&gt;Intro
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;任务：仅给少量“喜欢/不喜欢”参考图，判断候选图哪个更合用户口味
&lt;ul&gt;
&lt;li&gt;个性化数据极其稀缺&lt;/li&gt;
&lt;li&gt;个人的喜好往往是多维度、复杂、难以描述的，无法用一个数值描述&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method-3&#34;&gt;Method
&lt;/h3&gt;&lt;p&gt;个体偏好常涉及多维度视觉要素（如艺术风格、颜色、媒介、饱和度、细节等）&lt;/p&gt;
&lt;p&gt;远超“文本-图像对齐”或“美观度”等通用偏好所能刻画&lt;/p&gt;
&lt;p&gt;但是组成“&lt;strong&gt;复杂口味&lt;/strong&gt;”的基本组成要素，多多少少是能够列举出来的、通用的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;要么喜欢“高饱和度”，要么喜欢“低饱和度”&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;paper根据参考图像，构建用户画像&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;喜欢：……&lt;/li&gt;
&lt;li&gt;讨厌：……&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;按照画像进行评估与推理&lt;/p&gt;
&lt;h4 id=&#34;profile&#34;&gt;Profile
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;从Lexica中提取prompt中频率最高、且对生成结果影响最大的视觉元素（15个）&lt;/li&gt;
&lt;li&gt;找了 100 个受试者进行投票，让大家选出“最能代表个人偏好”的元素（缩小到5个）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结果：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;9
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Art Style（艺术风格）： 比如 Anime, Realistic, Surrealism。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Color（颜色）： 比如 Pastel, Neon, Grayscale。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Art Medium（艺术媒介）： 比如 Oil Painting, Digital Art, Sketch。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Saturation（饱和度）： 比如 Vibrant, Muted。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Detail（细节度）： 比如 Intricate, Minimalist。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;Profile将基于这五个维度进行描述，但问题的关键是如何用丰富的词汇描述准这五个维度&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;针对每一个维度，都收集了大量的细粒度描述词&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;对于颜色，不能只是红、蓝……&lt;/p&gt;
&lt;p&gt;应该包含“Blush Pink”（腮红粉）、“Electric Lime”（电光绿）等具体词汇&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251223191929206.png&#34;
	width=&#34;2361&#34;
	height=&#34;537&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251223191929206_hu_94642b68d864d90a.png 480w, https://example.com/p/personalization-survey/assets/image-20251223191929206_hu_9ef4b02375216792.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;439&#34;
		data-flex-basis=&#34;1055px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;dataset&#34;&gt;Dataset
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;模拟8w个用户&lt;/li&gt;
&lt;li&gt;随机采样五个视觉偏好要素，为每个用户分配其&lt;strong&gt;视觉偏好画像&lt;/strong&gt;与&lt;strong&gt;非偏好画像&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;真实用户可能同时具有多种偏好：为部分用户分配了&lt;strong&gt;多个偏好画像&lt;/strong&gt;（2w）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;基于profile，结合从 Lexica/COCO 等数据集里选的初始 Prompt（19w条） ，扔给 Text-to-Image进行图像生成
&lt;ul&gt;
&lt;li&gt;生成一组 &lt;strong&gt;参考图 (Reference Images)&lt;/strong&gt;：代表用户的历史喜好&lt;/li&gt;
&lt;li&gt;生成两张 &lt;strong&gt;候选图 (Candidate Images)&lt;/strong&gt;：一张符合偏好，一张符合“厌恶偏好”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251223194930780.png&#34;
	width=&#34;2199&#34;
	height=&#34;646&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251223194930780_hu_4d5eef3832252e38.png 480w, https://example.com/p/personalization-survey/assets/image-20251223194930780_hu_1b626db433b8d0fc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;340&#34;
		data-flex-basis=&#34;816px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;使用Claude3.7做&lt;strong&gt;先预测后评价&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入（相当于直接给了答案）
&lt;ul&gt;
&lt;li&gt;参考图（喜欢的 &amp;amp; 不喜欢的）&lt;/li&gt;
&lt;li&gt;两张候选图&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;输出：（需要CoT）
&lt;ul&gt;
&lt;li&gt;预测：基于参考图，把用户的画像（Art Style: xxx, Color: xxx）写出来&lt;/li&gt;
&lt;li&gt;评估：对比两张候选图，在 5 个维度上分别进行解释和打分&lt;/li&gt;
&lt;li&gt;作答：给出A和B哪个更喜欢&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对数据进行清洗：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;偏好画像预测与真实标签明显不匹配的样本&lt;/li&gt;
&lt;li&gt;结论与推理过程矛盾的样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后得到：60,000 名用户样本&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/personalization-survey/assets/image-20251223195620396.png&#34;
	width=&#34;2227&#34;
	height=&#34;478&#34;
	srcset=&#34;https://example.com/p/personalization-survey/assets/image-20251223195620396_hu_8a28a8d7a69aef5d.png 480w, https://example.com/p/personalization-survey/assets/image-20251223195620396_hu_42f38761ac763757.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;465&#34;
		data-flex-basis=&#34;1118px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;training&#34;&gt;Training……
&lt;/h4&gt;&lt;p&gt;pass&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Yo LLaVA Your Personalized Language and Vision Assistant</title>
        <link>https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/</link>
        <pubDate>Sat, 13 Dec 2025 21:20:00 +0800</pubDate>
        
        <guid>https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.alphaxiv.org/abs/2406.09400&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Yo&amp;rsquo;LLaVA: Your Personalized Language and Vision Assistant | alphaXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;h3 id=&#34;personalization&#34;&gt;Personalization
&lt;/h3&gt;&lt;p&gt;用户有时候想问的不是&lt;code&gt;我该给一只狗买什么生日礼物&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;而是&lt;code&gt;我该给我的狗买什么生日礼物&lt;/code&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;模型能够&lt;strong&gt;识别、理解并谈论用户特定的概念或对象&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;但是模型一般来说学的都是通用的内容，并不知道用户自身的setting&lt;/p&gt;
&lt;h3 id=&#34;yollava&#34;&gt;Yo&amp;rsquo;LLaVA
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;给定少量（几张）特定对象的照片，模型学会将该对象嵌入为一组&lt;strong&gt;潜在 Token（Latent Tokens）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;比单纯的文本提示更加到位&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;contribution&#34;&gt;Contribution
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;防止灾难性遗忘：Yo&amp;rsquo;LLaVA冻结了大部分的预训练参数，只针对一些特殊token&lt;/li&gt;
&lt;li&gt;捕捉细粒度视觉细节：引入了&lt;strong&gt;困难负样本挖掘（Hard Negative Mining）&lt;/strong&gt;，即使用视觉上相似但并非该对象的图片进行训练，迫使模型学习更具辨别力的特征&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;读入：五张个性化subject的图片&lt;/li&gt;
&lt;li&gt;构造对比学习：检索clip相似的若干图片和随机图片，进行recognition训练（带图）&lt;/li&gt;
&lt;li&gt;构造对话文本数据：使用LLaVA对图片进行描述，生成通用对话文本，进行纯文本训练&lt;/li&gt;
&lt;li&gt;输出：能够识别图片中的subject和直接描述subject的模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;related-work&#34;&gt;Related Work
&lt;/h2&gt;&lt;h3 id=&#34;soft-prompt-tuning&#34;&gt;Soft Prompt Tuning
&lt;/h3&gt;&lt;p&gt;Soft Prompt Tuning 并不修改预训练模型的核心权重，而是通过在&lt;strong&gt;输入端&lt;/strong&gt;引入一组&lt;strong&gt;可学习的Tokens&lt;/strong&gt;来进行优化&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;Yo&amp;rsquo;LLaVA的目标是只给若干张subjec的图片，能够做到&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过视觉识别subject
&lt;ul&gt;
&lt;li&gt;whether &lt;sks&gt; is in a photo or not&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;支持关于subject的VQA任务
&lt;ul&gt;
&lt;li&gt;ask about &lt;code&gt;&amp;lt;sks&amp;gt;&lt;/code&gt;’s location&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;缺乏图片输入的情况下，在纯文本环境中回答subject的视觉特征
&lt;ul&gt;
&lt;li&gt;ask questions about intrinsic attributes of &lt;code&gt;&amp;lt;sks&amp;gt;&lt;/code&gt; like its color, shape&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;personalizing-the-subject-as-a-learnable-prompt&#34;&gt;Personalizing the Subject as a Learnable Prompt
&lt;/h3&gt;&lt;p&gt;为了识别图片中是否存在特定的subject&lt;/p&gt;
&lt;p&gt;native的做法是：使用prompt详尽地描述subject的每一个细节，但要么很困难要么做不到&lt;/p&gt;
&lt;p&gt;需要引入Soft Prompt Tuning，对Prompt层进行学习&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;sks&amp;gt; is &amp;lt;token1&amp;gt;&amp;lt;token2&amp;gt;...&amp;lt;tokenk&amp;gt;. 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Can you recognize &amp;lt;sks&amp;gt; in this photo?
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;只需要增加几个新的token和对应输出权重&lt;/li&gt;
&lt;li&gt;不需要修改MLMs的预训练权重&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此，对于参数的变化总结为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输出层增加的参数：最后输出的分类头参数矩阵由$C\times N$扩展到$C\times (N+1)$
&lt;ul&gt;
&lt;li&gt;$C$表示上一个hidden feature的维度&lt;/li&gt;
&lt;li&gt;$N$表示词表维度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;lt;token1&amp;gt;&amp;lt;token2&amp;gt;. . . &amp;lt;tokenk&amp;gt;&lt;/code&gt;Prompt层中的可学习token，以及&lt;code&gt;&amp;lt;sks&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;定义可训练的参数:&lt;/p&gt;
$$
\theta = \left\{\text{&lt;sks&gt;, &lt;token1&gt;,...&lt;tokenk&gt;},W_{(:,N+1)}\right\}
$$&lt;p&gt;
构造大量对话文本$(I,X_q,X_a)$，分别表示图片、提问、标准回答，进行训练&lt;/p&gt;
&lt;p&gt;从而让模型学习到新的concept（注意冻结其他参数）&lt;/p&gt;
&lt;h3 id=&#34;enhancing-recognition-with-hard-negative-mining&#34;&gt;Enhancing Recognition with Hard Negative Mining
&lt;/h3&gt;&lt;p&gt;对于识别问题，比较简单的训练方式就是询问subject是否在图片中&lt;/p&gt;
&lt;p&gt;显然不能全是正例，不然会训崩&lt;/p&gt;
&lt;p&gt;paper从LAION中抽样了100张图片作为负例，混合训练&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215230809533.png&#34;
	width=&#34;1208&#34;
	height=&#34;492&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215230809533_hu_5319841205a1c781.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215230809533_hu_1eadf14d5e4847f6.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;245&#34;
		data-flex-basis=&#34;589px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;同时我们需要避免模型过度泛化&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;subject：yellow + dog&lt;/p&gt;
&lt;p&gt;model: any yellow animal&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;为了增强训练，paper采用了&lt;strong&gt;hard negative mining&lt;/strong&gt;的方法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;若subject是一只毛绒狗，则负例应该选择其他不是狗的毛绒动物&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215231211500.png&#34;
	width=&#34;700&#34;
	height=&#34;828&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215231211500_hu_2eef17c8d9139b2e.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251215231211500_hu_4df7f716972627da.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Hard Negative&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;84&#34;
		data-flex-basis=&#34;202px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因此最后训练数据图片的组成为：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100张随机抽取&lt;/li&gt;
&lt;li&gt;100张通过CLIP计算相似度得到top-m张图（m=100）&lt;/li&gt;
&lt;li&gt;正样本（5张左右）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后paper构造了30条左右的prompt，以不同方式询问与回答：&lt;code&gt;&amp;lt;sks&amp;gt;&lt;/code&gt;在图里吗？&lt;/p&gt;
&lt;h3 id=&#34;learning-to-engage-in-natural-conversations-about-the-subject&#34;&gt;Learning to Engage in Natural Conversations about the Subject
&lt;/h3&gt;&lt;p&gt;通过上述算法，模型掌握了识别的能力，但未必掌握了如何描述subject：&lt;code&gt;Describe &amp;lt;sks&amp;gt; in detail.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;因此需要进一步训练，使得模型能够在对话中熟悉掌握subject的本质特性&lt;/p&gt;
&lt;p&gt;paper定义了10份通用对话模板，分为&lt;strong&gt;人类&lt;/strong&gt;、&lt;strong&gt;物体&lt;/strong&gt;两种（方便套用任何需要进行个性化训练的subject）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;避免过于细节的问题，例如：subject的尾巴是什么颜色的？&lt;/p&gt;
&lt;p&gt;显然不是所有物体都有这个特征&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对于每一张图像，使用LLaVA针对问题生成回答，完成数据集构建&lt;/p&gt;
&lt;p&gt;为了让模型真实地记住subject的特征，避免模型通过图像泄露得到答案&lt;/p&gt;
&lt;p&gt;这一阶段训练是&lt;strong&gt;纯文本&lt;/strong&gt;的，只使用问题-答案对进行训练&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment
&lt;/h2&gt;&lt;h3 id=&#34;train&#34;&gt;Train
&lt;/h3&gt;&lt;p&gt;对于单个subject，使用5张图像作为输入，软提示的视角token数量设定为16&lt;/p&gt;
&lt;p&gt;使用LLaVA-1.5-13B，进行单轮对话训练，GPU选择单卡A6000&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset
&lt;/h3&gt;&lt;p&gt;数据集的初始构建由40个subject构成：&lt;code&gt;Person (10), Pets (5), Landmarks (5), Objects (15), and Fiction Characters (5).&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;每个subject含有10-20张图片，切分为训练集和测试集&lt;/p&gt;
&lt;h3 id=&#34;baselines&#34;&gt;Baselines
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Naive LLaVA&lt;/li&gt;
&lt;li&gt;LLaVA + Prompt：通过提示词提供subject信息给LLaVA&lt;/li&gt;
&lt;li&gt;GPT-4V
&lt;ul&gt;
&lt;li&gt;+Prompt&lt;/li&gt;
&lt;li&gt;+Images（GPT-4V支持多轮图像对话，因此可以提供多个subject的图片，单张图片1k token）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;图片信息显著比文本提示更加丰富，因此GPT-4V+Images应当是性能的理论上界（提供了充足的描述）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;文本提示的获得方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;人工（约 16 token）&lt;/li&gt;
&lt;li&gt;模型生成&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了方便对比，准备了两组文本提示词：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;拼接人工+模型生成（约1.3k的token）&lt;/li&gt;
&lt;li&gt;模型再总结（约16 token）&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;results&#34;&gt;Results
&lt;/h3&gt;&lt;h4 id=&#34;recognition-ability&#34;&gt;Recognition Ability
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;40 个主体，每个主体有 5 到 10 张包含该对应主体的test图像&lt;/li&gt;
&lt;li&gt;对于每个主体，其所有的测试图像均作为&lt;strong&gt;正样本&lt;/strong&gt;，而来自其余 39 个类别的测试图像则作为&lt;strong&gt;负样本&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;总共有 333 个正向测试样本和 13,320 个负向测试样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验提供图像+问题&lt;code&gt;Can you see if &amp;lt;sks&amp;gt; is in this photo? Answer with a single word or phrase.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091440247.png&#34;
	width=&#34;2038&#34;
	height=&#34;974&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091440247_hu_2bcbcc9cf4482e3d.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091440247_hu_5bf7089a10cc7438.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;results&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;502px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Weighted是正负样本准确率的均值&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;过长的描述会对性能产生负面影响（即使用 1.3k 个 token 仅达到 0.650），可能描述了多余内容&lt;/li&gt;
&lt;li&gt;给出的参考图像越多，性能越好&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091828473.png&#34;
	width=&#34;765&#34;
	height=&#34;530&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091828473_hu_7f4e43661a201c7f.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217091828473_hu_ed1a2e03807f7567.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;question-answering&#34;&gt;Question Answering
&lt;/h4&gt;&lt;p&gt;准备了多道A或者B的选择题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;171道视觉（基于图片提问）&lt;/li&gt;
&lt;li&gt;400纯文本&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;comparison-with-myvlm&#34;&gt;Comparison with MyVLM
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217092656075.png&#34;
	width=&#34;2051&#34;
	height=&#34;414&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217092656075_hu_faca1316d356e695.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217092656075_hu_eccb22db9d4a8924.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;495&#34;
		data-flex-basis=&#34;1188px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;ablation-studies&#34;&gt;Ablation Studies
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217093115393.png&#34;
	width=&#34;2146&#34;
	height=&#34;869&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217093115393_hu_f0cefe64659a01c9.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217093115393_hu_25314fe91eb4172e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20251217093115393&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;592px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;对Prompt的可学习token长度的消融：越长越好，为了均衡性能，选择16&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对subject传入的图像数的消融：越多越好，选择5&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据集构建消融&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;纯LLaVA：
&lt;ul&gt;
&lt;li&gt;识别能力随机&lt;/li&gt;
&lt;li&gt;描述能力完全不行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;构造识别数据：&lt;code&gt;&amp;lt;sks&amp;gt;&lt;/code&gt;是否在图中？
&lt;ul&gt;
&lt;li&gt;识别能力提升&lt;/li&gt;
&lt;li&gt;描述能力完全不行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;构造文本对话数据
&lt;ul&gt;
&lt;li&gt;识别能力略微提升&lt;/li&gt;
&lt;li&gt;描述能力具备&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;构造Hard Negative样本
&lt;ul&gt;
&lt;li&gt;识别能力显著提升&lt;/li&gt;
&lt;li&gt;描述能力具备&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217154719910.png&#34;
	width=&#34;1610&#34;
	height=&#34;344&#34;
	srcset=&#34;https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217154719910_hu_cc43ccd0a36ec226.png 480w, https://example.com/p/yo-llava-your-personalized-language-and-vision-assistant/assets/image-20251217154719910_hu_baaee4f1f0248f3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;数据集消融&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;468&#34;
		data-flex-basis=&#34;1123px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>LLaDA-Rec - Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation</title>
        <link>https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/</link>
        <pubDate>Fri, 21 Nov 2025 15:36:00 +0800</pubDate>
        
        <guid>https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;Generative recommendation基于AR LLM，面临两个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;unidirectional constraint：无法在建模item语义时，捕获全局依赖&lt;/li&gt;
&lt;li&gt;error accumulation：错误累积&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;补充一些前置知识：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RQ-VAE（Residual Quantized Variational Autoencoder）
&lt;ul&gt;
&lt;li&gt;需要把Item表示成一个离散的token序列（Semantic ID），使用类LLM的生成方式进行推荐&lt;/li&gt;
&lt;li&gt;RQ-VAE把Item的embedding压缩成离散token
&lt;ul&gt;
&lt;li&gt;RA-VAE是多层级的，每一层会拟合一点，最后输出一个token序列&lt;/li&gt;
&lt;li&gt;通过控制层数，控制token序列长度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;将生成式推荐运用到dLMs上也面临若干问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mismatch between Residual Quantization (RQ) and Discrete Diffusion
&lt;ul&gt;
&lt;li&gt;多层级的方案与dlms的并行不是很契合，且dlms序列中所有token同等重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Beam Search is Not Directly Applicable to Discrete Diffusion
&lt;ul&gt;
&lt;li&gt;Beam Search比较适合自回归的top k，但是固定从左到右&lt;/li&gt;
&lt;li&gt;dlms是双向的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Differences between Language Modeling and Recommendation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Contribution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Parallel Tokenization&lt;/strong&gt;：设计了多头VQ-VAE，将物品切分为多个子向量，每个向量并行查找独立的codebook，最终生成ID&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discrete Diffusion Training&lt;/strong&gt;：使用两种mask机制
&lt;ul&gt;
&lt;li&gt;User-History Mask&lt;/li&gt;
&lt;li&gt;Next-Item Mask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discrete Diffusion Inference&lt;/strong&gt;：适配了Beam Search&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminaries&#34;&gt;Preliminaries
&lt;/h2&gt;&lt;p&gt;推荐任务的问题定义：&lt;/p&gt;
$$
\mathcal{H} = \left [ i_1, i_2, ..., i_{n-1}\right] 
$$&lt;ul&gt;
&lt;li&gt;基于用户历史的物品信息，预测下一个可能的物品$i_n$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;生成式推荐将单个物品表示为&lt;strong&gt;定长&lt;/strong&gt;的若干个token&lt;/p&gt;
$$
\mathcal{S_H} = \left[c_{1,1},...,c_{1,M},...,c_{n-1,1},...,c_{n-1,M}\right]
$$&lt;p&gt;
我们需要找到一个最佳的模型$\theta$，使得：&lt;/p&gt;
$$
\theta^* = \arg \max_\theta  P_\theta(s_n\mid \mathcal{S_H})
$$&lt;p&gt;
这个转化为AR LLM的建模还是非常方便的&lt;/p&gt;
&lt;p&gt;对于dLM：&lt;/p&gt;
&lt;p&gt;$$
P_\theta(s_n\mid \mathcal{S_H}) = \prod_{t=1}^T\prod_{m=1}^M\begin{cases}
P_\theta(c_{n,m}\mid s_n^t,\mathcal{S_H}) &amp;amp; if \space [MASK]\
1 &amp;amp; otherwise&lt;/p&gt;
&lt;p&gt;\end{cases}
$$&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/assets/image-20251127231720077.png&#34;
	width=&#34;1360&#34;
	height=&#34;636&#34;
	srcset=&#34;https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/assets/image-20251127231720077_hu_d91e9c45c767b211.png 480w, https://example.com/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/assets/image-20251127231720077_hu_ed7572c2e81d9356.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Method&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;213&#34;
		data-flex-basis=&#34;513px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;parallel-tokenization-via-multi-head-vq-vae&#34;&gt;Parallel Tokenization via Multi-Head VQ-VAE
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;希望多个 token 之间是“完全平等”的，不应该存在 RQ-VAE 那种“前面的 token 更重要&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Embedding通过Bert、Sentence-T5等得到$v_i$&lt;/li&gt;
&lt;li&gt;通过Encoder（MLP实现），得到潜在空间$z$&lt;/li&gt;
&lt;li&gt;将向量切成多个子向量，每个向量送入不同的Head&lt;/li&gt;
&lt;li&gt;每个子向量查codebook，并行得到token(code index)&lt;/li&gt;
&lt;li&gt;将code index对应的向量（code embeddings），进行拼接&lt;/li&gt;
&lt;li&gt;Decoder重建$\hat v_i$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整个Tokenizer的损失由两部分组成：&lt;/p&gt;
$$
L_{Recon} = \left \| v_i-\hat v_i \right \|^2_2
$$&lt;p&gt;
（L2范数的平方）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Encoder：学习如何编码到合适的latent space&lt;/li&gt;
&lt;li&gt;Codebook：学习到如何覆盖latent space&lt;/li&gt;
&lt;li&gt;Decoder：重建&lt;/li&gt;
&lt;/ul&gt;
$$
L_{VQ} = \sum_{m=1}^M \left (  \left\| sg[z_{i,m}] - e_{c_i,m}\right\|_2^2 + \alpha \left\|z_{i,m}-sg[e_{c_{i,m}}]\right\|^2_2 \right)
$$&lt;ul&gt;
&lt;li&gt;对于第一项
&lt;ul&gt;
&lt;li&gt;sg代表阻止接受梯度，只有$e$会接受梯度&lt;/li&gt;
&lt;li&gt;这样只更新codebook，使得codebook更接近latent space向量$z$（请靠近encoder的输出）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;对于第二项
&lt;ul&gt;
&lt;li&gt;sg阻止codebook的梯度&lt;/li&gt;
&lt;li&gt;更新encoder，贴近codebook&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最终的损失：&lt;/p&gt;
$$
\mathcal{L_{\text{VQ-VAE}}} = \mathcal{L_\text{Recon}} + \mathcal{L_\text{VQ}}
$$&lt;h3 id=&#34;discrete-diffusion-training&#34;&gt;Discrete Diffusion Training
&lt;/h3&gt;&lt;h4 id=&#34;user-history-level-masking&#34;&gt;User-History Level Masking
&lt;/h4&gt;&lt;p&gt;参考LLaDA的预训练&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;让模型学会用户序列内部的关系&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;next-item-level-masking&#34;&gt;Next-Item Level Masking
&lt;/h4&gt;&lt;p&gt;参考LLaDA的SFT&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理解同一 item 的 token 内部结构&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;但是LLaDA-Rec的训练没有分成两个阶段&lt;/p&gt;
&lt;p&gt;提出了一个联合损失函数：&lt;/p&gt;
$$
L_{total} = L_{Item-Mask} + \lambda_{His-Mask}L_{His-mask} + \lambda_{Reg}\left\|\theta\right\|^2
$$&lt;h3 id=&#34;discrete-diffusion-inference&#34;&gt;Discrete Diffusion Inference
&lt;/h3&gt;&lt;p&gt;直接使用模型不太行，没法做到生成前k个推荐项目&lt;/p&gt;
&lt;p&gt;所以如何把Beam-Search嵌入到dlms中&lt;/p&gt;
&lt;p&gt;一开始我们会有一个全部都是&lt;code&gt;[MASK]&lt;/code&gt;的序列，长度为$M$​：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;[MASK] [MASK] [MASK] ... [MASK] [MASK] 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;dlm可以预测所有位置的生成token的置信度&lt;/p&gt;
&lt;p&gt;假设我们要迭代$T$次，因此$K = M/T$&lt;/p&gt;
&lt;p&gt;每次依照置信度选择前$K$个token位置，其他位置remask&lt;/p&gt;
&lt;p&gt;根据这些位置，以及超参数$B$，迭代出$B$条置信度最高的路径&lt;/p&gt;
&lt;p&gt;做beam search&lt;/p&gt;
&lt;p&gt;不断迭代&lt;/p&gt;
</description>
        </item>
        <item>
        <title>TiDAR - Think in Diffusion, Talk in Autoregression</title>
        <link>https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/</link>
        <pubDate>Fri, 14 Nov 2025 16:11:00 +0800</pubDate>
        
        <guid>https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;h3 id=&#34;memory-bound&#34;&gt;memory-bound
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;AR模型的Latency：每次predict next token，模型权重、KV Cache需要从显存中加载到GPU核心
&lt;ul&gt;
&lt;li&gt;瓶颈是&lt;strong&gt;显存带宽&lt;/strong&gt;，GPU计算密度远远不够&lt;/li&gt;
&lt;li&gt;因此单次decode越多的token，计算密度越高（在memory-bound的限制之下）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于单次推理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AR：&lt;/li&gt;
&lt;/ul&gt;
$$
x_{t+1}=F(x_{&lt; t};x_t)
$$&lt;ul&gt;
&lt;li&gt;dLMs：&lt;/li&gt;
&lt;/ul&gt;
$$
x_{t+1},x_{t+2},...,x_{t+k+1} = F(x_{&lt; t};M_{t+1},M_{t+2},...,M_{t+k+1})
$$&lt;p&gt;&lt;strong&gt;只要没有触发到compute-bound&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;两者时间开销是一致的&lt;/p&gt;
&lt;p&gt;意味着多预测的部分$x_{t+2},&amp;hellip;,x_{t+k+1}$​是&lt;strong&gt;免费&lt;/strong&gt;的，没有额外的时间开销&lt;/p&gt;
&lt;p&gt;论文将这部分多预测出来的部分（在不增加开销的情况下），命名为&lt;strong&gt;Free Token Slots&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;什么是slots？&lt;/p&gt;
&lt;p&gt;模型的输出是多个token，&lt;strong&gt;其中需要计算kv的部分认为是slots&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;剩余已经计算过kv的token，以下实验证明了影响不大&lt;/p&gt;
&lt;p&gt;因此paper主要聚焦于slots&lt;/p&gt;&lt;/blockquote&gt;
&lt;hr&gt;
&lt;p&gt;这里做了一个基于qwen3-32B的实验（Flash Attention 2、H100、batch size=1）&lt;/p&gt;
&lt;p&gt;三条线分别是prefix=1024、2048、4096（&lt;strong&gt;kv cache提前计算好&lt;/strong&gt;）&lt;/p&gt;
&lt;p&gt;输入：&lt;code&gt;token1 token2 ... token1023 token1024 [slots1] [slots2] ... [slotsn]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;输出：&lt;code&gt;logit1 logit2 ... logit1023 logit1024 logit1 logit2 ... logitn &lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;因此实际影响单次forward的是输入的slots的数量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251116143635413.png&#34;
	width=&#34;1240&#34;
	height=&#34;859&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251116143635413_hu_828a060f8e157568.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251116143635413_hu_727a2630d7cf1aa2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Free Token Slots&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Free Token Slots&lt;/strong&gt;：该区间（1 - 100）延迟基本不增加&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cheap Token Slots&lt;/strong&gt;：该区间延迟增加的不多，比较cheap&lt;/li&gt;
&lt;li&gt;蓝色区间：触发了compute-bound&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;independence&#34;&gt;independence
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;AR是条件概率，按照从左到右依次生成&lt;/li&gt;
&lt;li&gt;dLMs从加噪序列中decode所有的token
&lt;ul&gt;
&lt;li&gt;我们通过策略选择采样$k$个token进行保留，其他进行remask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;而被采样的token应该是多个边缘分布的乘积&lt;/p&gt;
$$
\prod_i p_\theta^i(x^i\mid x_t)
$$&lt;p&gt;
这些token都是基于解码前的加噪序列得到的条件概率生成&lt;/p&gt;
&lt;p&gt;彼此可以看成独立的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这里不太认可。生成的时候并不是模型独立生成每一个token，而是同时生成&lt;/p&gt;
&lt;p&gt;你要说完全独立没关系我觉得是不对的&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;省流，从概率建模的角度解释dLMs的生成质量与$k$高度相关
&lt;ul&gt;
&lt;li&gt;$k=1$时，从左到右（因果掩码），退化成AR LLM&lt;/li&gt;
&lt;li&gt;$k$越小，质量越高&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;real-intro&#34;&gt;real-intro
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Challenge&lt;/strong&gt;：生成模型的&lt;strong&gt;质量-并行&lt;/strong&gt;问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AR LLM：生成质量高，但是苦于memory-bound无法提升性能&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;dLMs：由于token间的独立性假设，生成质量受限于并发量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Contribution&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出TiDAR架构，利用Free-token-slots，并行完成&lt;strong&gt;基于扩散的草稿&lt;/strong&gt;和&lt;strong&gt;基于自回归的采样&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;提供完整训练方案，进行全面评估，证明架构的优势&lt;/li&gt;
&lt;li&gt;进行详细的消融实验，验证核心设计。同时从扩散模型、投机采样角度进行分析&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;修改一下论文顺序，先写一下怎么推理&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为了利用&lt;code&gt;free-token-slots&lt;/code&gt;，需要Diffusion和AR同时在一次forward中同时出现&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;fully-parallelizable-self-speculative-generation&#34;&gt;Fully Parallelizable Self-Speculative Generation
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117162442502.png&#34;
	width=&#34;2031&#34;
	height=&#34;873&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117162442502_hu_e5b07fb0e1bbecb8.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117162442502_hu_c78639f221d329ce.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;232&#34;
		data-flex-basis=&#34;558px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;整个过程类似投机采样，整体的思想如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先由Diffusion Model生成一个&lt;strong&gt;draft&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;由AR进行&lt;strong&gt;rejection sampling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;如果和AR的prediction一致，则保留&lt;/li&gt;
&lt;li&gt;否则直接丢弃&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;In the each subsequent decoding step, draft tokens from the last step are rejectively sampled by checking &lt;strong&gt;whether they match the prediction&lt;/strong&gt; from the autoregressive joint distribution computed at current step using causal attention.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;概述整个算法（&lt;strong&gt;定义block_len为3（可调整）&lt;/strong&gt;），即我们会以三个token三个token为一组进行讨论&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Step0：Draft初始化
&lt;ul&gt;
&lt;li&gt;输入Prefix Tokens（实质上就是Prompt）和block size（这里是3）个MASK&lt;/li&gt;
&lt;li&gt;为了兼容不同长度，同样调整了Attention Mask的顺序
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;(block_size + max_seq_len,block_size + max_seq_len)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;直接按照真实长度切出来&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118223504302.png&#34;
	width=&#34;761&#34;
	height=&#34;551&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118223504302_hu_3c3a6f2560ea2832.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118223504302_hu_72e15dca633e49ef.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Prefill&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;331px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step1: 输入&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prefix Token&lt;/strong&gt;：&lt;code&gt;ABC&lt;/code&gt;，表示为之前已有（被成功采样）的token序列，提前计算好KV-Cache&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diffusion Draft&lt;/strong&gt;：由Diffusion生成的草稿&lt;code&gt;DEF&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Step2: 并行处理两件事&lt;/strong&gt;（实质上是一个Transformer模型的&lt;strong&gt;单次前向推理&lt;/strong&gt;）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AR部分：基于Causal Attention，输出&lt;strong&gt;Diffusion Draft&lt;/strong&gt;部分对应的logits（&lt;strong&gt;带有shift&lt;/strong&gt;）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DLMs部分：假设&lt;strong&gt;所有拒绝采样的结果&lt;/strong&gt;，提前准备好对应的&lt;code&gt;[MASK]&lt;/code&gt;让DLM独立预测（生成下一轮使用的草稿）&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212424712.png&#34;
	width=&#34;698&#34;
	height=&#34;297&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212424712_hu_868041ff93ce9f34.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212424712_hu_6bed231ec807f8b1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;AR Shift&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;564px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;因此我们本质上把&lt;code&gt;ABC DEF [M][M][M] [M][M][M] [M][M][M]&lt;/code&gt;作为输入&lt;/p&gt;
&lt;p&gt;送入到TiDAR中，做了一次前向传播&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;这里&lt;strong&gt;调整了Prefix的顺序&lt;/strong&gt;，方便&lt;strong&gt;复用Attention Mask矩阵&lt;/strong&gt;，&lt;strong&gt;新采样的token直接加到最后&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;也就是&lt;code&gt;DEF [M][M][M] [M][M][M] [M][M][M] ABC...&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;注意这里只是Attention Mask的顺序，并不是输入序列&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212940737.png&#34;
	width=&#34;1186&#34;
	height=&#34;687&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212940737_hu_50c5c79ca3fde991.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251117212940737_hu_a36ef3f0139784d5.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Decode&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;172&#34;
		data-flex-basis=&#34;414px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;对于第一组&lt;code&gt;[M][M][M]&lt;/code&gt;：Attention能看见的是&lt;code&gt;ABC D [M][M][M]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于第二组&lt;code&gt;[M][M][M]&lt;/code&gt;：Attention能看见的是&lt;code&gt;ABC DE [M][M][M]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;对于第三组&lt;code&gt;[M][M][M]&lt;/code&gt;：Attention能看见的是&lt;code&gt;ABC DEF [M][M][M]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Step3:  Sampling&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;根据AR部分的logits输出，决定上一轮草稿保留的内容（例子是保留DE，舍去F）&lt;/li&gt;
&lt;li&gt;因此Prefix Token和KV-Cache会将&lt;code&gt;DE&lt;/code&gt;加入，得到&lt;code&gt;ABC DE&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;下一轮的草稿为第二组&lt;code&gt;[M][M][M]&lt;/code&gt;的解码结果：&lt;code&gt;F&amp;quot;G&amp;quot;H&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Step4：输出&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Prefix Token&lt;/strong&gt;：&lt;code&gt;ABC DE&lt;/code&gt;及其KV-Cache&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Diffusion Draft&lt;/strong&gt;：由Diffusion生成的草稿&lt;code&gt;F&amp;quot;G&amp;quot;H&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一些思考：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;显然&lt;code&gt;block_len&lt;/code&gt;是不能无限大的，结合Figure1，通过单个Transformer架构的slots需要在&lt;code&gt;Free-Token-Slots&lt;/code&gt;的范围内&lt;/li&gt;
&lt;li&gt;paper只为AR采样1、2、3个token做了准备草稿，&lt;strong&gt;默认接受至少一个&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;diffusion-ar-dual-mode-backbone-training&#34;&gt;Diffusion-AR Dual-mode Backbone Training
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;论文其实是先讲的这一部分，但是先看完推理后再回来看train比较好&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;前期工作Block Diffusion提出了一种&lt;strong&gt;块内双向注意力，块间因果掩码&lt;/strong&gt;的方法&lt;/p&gt;
&lt;p&gt;TiDAR进行了修改：&lt;strong&gt;保留最后一个块（双向注意力），其他内容（或者叫前缀）全部因果掩码&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;带来如下好处&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;允许我们像AR一样计算链式的联合概率分布$p(x_i\mid x_{&amp;lt;i})$，方便进行拒绝采样，保证高质量，并且计算似然和AR一样高效&lt;/li&gt;
&lt;li&gt;在预训练和微调过程中可以计算前缀部分的NTP损失，&lt;strong&gt;损失信号的密度更高&lt;/strong&gt;，充分利用数据中每一个token&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;AR部分：shifted by one position&lt;/li&gt;
&lt;li&gt;dLM部分：一一对齐&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDAR对于扩散部分的token，全部掩码为&lt;code&gt;[MASK]&lt;/code&gt;，直接消除选择哪一种掩码策略的思考&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提高了扩散损失的密度（每一个token都参与）&lt;/li&gt;
&lt;li&gt;平衡了AR和扩散的损失：&lt;strong&gt;强制两者参与损失计算的token数量都是相等的（序列长度）&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;先前框架的不平衡：dLM的信号容易被AR淹没
&lt;ul&gt;
&lt;li&gt;AR：稠密的（len-1个token参与）&lt;/li&gt;
&lt;li&gt;dLMs：损失取决于多少token被掩码（远少于len）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;平衡的好处：更容易通过一个简单的超参数（加权因子）进行控制&lt;/li&gt;
&lt;li&gt;允许在推理时&lt;strong&gt;一步扩散&lt;/strong&gt;，避免多步迭代&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118015735020.png&#34;
	width=&#34;740&#34;
	height=&#34;747&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118015735020_hu_419e28ffa044b0e3.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118015735020_hu_bf8733a3f8d1fc3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Train Mask&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;99&#34;
		data-flex-basis=&#34;237px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于输入序列，扩充相同的长度掩码&lt;code&gt;ABC DEF&lt;/code&gt; -&amp;gt; &lt;code&gt;MMM MMM&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;前一半序列的注意力就是Causal Attention&lt;/li&gt;
&lt;li&gt;对于Diffusion部分（后一半），按照block size逐块考虑（采用双向注意力）
&lt;ul&gt;
&lt;li&gt;第一组&lt;code&gt;MMM&lt;/code&gt;恢复目标是&lt;code&gt;ABC&lt;/code&gt;，前缀为空&lt;/li&gt;
&lt;li&gt;第二组&lt;code&gt;MMM&lt;/code&gt;恢复目标是&lt;code&gt;DEF&lt;/code&gt;，前缀为&lt;code&gt;ABC&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;若后续还有别的组，前缀会继续累积&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TiDAR的建模目标可以表示为：&lt;/p&gt;
$$
\mathcal{L}_{TiDAR}(\theta) = \frac{1}{\alpha+1}\left( \sum_{i=1}^{S-1}\frac{\alpha}{S-1}\cdot\mathcal{L}_{AR}(x_i,x_{i+1};\theta)+ \sum_{i=1}^{S-1}\frac{1}{S-1}\cdot \mathcal{L}_{Diff}(\left[mask\right],x_i;\theta)\right )
$$&lt;ul&gt;
&lt;li&gt;$\alpha \in[0,1]$，损失函数的平衡项（paper里设为1）&lt;/li&gt;
&lt;li&gt;$\left{x_i\right}_S$是输入序列&lt;/li&gt;
&lt;li&gt;AR和Diffusion都是做对应的交叉熵&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment
&lt;/h2&gt;&lt;p&gt;（这里笔记忘记保存了，补一些重点）&lt;/p&gt;
&lt;p&gt;由于AR部分的logits是带label shift的，Diffusion部分没有&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118224652148.png&#34;
	width=&#34;1063&#34;
	height=&#34;332&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118224652148_hu_d600df02d3f6d42f.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118224652148_hu_e1f37bc07406f7ce.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;320&#34;
		data-flex-basis=&#34;768px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;实质上是一个模型，没有切割&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;自回归部分根据ABCD会进行一个&lt;code&gt;E&lt;/code&gt;的预测&lt;/p&gt;
&lt;p&gt;Diffusion部分的第一个Mask，也会根据attention得到相同的信息，预测&lt;code&gt;E&#39;&#39;&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;所以这里涉及到了相信自回归还是相信Diffusion、或是兼顾的思考&lt;/p&gt;
&lt;p&gt;作者顺手做了一个实验&lt;/p&gt;
$$
\text{logits}_{\text{mix}} = \beta*\text{logits}_i^{\text{AR}} + (1-\beta)*\text{logits}_i^{\text{Diff}},i\in|\text{Vocab}|
$$&lt;p&gt;
通过$\beta$对齐了两个logits&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118225438803.png&#34;
	width=&#34;788&#34;
	height=&#34;378&#34;
	srcset=&#34;https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118225438803_hu_fe1528daa70accea.png 480w, https://example.com/p/tidar-think-in-diffusion-talk-in-autoregression/assets/image-20251118225438803_hu_21e4be9c18baf35a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;500px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过$\alpha$控制loss比例，横轴是$\beta$&lt;/li&gt;
&lt;li&gt;总体没有明显性能差距，&lt;strong&gt;因此质量来源是拒绝采样，而非AR比DLM好&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitations&#34;&gt;Limitations
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;Batch_Size：paper只做了1的情况（毕竟贴近推理，而不是吞吐量测试）&lt;/li&gt;
&lt;li&gt;长上下文：翻了一倍文本，压力比较大&lt;/li&gt;
&lt;li&gt;free token slots的探索：需要更加系统化的视角
&lt;ul&gt;
&lt;li&gt;cuda&lt;/li&gt;
&lt;li&gt;调度&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Awesome LLaDA</title>
        <link>https://example.com/p/awesome-llada/</link>
        <pubDate>Thu, 13 Nov 2025 12:56:00 +0800</pubDate>
        
        <guid>https://example.com/p/awesome-llada/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;content&#34;&gt;Content
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2511.06254&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2511.06254 LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：生成式推荐与离散扩散模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2510.10481&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2510.10481 UltraLLaDA: Scaling the Context Length to 128K for Diffusion Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：扩充LLaDA的长上下文&lt;/li&gt;
&lt;li&gt;ICLR 2026：6666&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2509.24389&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2509.24389 LLaDA-MoE: A Sparse MoE Diffusion Language Model&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/llada-moe-asparse-moediffusion-language-model/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：修改了LLaDA的Dense Transformer为MoE架构&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2509.13866&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2509.13866 Masked Diffusion Models as Energy Minimization&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何以理论可解释的方式为离散域的掩码扩散模型（MDM）设计最优采样调度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.19223&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2505.19223 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/diffusion-language-model-birdresearch-202510/#llada-15-variance-reduced-preference-optimization-for-large-language-diffusion-models&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记（难度较高，未读完）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：提出VRPO方法，对LLaDA进行强化学习&lt;/li&gt;
&lt;li&gt;ICLR 2026：642&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.16933&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2505.16933 LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/diffusion-language-model-birdresearch-202510/#llada-v-large-language-diffusion-models-with-visual-instruction-tuning&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：实现了图片理解的多模态LLaDA模型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2502.09992 Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;note&lt;/strong&gt;：&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/diffusion-language-model-%e8%ae%ba%e6%96%87%e7%ac%94%e8%ae%b0%e4%b8%80/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;笔记&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;abstract&lt;/strong&gt;：LLaDA&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>LLaDA-MoE ASparse MoEDiffusion Language Model</title>
        <link>https://example.com/p/llada-moe-asparse-moediffusion-language-model/</link>
        <pubDate>Tue, 11 Nov 2025 13:54:00 +0800</pubDate>
        
        <guid>https://example.com/p/llada-moe-asparse-moediffusion-language-model/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112212631894.png&#34;
	width=&#34;1088&#34;
	height=&#34;573&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112212631894_hu_138266dd69e252e0.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112212631894_hu_2d18d20e2a5afd97.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;189&#34;
		data-flex-basis=&#34;455px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA-MoE：激活1.4B参数的情况下，超过先前8B的DLMs性能，取得DLMs的SOTA，与Qwen2.5-3B-Instruct性能相当&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-moe&#34;&gt;LLaDA-MoE
&lt;/h2&gt;&lt;h3 id=&#34;architecture&#34;&gt;Architecture
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112214355137.png&#34;
	width=&#34;1135&#34;
	height=&#34;647&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112214355137_hu_180d8637e9b2a256.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251112214355137_hu_d25e444d5191aadc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA-MoE&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;175&#34;
		data-flex-basis=&#34;421px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RMSNorm、SwiGLU、RoPE、QK-LayerNorm&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;train&#34;&gt;Train
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113103025656.png&#34;
	width=&#34;1896&#34;
	height=&#34;204&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113103025656_hu_8e34c220d489d6dd.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113103025656_hu_b33c90fba7bf100a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pipeline&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;929&#34;
		data-flex-basis=&#34;2230px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pretrain Stage 1：从头开始训练，10T&lt;/li&gt;
&lt;li&gt;Pretrain Stage 2：从相同的底层数据重新采样10T（提高数学、代码的权重），继续训练&lt;/li&gt;
&lt;li&gt;Annealing Stage 1：从Pretrain Stage 2中最好的checkpoint开始，训练500B的高质量文本&lt;/li&gt;
&lt;li&gt;Annealing Stage 2：将RoPE的base从10000提高到50000（扩充4k到8k的上下文），500B&lt;/li&gt;
&lt;li&gt;SFT&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Annealing：用更高质量的数据让模型“收敛得更好”&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;训练阶段（预训练+SFT）的损失函数同LLaDA&lt;/li&gt;
&lt;li&gt;预训练1%是随机长度，99%是4096定长（同LLaDA）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;moe-routing&#34;&gt;MoE Routing
&lt;/h4&gt;$$
p_t = \text{Softmax}(\text{Router}(h_t))\\
o_t = \sum_i p_{t,i}E_i(h_t), \quad \text{where }p_{t,i} \in\text{Topk}(p_t) 
$$&lt;ul&gt;
&lt;li&gt;$h_t$是hidden state&lt;/li&gt;
&lt;li&gt;$E$是专家网络&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MoE选取p最大的k个专家网络进行加权&lt;/p&gt;
&lt;p&gt;为了平衡负载，采用了标准的MoE auxiliary loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Load Balancing Loss&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P_i$：token级的专家$i$被分配的平均概率&lt;/li&gt;
&lt;li&gt;$f_i$：经过所有token每个专家被选中的频率&lt;/li&gt;
&lt;li&gt;$N$​：专家数&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\mathcal{L}_{LB} = N\sum_{i=1}^N f_iP_i
$$&lt;p&gt;通过该损失避免某个专家被频繁选中&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Z-Loss
&lt;ul&gt;
&lt;li&gt;$z_t$表示$\text{Router}(h_t)$&lt;/li&gt;
&lt;li&gt;$z_{t,j}$即为第$j$个专家的打分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\mathcal{L}_Z=\frac{1}{T}\sum_{i=1}^T\left(\log \sum_{j=1}^N e^{z_{t,j}}\right)^2
$$&lt;p&gt;通过该损失抑制logits分布，防止softmax极端化&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLaDA-MoE为LB设置0.01权重，为Z-Loss设定0.001&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121116877.png&#34;
	width=&#34;961&#34;
	height=&#34;287&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121116877_hu_26c4e1f6b4f9a486.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121116877_hu_724a9d33bf924a8f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;334&#34;
		data-flex-basis=&#34;803px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121209495.png&#34;
	width=&#34;860&#34;
	height=&#34;614&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121209495_hu_6189f9ed1a7edf66.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121209495_hu_eb3f3b8def23f671.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;base&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;140&#34;
		data-flex-basis=&#34;336px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121225582.png&#34;
	width=&#34;1074&#34;
	height=&#34;584&#34;
	srcset=&#34;https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121225582_hu_c121207b1204ea1a.png 480w, https://example.com/p/llada-moe-asparse-moediffusion-language-model/assets/image-20251113121225582_hu_36f1f7b89ca3b2db.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;SFT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;183&#34;
		data-flex-basis=&#34;441px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>UltraLLaDA Scaling the Context Length to 128K for Diffusion Large Language Models</title>
        <link>https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/</link>
        <pubDate>Tue, 11 Nov 2025 13:54:00 +0800</pubDate>
        
        <guid>https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;h3 id=&#34;现象&#34;&gt;现象
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;扩散语言模型的Local perception&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2506.14429&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111142128339.png&#34;
	width=&#34;1951&#34;
	height=&#34;1195&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111142128339_hu_b92255c0ca39e7db.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111142128339_hu_1c32c04fe496d8df.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;163&#34;
		data-flex-basis=&#34;391px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://aoijays.top/p/diffusion-language-model-birdresearch-202510/#long-context-phenomenology-of-diffusion-llms&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LongLLaDA · 大海捞针实验&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LongLLaDA观测到这个现象是&lt;strong&gt;RoPE+双向上下文&lt;/strong&gt;带来的&lt;/li&gt;
&lt;li&gt;提出了一种Training Free的方法，调整了RoPE机制，提升上下文能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;关键挑战&#34;&gt;关键挑战
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;如何将AR LLM的上下文扩展技术（不重新预训练）迁移到DLM&lt;/li&gt;
&lt;li&gt;training-free在AR方面证明了效果不如post-training，是否对DLM也是一致的
&lt;ul&gt;
&lt;li&gt;希望模型通过后训练，调整内部机制&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;核心贡献&#34;&gt;核心贡献
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;a Diffusion-aware NTK method
&lt;ul&gt;
&lt;li&gt;无需从头训练&lt;/li&gt;
&lt;li&gt;受神经切线核（Neural Tangent Kernel, NTK）理论启发，开发了一个适配DLM的NTK方法&lt;/li&gt;
&lt;li&gt;能适应扩散模型的迭代去噪特性，使 RoPE 可以稳定外推到 128K tokens&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;比较了后训练过程中使用的mask策略，分析对优化稳定性和长程回忆的影响&lt;/li&gt;
&lt;li&gt;UltraLLaDA，与LongLLaDA、LLaDA进行benchmark，证明是SOTA&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminary-work&#34;&gt;Preliminary Work
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;RoPE&lt;/strong&gt;：通过旋转向量的方式引入位置信息，旋转角度与位置index是线性关系&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;为了增加上下文，RoPE需要外推到更远的Token&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;外插：直接应用到更大的index，但是会造成信号失真与混乱（模型没见过这么大的）&lt;/li&gt;
&lt;li&gt;内插：等比例缩放index到小窗口内，但是会比较模糊&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
\cos(k\alpha^{\frac{-2i}{d}})\\
\cos(\frac{k}{\lambda}\alpha^{\frac{-2i}{d}})
$$&lt;p&gt;&lt;strong&gt;序列位置$k$的编码向量（维度为$d$）的第$i$个分量&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;NTK-Aware Scaling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;定义如下：&lt;/p&gt;
$$
\lambda = \left(\frac{T_{target}}{T_{train}}\right)^{\frac{d}{d-2}}
$$&lt;p&gt;
将原本的底数$\alpha$转化为$\alpha\lambda$&lt;/p&gt;
$$
\cos(k(\alpha\lambda)^{\frac{-2i}{d}}) = \cos(k\alpha^{\frac{-2i}{d}}\left(\frac{T_{target}}{T_{train}}\right)^{\frac{-2i}{d-2}})
$$&lt;ul&gt;
&lt;li&gt;低维度：$i$比较小，频率高，$\frac{-2i}{d-2}$接近1，式子近似为$\cos(k\alpha ^ {\frac{-2i}{d}})$&lt;/li&gt;
&lt;li&gt;高维度：$i$比较大，频率低，$\frac{-2i}{d-2}$接近-1，式子近似为$\cos(\frac{k}{\lambda}\alpha^{\frac{-2i}{d}})$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;省流：高频部分外插，低频部分内插&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;上面都是定性的理解&lt;/p&gt;
&lt;p&gt;正确的公式实际是：&lt;/p&gt;
$$
\lambda_{baseline} = b^{-1}\cdot\left ( \frac{T_{target}}{2\pi} \right)^{\frac{d}{d_{crit}}},  
d_{crit} = 2\left \lceil \frac{d}{2}\log_b \frac{T_{train}}{2\pi} \right \rceil
$$&lt;blockquote&gt;
&lt;p&gt;b通常是10000&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;输入：$T_{target}, T_{train}$&lt;/li&gt;
&lt;li&gt;输出：$\lambda$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;LongLLaDA将此方法从AR LLM迁移到DLM，并且不进行后训练&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;从最开始的现象已经说明，RoPE + 双向上下文是不可或缺的&lt;/p&gt;
&lt;p&gt;但是LongLLaDA没有任何关于双向上下文的适配，潜力没有被开发完全&lt;/p&gt;
&lt;h3 id=&#34;diffusion-aware-ntk-in-ultrallada&#34;&gt;Diffusion-aware NTK in UltraLLaDA
&lt;/h3&gt;&lt;p&gt;AR和DLM能看见的上下文窗口是不一样的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AR LLM：$[-T_{train}-1,0]$&lt;/li&gt;
&lt;li&gt;DLM：$[-(T_{train}-1), T_{train}-1]$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DLM的真实上下文信息应该是2倍，因此需要对NTK的输入进行修正：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：$T_{Ecap} \approx 2T_{target}, T_{cap} \approx 2T_{train}$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111183735012.png&#34;
	width=&#34;539&#34;
	height=&#34;416&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111183735012_hu_b77c22bce8c79b7c.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111183735012_hu_d91afef18895f313.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;129&#34;
		data-flex-basis=&#34;310px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;带来更小的频率，该缩放机制增加了所有维度上的RoPE周期&lt;/p&gt;
&lt;p&gt;从而有效减缓 RoPE 旋转速度，并延长所有注意力维度上的位置波长&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;case-study-of-masking-for-diffusion-llm-context-extension&#34;&gt;Case Study of Masking for Diffusion LLM Context Extension
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;后训练数据准备&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;来源：PG19&lt;/li&gt;
&lt;li&gt;处理：短文档通过拼接到达64k，长文档切割成64k的chunk&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;存在问题：&lt;strong&gt;跨文档干扰&lt;/strong&gt;，跨越文档边界进行注意力计算，错误吸收上下文信息&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;AR：由于Causal Mask，只能看见之前的文档，天然限制了部分干扰&lt;/li&gt;
&lt;li&gt;DLM：能看见所有的文档，干扰非常强&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;处理策略（idea来自AR LLM）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;baseline：直接拼接，什么都不做&lt;/li&gt;
&lt;li&gt;Adaptive Attention Masking：只计算文档内部的注意力&lt;/li&gt;
&lt;li&gt;End-of-document：文档之间插入special token（并没有显式地禁止注意力跨文档），采用Full Attention&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111185846564.png&#34;
	width=&#34;974&#34;
	height=&#34;397&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111185846564_hu_1d6deb03ba28f546.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111185846564_hu_66dfca3e11b2ed50.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;245&#34;
		data-flex-basis=&#34;588px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对三种策略都进行了训练（结合前文所提NTK）&lt;/p&gt;
&lt;p&gt;训练参数：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190447991.png&#34;
	width=&#34;945&#34;
	height=&#34;397&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190447991_hu_8499a077ed252911.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190447991_hu_2b3a3f07530dfbc5.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;571px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对特定任务采用不同策略训练的UltraLLaDA模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;后续的实验证明：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用直接拼接后训练的模型常产生不连贯结果，这可能是由于无关内容相互渗透所致&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;h3 id=&#34;train-free-ntk&#34;&gt;Train-Free NTK
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;目的：在Train-Free的情况下，修正NTK的上下文长度参数输入的作用&lt;/li&gt;
&lt;li&gt;方法：未做后训练，只修改编码方式进行测试&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184208987.png&#34;
	width=&#34;364&#34;
	height=&#34;127&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184208987_hu_f5cdab02acb4203.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184208987_hu_38ee98da8172c1fc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;286&#34;
		data-flex-basis=&#34;687px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184247848.png&#34;
	width=&#34;431&#34;
	height=&#34;278&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184247848_hu_cb0896b057c69228.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111184247848_hu_1e0ffc824f1ebd29.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;155&#34;
		data-flex-basis=&#34;372px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结论：引入&lt;strong&gt;双向覆盖&lt;/strong&gt;对于扩展DLM的上下文长度至关重要&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;niah&#34;&gt;NIAH
&lt;/h3&gt;&lt;p&gt;Needle-in-a-haystack long-context retrieval task&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;该任务将单个相关语句嵌入长达 128K 标记的干扰文本中，要求模型准确检索目标语句&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111155906820.png&#34;
	width=&#34;1846&#34;
	height=&#34;773&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111155906820_hu_89aa3fc2ffca11cc.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111155906820_hu_e24da556f251259a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;238&#34;
		data-flex-basis=&#34;573px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;全部100%检索成功&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于模型缺陷，LongLLaDA 无法进行 32K 以上的评估&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结论：后训练方法即使在极长上下文（128K）中仍能保持卓越的检索能力，而Train-Free会随上下文长度增加快速失效&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ppl&#34;&gt;PPL
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;基于PG19中128K长度的文档的语言建模困惑度评估&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190938213.png&#34;
	width=&#34;718&#34;
	height=&#34;147&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190938213_hu_a51476eaaa67873e.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111190938213_hu_311910ab80f2cabd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;488&#34;
		data-flex-basis=&#34;1172px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结论：UltraLLaDA的训练在超长序列建模中具有很强的鲁棒性&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;longbench&#34;&gt;LongBench
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;截断在16K上下文长度（大部分任务只有这么长）
&lt;ul&gt;
&lt;li&gt;单/多文档问答、摘要、上下文学习、合成推理任务、代码补全&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191347098.png&#34;
	width=&#34;730&#34;
	height=&#34;138&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191347098_hu_d4f035a04a506e20.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191347098_hu_5575a6a54efc253d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;528&#34;
		data-flex-basis=&#34;1269px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结论：长上下文训练不仅扩展了上下文长度，即使在 16K 范围内（基线模型能力范围内）也能在挑战性任务上&lt;strong&gt;实现质量增益&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;归因于后训练过程带来的长距离连贯性与理解能力的提升&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;ruler&#34;&gt;RULER
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;在4K至32K上下文长度下（涵盖检索、聚合、问答及多跳变量追踪任务）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191700298.png&#34;
	width=&#34;927&#34;
	height=&#34;382&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191700298_hu_b0e50245ee924e09.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111191700298_hu_90c35c7c37ac0654.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;242&#34;
		data-flex-basis=&#34;582px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在检索（NIAH）和追踪（VT）类别中均展现出强劲的扩展性&lt;/li&gt;
&lt;li&gt;在聚合（AGG）及部分问答任务（QA）上的提升相对有限&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;消融实验&#34;&gt;消融实验
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;针对NTK和跨文档策略进行消融实验&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192035623.png&#34;
	width=&#34;743&#34;
	height=&#34;177&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192035623_hu_b25f67a29a91e17a.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192035623_hu_498e5b060ad7574c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;419&#34;
		data-flex-basis=&#34;1007px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192105448.png&#34;
	width=&#34;922&#34;
	height=&#34;360&#34;
	srcset=&#34;https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192105448_hu_1bb19d9f27b01aaa.png 480w, https://example.com/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/assets/image-20251111192105448_hu_decefa29b95b392d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;256&#34;
		data-flex-basis=&#34;614px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;EOD 拼接策略在较短或中等长度下表现更优&lt;/li&gt;
&lt;li&gt;在更长序列中，自适应掩码策略会反超EOD拼接&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>Multimodal Diffusion Language Model · BirdResearch · 202510</title>
        <link>https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/</link>
        <pubDate>Fri, 17 Oct 2025 13:39:00 +0800</pubDate>
        
        <guid>https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;mmada-multimodal-large-diffusion-language-models&#34;&gt;MMaDA: Multimodal Large Diffusion Language Models
&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先前的多模态架构混合，不同模态需要不同组件、不同数据处理方式&lt;/li&gt;
&lt;li&gt;扩散模型后训练策略欠缺研究&lt;/li&gt;
&lt;li&gt;如何文本与视觉模态协同学习、各方面性能超过各领域现有模型&lt;/li&gt;
&lt;li&gt;如何确保模型具有泛化能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915.png&#34;
	width=&#34;1127&#34;
	height=&#34;554&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915_hu_a5cdfff39a893f5f.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021144451915_hu_a6e35a99ffe8ad82.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;compared to other llms&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;203&#34;
		data-flex-basis=&#34;488px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;核心贡献&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一Diffusion架构：消除模态专用组件，保持跨任务性能&lt;/li&gt;
&lt;li&gt;混合Long-CoT的后训练：统一CoT格式，对齐跨模态推理过程，协同训练&lt;/li&gt;
&lt;li&gt;UniGRPO：专用的强化学习方法&lt;/li&gt;
&lt;li&gt;SOTA：文本推理、多模态理解、文生图三方面均是SOTA（AR、混合、扩散）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731.png&#34;
	width=&#34;987&#34;
	height=&#34;772&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731_hu_7714d48219f97e9f.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021150351731_hu_c7254ad29497536a.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;TASKs&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;127&#34;
		data-flex-basis=&#34;306px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;h4 id=&#34;pretrain&#34;&gt;Pretrain
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Data Tokenization
&lt;ul&gt;
&lt;li&gt;文本：采用&lt;strong&gt;LLaDA&lt;/strong&gt;的tokenizer&lt;/li&gt;
&lt;li&gt;图像：采用&lt;strong&gt;Show-o&lt;/strong&gt;所使用的pretrained image quantizer
&lt;ul&gt;
&lt;li&gt;基于&lt;strong&gt;MAGVIT-v2&lt;/strong&gt;架构（一个图像离散化模型）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;MAGVIT-v2的输入与输出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：单张静态图片的像素阵列、由多帧图像组成的序列&lt;/li&gt;
&lt;li&gt;输出：一个token序列&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;论文中采用$F=16$的下采样因子&lt;/p&gt;
&lt;p&gt;对于$H\times W$的图像，转化为一维的$\frac{H\times W}{F^2}$长度序列&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一的概率建模与目标
&lt;ul&gt;
&lt;li&gt;定义MMaDA为一个Mask Token Predictor，直接预测文本与图像的&lt;code&gt;[MASK]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;仅在&lt;code&gt;[MASK]&lt;/code&gt;的图像或文本Token上做统一交叉熵损失&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
L_{unity}(\theta) = -E_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L I(x_t^i = [MASK])\log p_\theta (x_0^i|x_t)\right]
$$&lt;h4 id=&#34;post-training-with-mixed-long-cot-finetuning&#34;&gt;Post-Training with Mixed Long-CoT Finetuning
&lt;/h4&gt;&lt;p&gt;MMaDA明确面向：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;推理密集型任务（例如数学）&lt;/li&gt;
&lt;li&gt;具备World-knowledge-aware的文生图
&lt;ul&gt;
&lt;li&gt;事实一致性非常重要&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920.png&#34;
	width=&#34;1344&#34;
	height=&#34;583&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920_hu_6214c2797a50c125.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021161828920_hu_4b28f5ab45f1859e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Long-CoT Finetuning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;230&#34;
		data-flex-basis=&#34;553px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;为进行稳定的后训练，论文整理了一个包含三类核心任务（文本推理、多模态推理、文本到图像生成）CoT数据集&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;利用这篇数据，在RL之前通过SFT做冷启动&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;统一的CoT格式：消除不同任务的输出异构性&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;|&amp;lt;special_token&amp;gt;| &amp;lt;reasoning_process&amp;gt; |&amp;lt;special_token&amp;gt;| &amp;lt;result&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;blockquote&gt;
&lt;p&gt;后续证明了有益于跨模态的协同训练与对齐&lt;/p&gt;
&lt;p&gt;希望文本推理逻辑指导图像生成&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;多样性、复杂性、准确性
&lt;ul&gt;
&lt;li&gt;通过已有的LLM、VLM，合成多样化的数据&lt;/li&gt;
&lt;li&gt;使用模型过滤，只保留高质量、长形式的CoT样本&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;MMaDA进行了混合任务的CoT微调&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留提示词，对response进行加噪&lt;/li&gt;
&lt;li&gt;通过预训练得到的Predictor进行损失计算&lt;/li&gt;
&lt;/ul&gt;
$$
L_{Mixed-SFT}(\theta) = -E_{t,p_0,r_0,r_t}\left[\frac{1}{t}\sum_{i=1}^{L&#39;} I(r_t^i = [MASK])\log p_\theta (r_0^i|p_0,r_t)\right]
$$&lt;h4 id=&#34;post-training-with-unified-rl&#34;&gt;Post-Training with Unified RL
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810.png&#34;
	width=&#34;2085&#34;
	height=&#34;1055&#34;
	srcset=&#34;https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810_hu_909d17c24076e0b.png 480w, https://example.com/p/multimodal-diffusion-language-model-birdresearch-202510/assets/image-20251021163518810_hu_a2527497ddba309b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Training&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;197&#34;
		data-flex-basis=&#34;474px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自回归模型：每个Token的条件概率都非常好计算，适合RL&lt;/li&gt;
&lt;li&gt;Diffusion：过程复杂，无法直接使用传统强化学习方法
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;局部掩码依赖&lt;/strong&gt;：只有&lt;code&gt;[MASK]&lt;/code&gt;处有预测概率，其他位置已知&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掩码比例敏感&lt;/strong&gt;：训练必须兼容不同噪声程度的恢复
&lt;ul&gt;
&lt;li&gt;LLaDA采样大量样本，造成RL开销巨大&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;非自回归序列似然&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;AR模型：句子概率可以通过token概率乘积计算&lt;/li&gt;
&lt;li&gt;Diffusion：很难计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;unigrpo&#34;&gt;UniGRPO
&lt;/h5&gt;&lt;blockquote&gt;
&lt;p&gt;这部分搁置一下 后续补一下RL的知识&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;主要有三个关键点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;结构化加噪策略&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;序列对数似然近似为：被遮位置对数概率的平均&lt;/li&gt;
&lt;li&gt;用&lt;strong&gt;旧策略&lt;/strong&gt;和&lt;strong&gt;当前策略&lt;/strong&gt;的“近似序列似然”做&lt;strong&gt;比值&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;UniGPRO的奖励是多样化的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本推理奖励
&lt;ul&gt;
&lt;li&gt;答案正确奖励&lt;/li&gt;
&lt;li&gt;格式奖励（&lt;code&gt;&amp;lt;think&amp;gt;&amp;lt;think&amp;gt;&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;多模态推理奖励
&lt;ul&gt;
&lt;li&gt;同上&lt;/li&gt;
&lt;li&gt;CLIP奖励：使用原始 CLIP 分数衡量文本-图像的语义一致性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;文生图奖励
&lt;ul&gt;
&lt;li&gt;同上&lt;/li&gt;
&lt;li&gt;图像奖励：反映人类偏好得分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;文本生成：采用半自回归采样
&lt;ul&gt;
&lt;li&gt;Masking Schedule采用线性计划，与LLaDA一致&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;图像生成：采用低置信度重掩码
&lt;ul&gt;
&lt;li&gt;余弦噪声调度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments
&lt;/h3&gt;&lt;p&gt;一般的benchmark跳过&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion Language Model · BirdResearch · 202510</title>
        <link>https://example.com/p/diffusion-language-model-birdresearch-202510/</link>
        <pubDate>Wed, 01 Oct 2025 15:54:00 +0800</pubDate>
        
        <guid>https://example.com/p/diffusion-language-model-birdresearch-202510/</guid>
        <description>&lt;h2 id=&#34;ar-llm&#34;&gt;AR LLM
&lt;/h2&gt;&lt;p&gt;对于自回归模型，假设输入是：&lt;code&gt;[你, 好, ！]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;词汇表是：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;8
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;0: &amp;lt;pad&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;1: 你
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;2: 好
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;3: 我
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;4: 很
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;5: 好
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;6: ！
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;7: &amp;lt;eos&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131.png&#34;
	width=&#34;966&#34;
	height=&#34;655&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131_hu_a8cad21da2d3ad7b.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008181120131_hu_7c248c0bc453526d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20251008181120131&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;147&#34;
		data-flex-basis=&#34;353px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;dream-7b-diffusion-large-language-models&#34;&gt;Dream 7B: Diffusion Large Language Models
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2508.15487&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2508.15487 Dream 7B: Diffusion Large Language Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209.png&#34;
	width=&#34;1492&#34;
	height=&#34;465&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209_hu_640220d5a4a7936d.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008113335209_hu_ebb3f34c95380aba.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Performance&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;320&#34;
		data-flex-basis=&#34;770px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;问题：
&lt;ul&gt;
&lt;li&gt;AR模型对于需要整体考虑的任务（长期规划、多约束）场景表现差&lt;/li&gt;
&lt;li&gt;AR模型对于长文本的一致性较差&lt;/li&gt;
&lt;li&gt;在各类通用任务中，要达到与Qwen2.5等顶尖自回归模型相当的性能仍存在显著差距&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;贡献：
&lt;ul&gt;
&lt;li&gt;基于&lt;strong&gt;自回归的LLM 初始化&lt;/strong&gt;和&lt;strong&gt;上下文自适应噪声调度技术&lt;/strong&gt;来实现扩散语言模型的规模化&lt;strong&gt;训练&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Dream 7B Base和Dream 7B Instruct&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;approach&#34;&gt;Approach
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415.png&#34;
	width=&#34;846&#34;
	height=&#34;343&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415_hu_62334379a41411ae.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008193402415_hu_287610e7745136e9.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Dream&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;591px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用Transformer以偏移方式，预测所有&lt;code&gt;[MASK]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;常规的MDM是直接预测对应位置的&lt;code&gt;[MASK]&lt;/code&gt;，需要重新训练一个新的Transformer&lt;/p&gt;
&lt;h4 id=&#34;ar-based-llm-initialization&#34;&gt;AR-based LLM Initialization
&lt;/h4&gt;&lt;p&gt;自回归模型的训练目标就是使用第$i$个隐藏状态预测$i+1$的token&lt;/p&gt;
&lt;p&gt;因此我们以偏移方式进行预测，没有打破这种位置关系&lt;/p&gt;
&lt;p&gt;因此将已有的自回归模型参数作为初始值&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留AR模型的知识&lt;/li&gt;
&lt;li&gt;加速收敛&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;context-adaptive-token-level-noise-rescheduling&#34;&gt;Context-Adaptive Token-Level Noise Rescheduling
&lt;/h4&gt;&lt;p&gt;先前衡量噪声程度一般都是句子级别的：LLaDA衡量某个句子在$t$时刻的权重是$\frac{1}{t}$&lt;/p&gt;
&lt;p&gt;本文发现不同token之间的上下文信息是不同的，因此需要对噪声的衡量更加精细，避免学习的不平衡&lt;/p&gt;
&lt;p&gt;公式化地，定义损失函数：&lt;/p&gt;
$$
L(\theta) = -\mathbb{E}_{x_0,t,x_t}\sum_{i=1}^{L}1\left [x_t^i=M\right] \cdot w(t,x_t,i) \cdot \log p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;
对于LLaDA，其$w(t,x_t,i) = \frac{1}{t}$&lt;/p&gt;
&lt;p&gt;考虑对于某个token的上下文信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;距离越近的&lt;code&gt;unmask&lt;/code&gt;的token提供的信息越丰富&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此论文定义为：&lt;/p&gt;
$$
w(t,x_t,i) = \frac{1}{2}\sum_{j=1}^L\left [x_t^j\neq M\right] Geo(p, |i-j|-1)
$$&lt;p&gt;
其中$Geo$表示几何分布核：&lt;/p&gt;
$$
Geo(p,d) = (1-p)^d\cdot p, \quad d\geq 0
$$&lt;ul&gt;
&lt;li&gt;距离$d$越大，贡献越小&lt;/li&gt;
&lt;li&gt;超参数$p$：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071.png&#34;
	width=&#34;1139&#34;
	height=&#34;602&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071_hu_ad3745440641ea7f.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008202642071_hu_9cf88da2296088af.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;超参数p&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;189&#34;
		data-flex-basis=&#34;454px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;train&#34;&gt;Train
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dream-7B采用了与Qwen2.5-7B完全相同的Transformer架构配置&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pretrain&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SFT&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;采用了之前的技巧，训练上与LLaDA没什么不同（注意损失函数）&lt;/p&gt;
&lt;h3 id=&#34;experiment&#34;&gt;Experiment
&lt;/h3&gt;&lt;h4 id=&#34;base模型&#34;&gt;Base模型
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486.png&#34;
	width=&#34;1013&#34;
	height=&#34;712&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486_hu_530b344c72b58167.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008211344486_hu_e41e555b15c86c6f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;341px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;推理任务中（ARC-E、ARC-C）表现良好&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;规划任务中领先幅度巨大&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;训练数据量非常小&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;初始化策略和上下文自适应噪声调度有效性&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dream-instruct&#34;&gt;Dream-Instruct
&lt;/h4&gt;&lt;p&gt;180万条数据，进行3轮微调&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828.png&#34;
	width=&#34;1224&#34;
	height=&#34;556&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828_hu_215f137c20397b9a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008212201828_hu_de627354d9bd135e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;220&#34;
		data-flex-basis=&#34;528px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;论文中没做分析&lt;/li&gt;
&lt;li&gt;这里和LLaDA一样，SFT之后效果落后，甚至出现了性能下降&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;扩散大语言模型在遵循指令任务中具备与基于自回归的大语言模型相匹敌的潜力，为未来高级扩散大语言模型后训练方案奠定了基础&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;ar-initialization的贡献&#34;&gt;AR Initialization的贡献
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;验证：AR LLM初始化是有效的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验设计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaMA3.2-1B参数初始化的Dream-1B和从头训练的Dream1B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155.png&#34;
	width=&#34;1266&#34;
	height=&#34;825&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155_hu_e7ebe20ad61e1c37.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008213718155_hu_81eed865a386611d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Loss 对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;153&#34;
		data-flex-basis=&#34;368px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Loss始终更低，证明了初始化是有效的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时在这个实验中，论文说明了学习率的影响非常大：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大的学习率：破坏AR LLM的有益特性&lt;/li&gt;
&lt;li&gt;小的学习率：阻碍学习扩散的过程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（但似乎没写上下文自适应噪声调度机制的消融实验）&lt;/p&gt;
&lt;h4 id=&#34;planning-ability&#34;&gt;Planning Ability
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368.png&#34;
	width=&#34;1535&#34;
	height=&#34;458&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368_hu_87ecdcf0260af463.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214118368_hu_66b211674994c51b.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;规划能力对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;335&#34;
		data-flex-basis=&#34;804px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dream 模型在两项任务中始终优于其他同等规模的基线模型&lt;/li&gt;
&lt;li&gt;扩散语言模型在解决涉及多重约束或特定目标优化的问题时具有天然优势（？）&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;trade-off&#34;&gt;Trade-off
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200.png&#34;
	width=&#34;987&#34;
	height=&#34;692&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200_hu_8669f15f499a63cc.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008214402200_hu_34d18c98a9954339.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Trade-off&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;142&#34;
		data-flex-basis=&#34;342px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Diffusion language models provide a unique advantage through their adjustable inference process&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;基于时间步长的方法为推理时缩放引入了新的维度，可与现有技术协同工作，例如 OpenAI o1和DeepSeek R1等大型语言模型中使用的思维链推理&lt;/li&gt;
&lt;li&gt;这种可调节的计算质量权衡代表了扩散模型区别于传统自回归模型的关键优势。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-15-variance-reduced-preference-optimization-for-large-language-diffusion-models&#34;&gt;LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.19223&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2505.19223 LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427.png&#34;
	width=&#34;1719&#34;
	height=&#34;730&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427_hu_d7188d0aed1b164.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008221825427_hu_6a39ec4afc25f7a1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA1.5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;235&#34;
		data-flex-basis=&#34;565px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;看不懂&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;大概就是通过VRPO这个方法，基于LLaDA的工作，对LLaDA-instruct进行RL&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570.png&#34;
	width=&#34;877&#34;
	height=&#34;636&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570_hu_66310da504c89ddc.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008234343570_hu_5e68522af6e9a73.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA RL&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;137&#34;
		data-flex-basis=&#34;330px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;longllada-unlocking-long-context-capabilities-in-diffusion-llms&#34;&gt;LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2506.14429&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2506.14429  LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746.png&#34;
	width=&#34;1304&#34;
	height=&#34;432&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746_hu_cdf13d8671fc9b92.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251008235711746_hu_37b525e60acea430.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;长上下文对比&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;301&#34;
		data-flex-basis=&#34;724px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;核心问题：扩散型LLMs在长文本处理领域的研究空白&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;为什么扩散LLM在直接长度外推时保持稳定的困惑度并呈现&lt;strong&gt;局部感知特性&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;针对自回归 LLM 建立的长度扩展技术能否迁移至扩散架构&lt;/li&gt;
&lt;li&gt;自回归基线相比，扩散 LLM 在长上下文基准测试中表现如何？会显现哪些独特能力或局限性？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;贡献：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;揭示了其在上下文外推过程中保持稳定困惑度和局部感知的独特特性，并通过RoPE机制进行了解释&lt;/li&gt;
&lt;li&gt;基于 NTK 的 RoPE 外推法与缩放定律可无缝迁移至扩散 LLMs，实现 6 倍上下文扩展&lt;/li&gt;
&lt;li&gt;benchmark表明：扩散 LLMs 在&lt;strong&gt;检索任务&lt;/strong&gt;中与自回归模型表现相当，在&lt;strong&gt;聚合任务&lt;/strong&gt;中稍显不足，但在&lt;strong&gt;问答任务&lt;/strong&gt;中表现卓越&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;long-context-phenomenology-of-diffusion-llms&#34;&gt;Long-Context Phenomenology of Diffusion LLMs
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;大海捞针测试（Needle-In-A-Haystack, NIAH）&lt;/p&gt;
&lt;p&gt;在一个超长的上下文（haystack，干草堆）里，研究者会插入一小段关键信息（needle，针）&lt;/p&gt;
&lt;p&gt;模型的任务是：在生成或问答过程中，能否准确地“找到”并使用这段信息。&lt;/p&gt;
&lt;p&gt;这类测试会改变针的位置（例如放在靠前、中间或靠后部分）以及上下文的总长度，用来观察模型在不同深度和不同长度下的表现。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;实验目的：揭示扩散 LLM 在长上下文中出现的&lt;strong&gt;局部感知 (local perception)&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验设计：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;输入：在不同长度（最多32k）的长上下文中插入一个needle&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出：限定模型输出最多32个token&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;实验对象&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DLM：block size = 32，采样步数 = 32&lt;/li&gt;
&lt;li&gt;LLM：默认&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;评估指标&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;找到Needle的成功率&lt;/li&gt;
&lt;li&gt;模型在不同深度（前文、中间、后文）找到Needle的能力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065.png&#34;
	width=&#34;1135&#34;
	height=&#34;686&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065_hu_335374cd5412e855.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009004938065_hu_befee1c766cd6b1e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaDA与LLaMA系列实验结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;附录中补充了其他DLM模型的实验&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;AR LLM在8K内的上下文表现完美，超过8K长度无法完成任何任务&lt;/li&gt;
&lt;li&gt;DLM出现了类似**滑动窗口（窗口长度为4k）**的表现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DLM受采样步数影响较大，因此定量补充了实验：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059.png&#34;
	width=&#34;1169&#34;
	height=&#34;749&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059_hu_546e20570a7401cb.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009005810059_hu_181f5578c1ab51ce.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Sample Step&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;156&#34;
		data-flex-basis=&#34;374px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;表明扩散 LLMs 的长上下文性能虽受采样步数影响，但仍受限于模型支持的最大上下文长度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;机制分析&#34;&gt;机制分析
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;自回归只能看见后续的：$[0, T_{train} - 1]$（LLaMA的$T_{train} = 8192$​）&lt;/li&gt;
&lt;li&gt;DLM是双向注意力：$[1-T_{train},T_{train}-1]$（LLaDA的$T_{train}=4096$）
&lt;ul&gt;
&lt;li&gt;对于单个token，可以同时出现在左边的上下文窗口，也可以出现在右边的上下文窗口&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544.png&#34;
	width=&#34;1890&#34;
	height=&#34;919&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544_hu_57b583127a349bfd.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009100213544_hu_9380a40281cc2024.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;context&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;205&#34;
		data-flex-basis=&#34;493px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;留坑：RoPE&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;LLaMA完全丢失了负相对位置的信息，外推能力受限&lt;/li&gt;
&lt;li&gt;LLaDA虽然$T_{train}$比较小，但是能够接受到一个负正窗口&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;LLaMA：只学习了从头往后一个个token读取的能力&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;它可以知道，第2个token是第1个token的后一个……第1000个token是第999个token的后一个……&lt;/p&gt;
&lt;p&gt;（像翻书一样可以一页一页翻）&lt;/p&gt;
&lt;p&gt;但是一旦碰到第10000页，它推理不出这是9999页过来的（超出上下文，没有学习过这种关系）&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LLaDA：双向上下文&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;可以推断出9999是10000的前一页&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;论文补充了t-SNE可视化实验&lt;/p&gt;
&lt;p&gt;观察了两个模型最后的Q和K states&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445.png&#34;
	width=&#34;2054&#34;
	height=&#34;955&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445_hu_3136df464a2be127.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009140617445_hu_d78712eecfc63560.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;t-SNE&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;215&#34;
		data-flex-basis=&#34;516px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA随着上下文长度增加，仍然保持形状&lt;/li&gt;
&lt;li&gt;LLaMA出现了明显的聚类分离，表示内部出现了&lt;code&gt;distribution shift&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;context-extension&#34;&gt;Context Extension
&lt;/h3&gt;&lt;p&gt;将 &lt;strong&gt;NTK-based RoPE extrapolation&lt;/strong&gt;（一种在自回归 LLM 中已验证的旋转位置嵌入扩展方法）迁移到扩散式 LLM&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;缩放旋转基数 β0&lt;/strong&gt;，让正弦/余弦函数周期变长，相当于“拉伸坐标轴”，从而容纳更长的上下文&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144.png&#34;
	width=&#34;1226&#34;
	height=&#34;718&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144_hu_c1933c2297845371.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142455144_hu_662f939c281b78cd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;base&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;170&#34;
		data-flex-basis=&#34;409px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056.png&#34;
	width=&#34;1223&#34;
	height=&#34;702&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056_hu_2f6517f50f9ebb35.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009142511056_hu_70ac19fca8c8e977.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;instruct&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;174&#34;
		data-flex-basis=&#34;418px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;小幅扩展有效&lt;/strong&gt;： 8k 或 16k，几乎在所有深度下都保持接近 100% 的检索准确率。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;中等扩展出现性能下降&lt;/strong&gt;：24k ，出现lost-in-the-middle现象&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;自回归模型中同样有的现象&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;大规模扩展失败&lt;/strong&gt;：模型无法再有效外推，说明方法的实际上限已到达。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;附录中对同类的DLM做了相同的实验&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;experiment-1&#34;&gt;Experiment
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;SD、MD、Sum 和 Syn 分别代表单文档问答、多文档问答、摘要和合成任务&lt;/p&gt;
&lt;p&gt;Avg 是所有子任务按评估数据数量加权的平均得分&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304.png&#34;
	width=&#34;2012&#34;
	height=&#34;745&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304_hu_309eb6e92ee1f29a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009145901304_hu_99cab5b931fefe45.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LongBench&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;270&#34;
		data-flex-basis=&#34;648px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;平均得分媲美AR LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;检索（NIAH）/聚合（AGG）/问答（QA)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639.png&#34;
	width=&#34;1978&#34;
	height=&#34;1185&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639_hu_a472d06a9de10b3a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009150019639_hu_b08d80dfb6e4d0da.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Ruler&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;166&#34;
		data-flex-basis=&#34;400px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;检索任务：相当&lt;/li&gt;
&lt;li&gt;聚合任务：不如AR LLM&lt;/li&gt;
&lt;li&gt;问答任务：超过AR LLM&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-v-large-language-diffusion-models-with-visual-instruction-tuning&#34;&gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2505.16933&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087.png&#34;
	width=&#34;1961&#34;
	height=&#34;949&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087_hu_73052e8f451b3af2.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009155310087_hu_31fdd1ed720a7b25.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;206&#34;
		data-flex-basis=&#34;495px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;问题：完全基于扩散机制的多模态大语言模型能否达到与AR LLM相匹敌的性能？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;论文贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首个完全基于扩散模型的多模态大语言模型&lt;/li&gt;
&lt;li&gt;在多个基准测试中展现出卓越的可扩展性&lt;/li&gt;
&lt;li&gt;在混合型及纯扩散式多模态大语言模型中均SOTA&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;visual-instruction-tuning&#34;&gt;Visual Instruction Tuning
&lt;/h3&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vison Tower（CLIP或SigLIP）：图像转视觉表征&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;MLP connector：嵌入LLM词空间&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Language Tower：LLM&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主流的多模态大模型架构之一，只需要相对较少的数据（less than 100w 图文数据对）&lt;/p&gt;
&lt;p&gt;本文主要研究如何在DLM中进行Visual Instruction Tuning&lt;/p&gt;
&lt;h3 id=&#34;method&#34;&gt;Method
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Language Tower：LLaDA 8B（与LLaMA3-8B相当的语言模型）&lt;/li&gt;
&lt;li&gt;Vison Tower：SigLIP&lt;/li&gt;
&lt;li&gt;MLP Connector：a two-layer MLP&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;training&#34;&gt;Training
&lt;/h4&gt;&lt;p&gt;训练阶段引入了含有多轮对话的数据&lt;/p&gt;
&lt;p&gt;为了简化描述，文章以2轮对话的数据进行说明，定义符号：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\mathcal{v}$：Vison Tower和MLP Connector生成的视觉表征向量&lt;/li&gt;
&lt;li&gt;$[M]$​：掩码标记&lt;/li&gt;
&lt;li&gt;数据：$(\mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2)$&lt;/li&gt;
&lt;li&gt;$p_0^1 = [ p_0^{1,i}]$：首轮提示文本&lt;/li&gt;
&lt;li&gt;$p_0^2 = [ p_0^{2,i}]$ ：次轮提示文本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于一个二轮对话，训练目标定义为：&lt;/p&gt;
$$
L(\theta) = -\mathbb{E}_{\mathcal{v},t,p_0^1,r_0^1,r_t^1,p_0^2,r_0^2,r_t^2}\left[\frac{1}{t}\sum_{i=1}^{L_{p_1}}\sum_{j=1}^{L_{p_2}}1\left[r_t^{1,i}=M\wedge r_t^{2,j}=M\right] \cdot \log p_\theta(r_0^{1,i},r_0^{2,j}\mid \mathcal{v}, p_0^1,r_0^1,p_0^2,r_0^2 ) \right]
$$&lt;blockquote&gt;
&lt;p&gt;在多轮对话场景下，&lt;strong&gt;不同轮次的响应是强相关的&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;用户的问题可能在第 1 轮，答案在第 2 轮&lt;/li&gt;
&lt;li&gt;推理链条往往横跨多个回合，不能只看单独的 token&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;模型必须在预测某个 token 时，同时考虑另一轮对话中的掩码 token&lt;/p&gt;
&lt;p&gt;这样就把 &lt;strong&gt;跨轮次的依赖关系&lt;/strong&gt; 学进去，而不是每轮单独学&lt;/p&gt;
&lt;p&gt;联合约束迫使模型去捕捉 &lt;strong&gt;对话轮次之间的因果逻辑&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;理论上这个式子在先前工作中已经被证明为整个任务的负对数似然上界&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在多轮对话中似乎可以采用causal mask，阻止早期对话轮次访问了后期的对话轮次&lt;/li&gt;
&lt;li&gt;后文消融实验证明双向注意力的效果更好（实现对整体对话语境的全面理解）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;该机制在近期&lt;strong&gt;视频扩散模型&lt;/strong&gt;中已证实可有效提升生成视频的时间连贯性&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;本身训练的流程和LLaDA的SFT流程比较相似，加噪只会在Response中，且同时对多轮对话中的Response进行加噪&lt;/p&gt;
&lt;p&gt;一次性让模型恢复所有对话中的MASK&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499.png&#34;
	width=&#34;1569&#34;
	height=&#34;545&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499_hu_14f4dbdd112ab8a5.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009200659499_hu_55f6d0881c6145a4.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;AR &amp;#43; Train &amp;#43; Inference&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;287&#34;
		data-flex-basis=&#34;690px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;training-strategies&#34;&gt;Training Strategies
&lt;/h4&gt;&lt;p&gt;整个训练过程参考了LLaVA的训练策略&lt;/p&gt;
&lt;p&gt;建立语言和视觉对齐关系并培养视觉指令跟随能力&lt;/p&gt;
&lt;p&gt;训练目标函数与上文相同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;阶段一：语言-图像对齐
&lt;ul&gt;
&lt;li&gt;目的：图像与语言的分布不一致，如果直接做指令调优，模型学习跨模态语义很困难&lt;/li&gt;
&lt;li&gt;方法：&lt;strong&gt;将视觉表征与 LLaDA 的词向量进行对齐&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;冻结Vison Tower和Language Tower（这两个本身进行过预训练），只训练MLP Connector&lt;/li&gt;
&lt;li&gt;数据集：LLaVA-Pretrain&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;阶段二：视觉指令调优Visual Instruction Tuning
&lt;ul&gt;
&lt;li&gt;目的：（单图像训练）建立基本的图像理解能力，（多图像训练）扩展到时序和跨图像推理&lt;/li&gt;
&lt;li&gt;方法：（两个阶段）解冻所有层
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;单图像训练Single image&lt;/strong&gt;：在 &lt;strong&gt;1,000 万单图像样本&lt;/strong&gt;上训练，增强对单张图像的理解与响应能力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;统一视觉训练阶段one vision&lt;/strong&gt;：在 &lt;strong&gt;约 200 万多模态样本&lt;/strong&gt;（包括单图、多图和视频）上训练，使模型具备处理复杂场景的能力&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据集：&lt;strong&gt;MAmmoTH-VL&lt;/strong&gt; 数据集&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;阶段三：多模态推理增强 Multimodal Reasoning Enhancement
&lt;ul&gt;
&lt;li&gt;目的：增强模型处理复杂任务的多模态推理能力，加入reasoning data提升数学、跨图像和逻辑推理任务的表现&lt;/li&gt;
&lt;li&gt;方法
&lt;ul&gt;
&lt;li&gt;推理训练：使用来自 VisualWebInstruct聚焦推理的多模态数据对 LLaDA-V 进行训练（90 万个问答对，详尽的推理链和最终答案）&lt;/li&gt;
&lt;li&gt;平衡训练：参考qwen系列，融合VisualWebInstruct（其中50%添加&lt;code&gt;\think&lt;/code&gt;）和MAmmoTH-VL(one vison部分，全部添加&lt;code&gt;\no_think&lt;/code&gt;，鼓励直接回答)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;inference&#34;&gt;Inference
&lt;/h4&gt;&lt;p&gt;推理时根据已有的对话记录，对当前的prompt进行单轮的response生成&lt;/p&gt;
&lt;p&gt;重掩码策略采用low-confidence strategy&lt;/p&gt;
&lt;h3 id=&#34;experiment-2&#34;&gt;Experiment
&lt;/h3&gt;&lt;p&gt;可扩展性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA-V 随着&lt;strong&gt;训练数据&lt;/strong&gt;增加性能持续提升&lt;/li&gt;
&lt;li&gt;在 &lt;strong&gt;多学科与数学推理任务&lt;/strong&gt; 上，LLaDA-V 扩展性明显优于 LLaMA3-V&lt;/li&gt;
&lt;li&gt;但在 &lt;strong&gt;图表/文档理解&lt;/strong&gt; 和 &lt;strong&gt;真实场景理解&lt;/strong&gt; 任务上，LLaMA3-V 表现更优&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275.png&#34;
	width=&#34;1402&#34;
	height=&#34;695&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275_hu_2a1887244b431296.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009221821275_hu_3b1ccaa0b03d7012.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;scalability&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;484px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Benchmark&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于已有的混合或扩散模型，LLaDA-V是SOTA&lt;/li&gt;
&lt;li&gt;对比LLaMA3-V：6 个任务上超越&lt;/li&gt;
&lt;li&gt;对比Qwen2-VL：整体仍落后&lt;/li&gt;
&lt;li&gt;图表/文档理解和 RealWorldQA 上表现稍差&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556.png&#34;
	width=&#34;1710&#34;
	height=&#34;757&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556_hu_16fbc19e2c4d96ea.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222242556_hu_c8031429e440442.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;225&#34;
		data-flex-basis=&#34;542px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;消融实验&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对比了Causal Mask和无Mask（多轮对话）&lt;/li&gt;
&lt;li&gt;12个benchmark中7个更优&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983.png&#34;
	width=&#34;953&#34;
	height=&#34;567&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983_hu_d09e07717de8958f.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251009222503983_hu_d4cffbcd05dc86ef.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;消融实验&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;403px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;图像接入SigLIP的方式比较简单，会丢失分辨率和信息，造成图表问题表现差&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;llada-medv-exploring-large-language-diffusion-models-for-biomedical-image-understanding&#34;&gt;LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2508.01617&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2508.01617 LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;没怎么看，大概是把LLaDA-V的工作调整到了垂类领域&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;一些比较有趣的实验分析：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DLM在一些垂类领域非常合适，可以&lt;strong&gt;显式地控制&lt;/strong&gt;一个大概的生成长度&lt;/li&gt;
&lt;li&gt;模型可能出现重复 token（如 “the the the …”）的问题，尤其在&lt;strong&gt;采样步数较少&lt;/strong&gt;或&lt;strong&gt;长度设定较大&lt;/strong&gt;时&lt;/li&gt;
&lt;li&gt;直接使用LLaDA-V的参数做微调的性能反而更差，需要从LLaDA-instruct出发，重新走3个步骤&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;lavida-a-large-diffusion-language-model-for-multimodal-understanding&#34;&gt;LaViDa: A Large Diffusion Language Model for Multimodal Understanding
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2505.16839&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2505.16839&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AR LLM对强双向上下文要求的任务（文本填充、从图像中提取信息填充到json格式）很弱&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉语言场景中对输出模式的要求特别严格&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;标准扩散模型训练的数据效率低下，未被遮盖的token不参与损失函数计算，容易遗失关键语义信息（关键词）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现有的推理方式缺少了KV cache的支持（双向上下文固有的缺陷）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;短文本环境中是可容忍的&lt;/li&gt;
&lt;li&gt;对于VLM任务无法接受，常常伴有数百个visual token&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;固定比例的&lt;code&gt;unmask&lt;/code&gt;在迭代次数较少时效果非常差&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;贡献：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;第一个DLM视觉语言模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一种互补的mask方案，确保每个token都能参与到学习过程中，提高数据效率&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prefix-DLM decoding：缓存多模态的提示词与图像输入，从而加速推理过程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;受文生图技术的启发，采用了时间步偏移策略，自适应调整每次迭代的解码数量&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;method-1&#34;&gt;Method
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779.png&#34;
	width=&#34;857&#34;
	height=&#34;381&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779_hu_83db3a345a7595c6.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251018171520779_hu_e775b30abb7e0603.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image-20251018171520779&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;224&#34;
		data-flex-basis=&#34;539px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Vision Encoder和DLM通过MLP进行连接&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;模型输入：图像$I$和文本$P$&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;vision-encoder&#34;&gt;Vision Encoder
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;$I \to_{resize} 768\times 768$&lt;/li&gt;
&lt;li&gt;切分成四个不重叠的部分（$I_{1:4} = (384,384)$）；直接resize原图为$(384,384)$，得到$I_5$&lt;/li&gt;
&lt;li&gt;每一个子图被独立地通过Vision Encoder（SigLIP-400M ）进行编码
&lt;ul&gt;
&lt;li&gt;$I_i \to V_i, \text{size} = 27 \times 27$，五个子图总共产生了3645个embeddings&lt;/li&gt;
&lt;li&gt;$2\times 2$平均池化（缩短序列，提升训练效率）：$14\times 14$，总共980个embeddings&lt;/li&gt;
&lt;li&gt;经过MLP构成的Projection Network，Flatten到1D&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;这里是输出5个一维向量，还是拼在一起的1个一维向量？&lt;/p&gt;
&lt;p&gt;暂时没去研究&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;dlm&#34;&gt;DLM
&lt;/h4&gt;&lt;p&gt;DLM的输入：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;视觉嵌入向量&lt;/li&gt;
&lt;li&gt;文本提示词$P$&lt;/li&gt;
&lt;li&gt;带有掩码的$X_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;输出：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;概率分布，用于获取$X_0$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文采用LLaDA和Dream作为DLM&lt;/p&gt;
&lt;h4 id=&#34;complementary-mask&#34;&gt;Complementary Mask
&lt;/h4&gt;&lt;p&gt;文本中信息量非常密集，一般就是几个词&lt;/p&gt;
&lt;p&gt;对于之前的加噪方法，不一定能恰好遮蔽需要的词&lt;/p&gt;
&lt;p&gt;例如&lt;code&gt;The answer is dog.&lt;/code&gt;，加噪为：&lt;code&gt;The [MASK] [Mask] dog.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;事实上我们只需要引入它的互补形式：&lt;code&gt;[MASK] answer is [MASK]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246.png&#34;
	width=&#34;1264&#34;
	height=&#34;883&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246_hu_6bc6d2d986ad22ba.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020151759246_hu_3cdadd9f2c6fbffc.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;complementary&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;143&#34;
		data-flex-basis=&#34;343px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;同时，这份数据会直接copy视觉以及提示词部分的嵌入&lt;/p&gt;
&lt;p&gt;因此对实际训练开销的影响较小&lt;/p&gt;
&lt;h4 id=&#34;prefix-dlm&#34;&gt;Prefix-DLM
&lt;/h4&gt;&lt;p&gt;定义序列长度为$L$，推理的迭代次数$K$，我们有NFE（fraction of the number of functional evaluations）：&lt;/p&gt;
$$
\text{NFE} = \frac{K}{L}
$$&lt;ul&gt;
&lt;li&gt;NFE=100%时，每次迭代生成一个Token&lt;/li&gt;
&lt;li&gt;NFE=50%时，每次迭代生成2个Token&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;但实际上由于毫无推理优化，DLM的速度比自回归慢&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365.png&#34;
	width=&#34;1423&#34;
	height=&#34;417&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365_hu_754ba3cf8053a3da.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020161428365_hu_4f0aa1432c96e31f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Attention&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;341&#34;
		data-flex-basis=&#34;818px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Causal Mask：可以不断复用之前的token的kv矩阵（之前的token没有变化，且之前的token对未来的token不感兴趣）&lt;/li&gt;
&lt;li&gt;Full Mask：每个token都会看前后的内容，因此需要不断重新计算&lt;/li&gt;
&lt;li&gt;Prefix-DLM：I和P部分不会发生变化，遮蔽了对未来answer的token&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;schedule-shift&#34;&gt;Schedule Shift
&lt;/h4&gt;&lt;p&gt;喷了一下等步长的解码，提出了非线性的递推：&lt;/p&gt;
$$
t&#39;_i = \frac{\alpha t_i}{1+(\alpha -1)t_i}
$$&lt;p&gt;
设计一个时间重参数化的函数，要求满足：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$t_0 = 0,t_1 = 1$，仍然保持边界&lt;/li&gt;
&lt;li&gt;单调递增&lt;/li&gt;
&lt;li&gt;曲率可控，方便控制早晚阶段的速度&lt;/li&gt;
&lt;li&gt;简单，防止数值不稳定或梯度爆炸&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文希望早期降噪快，后期降噪慢&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;希望先快速搭建一个骨架，后面慢慢填充&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;对这个式子求导：&lt;/p&gt;
$$
\frac{dt_i&#39;}{dt_i} =   \frac{\alpha}{(1+(\alpha-1)t)^2}
$$&lt;p&gt;
当$t=0$，导数为$\alpha$，$t=1$，导数为$\frac{1}{\alpha}$&lt;/p&gt;
&lt;p&gt;可以通过$\alpha$​和1的大小关系，控制是先快后慢，还是先慢后快&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428.png&#34;
	width=&#34;739&#34;
	height=&#34;681&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428_hu_afa6e273813b0639.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021190931428_hu_ba9745771f5c5b65.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;schedule&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;108&#34;
		data-flex-basis=&#34;260px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments
&lt;/h3&gt;&lt;p&gt;略过benchmark部分&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371.png&#34;
	width=&#34;1530&#34;
	height=&#34;1237&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371_hu_43223d13feada0c0.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251021202137371_hu_2a6ba790d8bb9229.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;benchmark&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;123&#34;
		data-flex-basis=&#34;296px&#34;
	
&gt;&lt;/p&gt;
&lt;h4 id=&#34;reasoning-distillation&#34;&gt;Reasoning Distillation
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087.png&#34;
	width=&#34;519&#34;
	height=&#34;219&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087_hu_73460ab98d2fb011.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020211558087_hu_aa6c53100831621.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;reasoning&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;236&#34;
		data-flex-basis=&#34;568px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;paper通过蒸馏VL-Rethinker-7B模型（19.2K CoT examples）&lt;/p&gt;
&lt;p&gt;训练得到LaViDa-Reason&lt;/p&gt;
&lt;p&gt;在MathVista、MathVerse和MathVision上均得到提升&lt;/p&gt;
&lt;h4 id=&#34;text-infilling&#34;&gt;Text Infilling
&lt;/h4&gt;&lt;p&gt;对于实际的文本填充任务，不需要生成所有内容，只需要填写需要填写的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;任意时间步长开始生成&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;There is a [M][M][M][M] in the image.&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里我们希望是&lt;code&gt;dog&lt;/code&gt;或者&lt;code&gt;traffic light&lt;/code&gt;，也是就是variable-length completions&lt;/p&gt;
&lt;p&gt;因此利用了第二阶段（激活全部参数）的20%数据，进行第三阶段训练，得到LaViDa-FIM&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文本中插入&lt;strong&gt;随机长度&lt;/strong&gt;的&lt;code&gt;[S][S]……[S][FIM]&lt;/code&gt;序列&lt;/li&gt;
&lt;li&gt;推理时，在掩码段后面附加&lt;code&gt;[FIM]&lt;/code&gt;，得到&lt;code&gt;There is a [M][M][M][M][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;模型自然可以生成：
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;There is a dog[S][S][S][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;There is a traffic light[S][S][FIM] in the image.&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;约束性诗歌生成：模型根据图像生成一首诗，每行以特定音节开头&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;强调了结构性约束和上下文一致性，测试双向生成能力&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484.png&#34;
	width=&#34;886&#34;
	height=&#34;492&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484_hu_7654eb8df0de9790.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215449484_hu_540b0ba14a6cc84c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;诗歌补全&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;180&#34;
		data-flex-basis=&#34;432px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081.png&#34;
	width=&#34;506&#34;
	height=&#34;199&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081_hu_6c166c051812bb1a.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020215153081_hu_f982c9463f0367e2.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;poem completion&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;610px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sentence：满足行级别的约束的比例&lt;/li&gt;
&lt;li&gt;Sample：满足样本级约束的比例&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;speed-vs-quality-trade-off&#34;&gt;Speed vs. Quality Trade off
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620.png&#34;
	width=&#34;557&#34;
	height=&#34;572&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620_hu_994630fc0b694545.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020224259620_hu_878a9a6de5bcea7.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Quality-Speed&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;97&#34;
		data-flex-basis=&#34;233px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;设定长度固定32，通过调整迭代次数K，进行实验&lt;/p&gt;
&lt;p&gt;（数据集COCO2017图像描述 500张，生成图像标题）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NFE=100%，稍慢于AR LLM，但是性能更强&lt;/li&gt;
&lt;li&gt;50-75%的性能和速度都很不错&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247.png&#34;
	width=&#34;560&#34;
	height=&#34;278&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247_hu_10c5f2cf2970a1ff.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225129247_hu_1bf8e32d51acfc60.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Prefix-DLM&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;483px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有效的加速推理，性能下降少&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258.png&#34;
	width=&#34;541&#34;
	height=&#34;233&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258_hu_fdf1729242ac0fa8.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020225349258_hu_48f7899bb17d908d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Timestep-shifting&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;232&#34;
		data-flex-basis=&#34;557px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;先快后慢是对的&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;消融实验部分&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证了互补掩码&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797.png&#34;
	width=&#34;575&#34;
	height=&#34;285&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797_hu_c22d64ad30e12606.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020230652797_hu_a75bfb12c4f27585.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Comp. Mask&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;201&#34;
		data-flex-basis=&#34;484px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;验证了图像分辨率的影响（输入分辨率）
&lt;ul&gt;
&lt;li&gt;OCR（上面四个）的提升更为明显&lt;/li&gt;
&lt;li&gt;一般视觉理解任务（最后一个）提升不多&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771.png&#34;
	width=&#34;409&#34;
	height=&#34;247&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771_hu_240c4baf6b5065ba.png 480w, https://example.com/p/diffusion-language-model-birdresearch-202510/assets/image-20251020231154771_hu_69ac30fdc1178978.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;image revolution&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;165&#34;
		data-flex-basis=&#34;397px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;STOP，接下来需要进入any2any的调研&lt;/p&gt;
&lt;p&gt;这部分内容另开一篇&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Diffusion Language Model · 论文笔记（一）</title>
        <link>https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
        <pubDate>Fri, 26 Sep 2025 15:46:00 +0800</pubDate>
        
        <guid>https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2502.09992&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Large Language Diffusion Models&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布&lt;/p&gt;
&lt;p&gt;因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布&lt;/p&gt;
&lt;p&gt;因此指令跟随、上下文学习并不是ARM的专利&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach
&lt;/h2&gt;&lt;h3 id=&#34;概率公式&#34;&gt;概率公式
&lt;/h3&gt;&lt;h4 id=&#34;前向过程forward-process&#34;&gt;前向过程Forward Process
&lt;/h4&gt;&lt;p&gt;序列中逐渐添加mask，直到所有序列全部masked&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个标记有一定概率masked，或者保持unmasked状态&lt;/li&gt;
&lt;li&gt;给定原始数据$x_0$，随机采样的一个时间点$t \in [0,1]$
&lt;ul&gt;
&lt;li&gt;$t=0$表示起点，全部token都是unmasked&lt;/li&gt;
&lt;li&gt;$t=1$表示终点，全部token都已经masked&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;序列中每个token的masked概率就是$t$&lt;/li&gt;
&lt;li&gt;该时刻的序列被定义为$x_t$​&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Bert的mask比例是固定的&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;反向过程reverse-process&#34;&gt;反向过程Reverse Process
&lt;/h4&gt;&lt;p&gt;参数为$\theta$的模型，生成序列$x_0$的概率$p_\theta(x_0)$就是我们需要训练的模型&lt;/p&gt;
&lt;p&gt;我们希望其尽可能接近真实数据的分布$p_{data}(x_0)$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;反向过程的目标：$x_{t=1}$出发，恢复$x_0$&lt;/li&gt;
&lt;li&gt;方法：通过&lt;code&gt;mask predictor&lt;/code&gt;，逐步填充&lt;code&gt;masked token&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;mask-predictor&#34;&gt;Mask Predictor
&lt;/h4&gt;&lt;p&gt;LLaDA的核心是一个&lt;code&gt;mask predictor&lt;/code&gt;&lt;/p&gt;
$$
p_\theta\left (\cdot\mid x_t \right)
$$&lt;p&gt;
其中的&lt;code&gt;·&lt;/code&gt;表示一个占位符&lt;/p&gt;
&lt;p&gt;例如：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;I [MASK] cats.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;则$p_\theta(\cdot \mid [I,[MASK],cats])$ 就是一个基于词表的概率分布表&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;token&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;p&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;like&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;eat&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.1&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;此时&lt;code&gt;Mask Predictor&lt;/code&gt;会对所有&lt;code&gt;[MASK]&lt;/code&gt;进行预测&lt;/p&gt;
&lt;p&gt;不像ARM只预测一个token&lt;/p&gt;
&lt;p&gt;假设序列为：$(x_t^1, x_t^2,&amp;hellip;,x_t^L)$&lt;/p&gt;
&lt;p&gt;我们的目标即为，对于&lt;code&gt;masked&lt;/code&gt;位置$i$，最大化概率：&lt;/p&gt;
$$
p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;其中$x_0^i$就是原序列的ground truth&lt;/p&gt;
&lt;h4 id=&#34;损失函数&#34;&gt;损失函数
&lt;/h4&gt;&lt;p&gt;对于单个&lt;code&gt;masked&lt;/code&gt;位置，我们希望概率尽可能大&lt;/p&gt;
&lt;p&gt;因此需要使用交叉熵损失&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;补习一下交叉熵&lt;/p&gt;
&lt;p&gt;假设真实分布是$q(y)$，模型分布是$p_\theta(y)$&lt;/p&gt;
&lt;p&gt;交叉熵定义为：&lt;/p&gt;
$$
H(q,p_\theta) = -\sum_y  q(y)\log{p_\theta(y)}
$$&lt;p&gt;
在该任务下，$q(y)$​是一个独热分布&lt;/p&gt;
$$
q(y) = \left\{\begin{matrix}
  1&amp;,y=x_0^i \\
  0&amp;,otherwise
\end{matrix}\right.
$$&lt;p&gt;
代入得&lt;/p&gt;
$$
\mathcal{L}(x_0^i,x_t) = -\log p_\theta(x_0^i\mid x_t)
$$&lt;p&gt;
故整体的损失就只需要对所有&lt;code&gt;masked&lt;/code&gt;位置求和&lt;/p&gt;
$$
\mathcal{L}(x_0,x_t) = -\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$&lt;p&gt;
其中$1\left[ x^i_t=M\right] $表示指示函数，确保代入计算的数值是&lt;code&gt;[MASK]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;但是这样是不合理的，序列中&lt;code&gt;[MASK]&lt;/code&gt;越多，损失似乎会越大&lt;/p&gt;
&lt;p&gt;因此需要做一下归一化&lt;/p&gt;
&lt;p&gt;对于$x_t$，其在该时刻会有$tL$个token被&lt;code&gt;masked&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;因此需要代入一个$\frac{1}{t}$​（值得一提的是，$\frac{1}{t} \geq 1$）&lt;/p&gt;
$$
\mathcal{L}(x_0,x_t) = -\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$&lt;p&gt;
建立关于模型参数$\theta$的损失函数则有：&lt;/p&gt;
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$&lt;ul&gt;
&lt;li&gt;$\triangleq$代表&lt;code&gt;定义为&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;这里不是在陈述一个“事实”，而是在&lt;strong&gt;引入损失函数的定义&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;如果写$=$，读者可能会以为“这是某个推导得到的等式”；&lt;/p&gt;
&lt;p&gt;如果写 $\triangleq$，读者一眼就知道：哦，这是“定义”，不是推导。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;从均匀分布$U(0,1)$采样的任意$t$，从数据集中采样的任意数据$x_0$，根据前向传播方法得到的$x_t$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;负对数似然的上界&#34;&gt;负对数似然的上界
&lt;/h4&gt;&lt;p&gt;我们本身的目标是使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$&lt;/p&gt;
&lt;p&gt;但是我们从未单独定义、训练$p_\theta(x)$这个模型&lt;/p&gt;
&lt;p&gt;而是定义了一个$p_\theta(x_0^i\mid x_t)$，不断重复迭代，起到了$p_\theta(x)$的作用&lt;/p&gt;
&lt;p&gt;因此我们上述内容得到的$L(\theta)$是作为模型$p_\theta(x_0^i\mid x_t)$的损失函数&lt;/p&gt;
&lt;p&gt;我们如何保证训练出这个模型，可以使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$？&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;我们定义真实的似然函数是&lt;/p&gt;
$$
\mathcal{L}(\theta) = -\mathbb{E}_{p_{data}(x_0)}\left [\log{p_{\theta}(x_0)}\right]
$$&lt;ul&gt;
&lt;li&gt;按照真实分布，采样数据$x_0$，得到的似然函数期望&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们需要最小化这个式子&lt;/p&gt;
&lt;p&gt;定义前向加噪分布$q$：&lt;/p&gt;
$$
q(x_t\mid x_0) = \prod_{i=1}^L \left [ (1-t)\times 1(x_t^i=x_0^i) + t\times 1(x_t^i=M) \right]
$$&lt;p&gt;
其描述了通过已知的前向过程，从$x_0$得到$x_t$的概率&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;$q$是我们引入的噪声分布，并非需要训练的参数模型，不使用$p$定义&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;由于前向过程是已知的，我们考虑使用它表示$p_\theta$&lt;/p&gt;
$$
p_\theta(x_0) = \sum_{x_t}p_\theta(x_0\mid x_t)p_\theta(x_t)= \sum_{x_t}p_\theta(x_0, x_t) 
$$&lt;blockquote&gt;
&lt;p&gt;原始句子出现的总概率就是把所有可能路径的概率加起来。&lt;/p&gt;
&lt;p&gt;（先得到$x_t$，再生成$x_0$）&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;引入已知的$q$&lt;/p&gt;
$$
p_\theta(x_0) = \sum_{x_t}q(x_t\mid x_0)\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}
$$&lt;p&gt;
其中$\sum_{x_t}q(x_t\mid x_0)] \times (\cdot)$，可以理解为对$q(x_t\mid x_0)$的期望&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;离散情况下：$\mathbb{E}_{x\sim r}(g(x)) = \sum_x r(x)g(x)$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;此时$x_0$是当作固定值，所有都可以看作关于$x_t$的函数&lt;/p&gt;
&lt;p&gt;因此则有：&lt;/p&gt;
$$
p_\theta(x_0) = \mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]
$$&lt;blockquote&gt;
&lt;p&gt;任意从$q$分布中采样$x_t$&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;采样 Jensen不等式：&lt;/p&gt;
$$
\log \mathbb{E}(z) \geq \mathbb{E}(\log{z})
$$&lt;p&gt;
此时则有：&lt;/p&gt;
$$
\log{p_\theta}(x_0) = \log{\mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]} \geq \mathbb{E}_{ q(x_t\mid x_0)}\left[\log{p_\theta(x_0,x_t)} - \log{q(x_t\mid x_0)} \right]
$$$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0,x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$&lt;p&gt;
分解一下联合概率&lt;/p&gt;
$$
\log{p_\theta(x_0,x_t)} = \log{p_\theta(x_0\mid x_t)} + \log{p_\theta(x_t)}
$$&lt;p&gt;
代回则有：&lt;/p&gt;
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}-\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$&lt;ul&gt;
&lt;li&gt;$p_\theta(x_t)$：其中$x_t$是前向过程人为生成的，因此与$\theta$无关&lt;/li&gt;
&lt;li&gt;$q(x_t\mid x_0)$是噪声项，与$\theta$无关&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此可以写成&lt;/p&gt;
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)} + \text{const}
$$&lt;p&gt;
两边同时对真实数据计算期望&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}(\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}) + \text{const}
$$&lt;p&gt;
左边实质上就是我们需要优化的目标，命名为负对数似然$\text{NLL}$&lt;/p&gt;
&lt;p&gt;根据&lt;/p&gt;
$$
\log {p_\theta(x_0\mid x_t)} = \sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)
$$&lt;p&gt;
代入得：&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const}
$$&lt;p&gt;
事实上右边就是：&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] = 
 -\mathbb{E}_{t,x_0,x_t}\left[\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right] = tL(\theta)
$$&lt;p&gt;
上述内容都是正项（负概率对数），$t \in [0,1]$，因此满足&lt;/p&gt;
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const} \leq L(\theta) + \text{const}
$$&lt;p&gt;
则有：&lt;/p&gt;
$$
\text{NLL} = -\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq L(\theta) + \text{const}
$$&lt;p&gt;
至此，成功证明了$L(\theta)$决定了$\text{NLL}$​的上界（常数可以忽略）&lt;/p&gt;
&lt;h3 id=&#34;pre-training&#34;&gt;Pre-Training
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2.png&#34;
	width=&#34;1628&#34;
	height=&#34;456&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_9a85fb3e2f02f873.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_3f7bfef69b5c6f92.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;A Conceptual Overview of LLaDA.&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;357&#34;
		data-flex-basis=&#34;856px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入：&lt;code&gt;mask predictor&lt;/code&gt;$p_\theta$，训练数据$p_{data}$&lt;/li&gt;
&lt;li&gt;输出：$p_{\theta}$（收敛）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976.png&#34;
	width=&#34;1570&#34;
	height=&#34;385&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_f227ce11f9ed9b16.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_7a4e250f2607c545.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pre-Train Algorithm&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;407&#34;
		data-flex-basis=&#34;978px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;mask predictor&lt;/code&gt;采用Transformer架构
&lt;ul&gt;
&lt;li&gt;不采用&lt;code&gt;causal mask&lt;/code&gt;，能看见双向上下文&lt;/li&gt;
&lt;li&gt;未使用KV Cache，采用标准的&lt;code&gt;Vanilla Multi-Head Attention&lt;/code&gt;，每个头单独一份&lt;code&gt;k,q,v&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Transformer架构尽量与LLaMA3对齐，从&lt;code&gt;attention&lt;/code&gt;和&lt;code&gt;FFN&lt;/code&gt;两个参数大头中，选择了减少&lt;code&gt;FFN&lt;/code&gt;的参数量，保持参数规模可以比较&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;在自回归 LLM 生成时，生成新 token 时可以复用之前的 K/V 矩阵（不用重新算整个序列的注意力）。&lt;/p&gt;
&lt;p&gt;这是 KV cache 的意义：极大加速推理，节省显存。&lt;/p&gt;
&lt;p&gt;但 LLaDA 每一步预测的是 &lt;strong&gt;全局被 mask 的位置（不是单个 token）&lt;/strong&gt;，所以每一步输入分布会变，全序列 K/V 都要重新计算 → KV cache 无法使用。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;99%的数据固定长度4096&lt;/li&gt;
&lt;li&gt;1%的数据随机采样长度&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;sft&#34;&gt;SFT
&lt;/h3&gt;&lt;p&gt;对于问答对$(p_0,r_0)$，我们不改变提问部分，只对&lt;strong&gt;回答部分&lt;/strong&gt;进行掩码加噪得到$r_t$&lt;/p&gt;
&lt;p&gt;损失函数设计为：&lt;/p&gt;
$$
-\mathbb{E}_{t,p_0,r_0,r_t}\left[\frac{1}{t}\sum_{i=1}^{L&#39;}1[r^i_t = M]p_\theta(r_0^i\mid p_0,r_t)\right]
$$&lt;ul&gt;
&lt;li&gt;$r_0$的长度是天然动态的，使用&lt;code&gt;EOS&lt;/code&gt;填充&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;inference&#34;&gt;Inference
&lt;/h3&gt;&lt;p&gt;给定$p_0$，我们从完全掩码的$r_1$开始&lt;/p&gt;
&lt;p&gt;设定超参数如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;迭代次数（采样步骤总数）：a trade off between efficency and quality&lt;/li&gt;
&lt;li&gt;生成长度：实质上是一个上界&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;假设我们从时间$t\in(0, 1]$转移到$s\in[0,t)$，需要做的事是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p_0,r_t$作为模型的输入，预测$r_0$（模型会&lt;code&gt;unmask&lt;/code&gt;所有被掩码的token）&lt;/li&gt;
&lt;li&gt;由于我们只转移到$s$，因此需要保留$sL$个掩码
&lt;ul&gt;
&lt;li&gt;对预测出的$r_0$，从中&lt;strong&gt;随机&lt;/strong&gt;&lt;code&gt;remask&lt;/code&gt;$\frac{s}{t}L$个token&lt;/li&gt;
&lt;li&gt;得到$r_s$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$s = t, r_t = r_s$重复迭代&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;默认将$|t-s|$​是一个定值，以定长的步长进行迭代&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257.png&#34;
	width=&#34;1647&#34;
	height=&#34;627&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_539b4cf9daaad9c4.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_3153327713f2069d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Reverse Process&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;262&#34;
		data-flex-basis=&#34;630px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;理论上remask策略是随机的&lt;/p&gt;
&lt;p&gt;但是论文给出了两种基于&lt;code&gt;退火&lt;/code&gt;的remask策略&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;在生成过程中，需要随机性逐步递减，冻结高确定性的部分、把随机性集中在不确定区域&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;low-confidence remasking&lt;/strong&gt;：取置信度最低的$\frac{s}{t}L$个预测token进行remask&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;semi-autoregressive&lt;/strong&gt; remasking：对序列进行分块，从左到右顺序生成&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789.png&#34;
	width=&#34;1473&#34;
	height=&#34;558&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_c595a78f46bb5a31.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_8789f573c459aaef.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;semi-autoregressive&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;263&#34;
		data-flex-basis=&#34;633px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;h3 id=&#34;实验1--scalability&#34;&gt;实验1 · Scalability
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;验证：LLaDA是否与自回归模型ARM具有相同的可拓展性&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;目的：证明论文的核心论点：&lt;/p&gt;
&lt;p&gt;​	理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布&lt;/p&gt;
&lt;p&gt;​	因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布&lt;/p&gt;
&lt;p&gt;​	因此指令跟随、上下文学习并不是ARM的专利&lt;/p&gt;&lt;/blockquote&gt;
&lt;h4 id=&#34;实验设计&#34;&gt;实验设计
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932.png&#34;
	width=&#34;1568&#34;
	height=&#34;583&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_4f064368af95b6bc.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_8bdc715fd4ddd50c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;架构&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;645px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;针对MDM和ARM两类语言模型，进行如下控制变量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型结构：采样同一套Transformer架构（优化器、参数量……各种机制），只修改了mask&lt;/li&gt;
&lt;li&gt;参数量：在1B规模下完全一致，在7B规模由于资源限制有一些不同
&lt;ul&gt;
&lt;li&gt;Transformer：causal mask&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;数据：预训练语料相同&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;唯一的实验的变量：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FLOPs：使用6ND公式作为横轴
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;N&lt;/strong&gt; 是模型的非 embedding 参数量（固定，比如 1B 或 8B）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;D&lt;/strong&gt; 是训练过的 token 数量（数据量，可以变化）&lt;/li&gt;
&lt;li&gt;实验通过改变 D ，计算出 6 × N × D （即训练 FLOPs）作为横轴的计算预算&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验指标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;MMLU、ARC-C、CMMLU、PIQA、GSM8K、HumanEval&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（多任务、推理、中文、物理、数学、代码）&lt;/p&gt;
&lt;h4 id=&#34;实验结果&#34;&gt;实验结果
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678.png&#34;
	width=&#34;1472&#34;
	height=&#34;707&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_598efe4120660d9c.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_8ec2789b84388b8c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;实验结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;499px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;部分任务体现优势&lt;/li&gt;
&lt;li&gt;对于性能稍逊的任务（PIQA），差距也在逐渐缩小&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;同时喷了先前的一篇工作的结论：&lt;code&gt;达到相同的似然需要16倍算力&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;似然是间接指标（LLaDA的lower bound）&lt;/li&gt;
&lt;li&gt;先前的工作只有GPT2的参数量，本文提高到7-8B&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;结论：LLaDA 在相同训练规模与算力条件下，表现出与 ARM 相似甚至更强的可扩展性。&lt;/p&gt;
&lt;h3 id=&#34;实验2--benchmark&#34;&gt;实验2 · Benchmark
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;验证：LLaDA经过预训练和SFT之后是否能够和已有的ARM在&lt;strong&gt;上下文学习&lt;/strong&gt;与&lt;strong&gt;指令遵循能力&lt;/strong&gt;上进行竞争&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验设计-1&#34;&gt;实验设计
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;实验对象：LLaDA、一系列参数相当的模型
&lt;ul&gt;
&lt;li&gt;Base阶段：比较了所有模型的预训练base模型&lt;/li&gt;
&lt;li&gt;instruct阶段：LLaDA只进行了SFT，其他模型均完成了SFT+RL
&lt;ul&gt;
&lt;li&gt;原文：交给未来的工作&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;任务：通用、数学科学、代码、中文等常见benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;实验结果-1&#34;&gt;实验结果
&lt;/h4&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237.png&#34;
	width=&#34;1208&#34;
	height=&#34;717&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_caa01b5802879200.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_c7856aff322a4d51.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Base&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;168&#34;
		data-flex-basis=&#34;404px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;在所有任务上超过LLaMA2 7B，与LLaMA3 8B相当&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有模型的训练数据存在差异&lt;/li&gt;
&lt;li&gt;作者认为LLaDA的优势区间与劣势区间的主要原因在数据质量与分布上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GSM8K数据集上体现了显著的优势，论文针对这个情况做了补充实验，证明不存在数据泄露&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440.png&#34;
	width=&#34;1205&#34;
	height=&#34;574&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_db0f9cb7c5edf55e.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_5dfaa65484da0854.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;SFT&amp;#43;RL&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;209&#34;
		data-flex-basis=&#34;503px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SFT数据质量较差，出现了性能下降（MMLU）&lt;/li&gt;
&lt;li&gt;没有采用RL，因此性能略微落后LLaMA3 8B&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在数据集透明度不足的情况下，以丰富的标准化流程、多样化任务，足以证明LLaDA的性能卓越，是唯一具备竞争力的非自回归模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验2--补充实验&#34;&gt;实验2 · 补充实验
&lt;/h3&gt;&lt;p&gt;验证：LLaDA在GSM8K数据集中的优势不来源于数据泄露（data leakage），检测在全新数据集中仍然能保证推理能力&lt;/p&gt;
&lt;p&gt;省流：找了一个2024年的新数据集，模仿GSM8K的形式做一遍实验&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145.png&#34;
	width=&#34;548&#34;
	height=&#34;173&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_ad845088f2d886d0.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_8a04e4c9a021371f.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;iGSM Dataset&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;316&#34;
		data-flex-basis=&#34;760px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA在所有难度（解题步骤数）中均显著优势&lt;/li&gt;
&lt;li&gt;两类模型随着难度上升准确度逐渐下降，但是LLaDA下降较慢&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA允许模型在每一步同时考虑全局 token 关系，因此在多变量方程、层次关系推理中优于单向自回归&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;实验3--reversal-reasoning-and-analyses&#34;&gt;实验3 · Reversal Reasoning and Analyses
&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;Reversal Curse（反向诅咒）：ARM从左到右生成序列，因此反向生成或逆序推理的表现很差&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;验证：LLaDA是否克服了反向诅咒&lt;/p&gt;
&lt;h4 id=&#34;实验设计-2&#34;&gt;实验设计
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;数据：496对著名中文诗句（上下两句），每一句子（A,B）构成两个任务
&lt;ul&gt;
&lt;li&gt;Forward：给定A预测B&lt;/li&gt;
&lt;li&gt;Backward：给定B预测A&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;窈窕淑女的下一句是什么？直接输出句子即可。 Answer: 君子好逑。
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;不拘一格降人才的上一句是什么？直接输出句子即可。 Answer: 我劝天公重抖擞。
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318.png&#34;
	width=&#34;990&#34;
	height=&#34;401&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_f6b1b119ec68bdcd.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_9284f82611816e94.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Reversal Curse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;246&#34;
		data-flex-basis=&#34;592px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;GPT-4o 和 Qwen2.5 均有更大数据和RL优化，但仍失败&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;LLaDA 虽仅 SFT，无RL，仍在 reversal 上大幅领先&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;附录补充&#34;&gt;附录补充
&lt;/h4&gt;&lt;p&gt;论文从三个角度补充了为什么LLaDA是无方向偏置的模型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;理论证明：LLaDA本质上等价于在所有生成顺序上做平均，从而消除方向偏置
&lt;ul&gt;
&lt;li&gt;解释为什么 diffusion 结构在数学上是方向对称的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;实现机制：理论正确的情况下，需要确保算法实现不出现&lt;strong&gt;从左到右&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;确保生成算法本身不引入方向信息&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;超参数层面：通过实验说明采样步数与效率不会干扰方向一致性
&lt;ul&gt;
&lt;li&gt;排除方向性差异由采样精度造成的可能性&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h5 id=&#34;a2-inference&#34;&gt;A.2 Inference
&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;目的：证明LLaDA训练和推理目标等价于&lt;strong&gt;对所有可能生成顺序的平均建模&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于训练时的核心目标函数：&lt;/p&gt;
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$&lt;p&gt;
训练目标是：模型在任意mask模式下，都能预测出原token&lt;/p&gt;
&lt;p&gt;该训练模式不会看到任何固定方向的序列，故天然是双向建模的&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;推不动了，pass一下&lt;/p&gt;&lt;/blockquote&gt;
&lt;h5 id=&#34;remasking&#34;&gt;Remasking
&lt;/h5&gt;&lt;p&gt;反向过程的核心：预测 - 重新掩码 - 继续预测&lt;/p&gt;
&lt;p&gt;上文提到了三种不同的掩码策略&lt;/p&gt;
&lt;p&gt;论文使用GSM8K进行了消融实验&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;生成长度固定512&lt;/li&gt;
&lt;li&gt;采样步数固定256（长度的一半）&lt;/li&gt;
&lt;li&gt;block：32&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549.png&#34;
	width=&#34;1329&#34;
	height=&#34;223&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_a185194ec8c04b53.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_9209def52fe04210.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;remask&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;595&#34;
		data-flex-basis=&#34;1430px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Base模型：最低置信度即可，半自回归是不需要的&lt;/li&gt;
&lt;li&gt;Instruct模型：必须是最低置信度+半自回归
&lt;ul&gt;
&lt;li&gt;单独最低置信度会严重降低性能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;论文解释：SFT阶段引入了大量&lt;code&gt;EOS&lt;/code&gt;，模型一般会给&lt;code&gt;EOS&lt;/code&gt;较大的置信度。因此推理时&lt;code&gt;EOS&lt;/code&gt;会被大量生成，并且几乎不可能被&lt;code&gt;remask&lt;/code&gt;（置信度非常高）&lt;/p&gt;
&lt;p&gt;因此需要引入半自回归，保证每个块内收敛出连续的内容，抑制&lt;code&gt;EOS&lt;/code&gt;早产&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;尽管引入半自回归，但是块内仍然是并行的（？）&lt;/p&gt;
&lt;p&gt;补充一下，模型对生成长度这个超参数非常不敏感&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980.png&#34;
	width=&#34;1607&#34;
	height=&#34;288&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_496709f8b88b2255.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_4144e169c1540880.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Length&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;557&#34;
		data-flex-basis=&#34;1339px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;但对采样步数非常敏感（生成长度1024）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929.png&#34;
	width=&#34;1472&#34;
	height=&#34;508&#34;
	srcset=&#34;https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_afc64e86f9a1bafe.png 480w, https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_6b0c69ba2a7f9c56.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;采样&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;289&#34;
		data-flex-basis=&#34;695px&#34;
	
&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;结论：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLaDA不受自回归方向性的约束，具有更平衡的前后向建模能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;case-studies&#34;&gt;Case Studies
&lt;/h3&gt;&lt;p&gt;附录中展示了一些其他例子，说明生成的对话是出色的（单轮、多轮）&lt;/p&gt;
</description>
        </item>
        <item>
        <title>大语言模型教育纠错 · 论文笔记（三）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/</link>
        <pubDate>Sat, 06 Sep 2025 20:45:55 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.alphaxiv.org/abs/2412.09416v2&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors | alphaXiv&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/kaushal0494/UnifyingAITutorEvaluation&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;kaushal0494/UnifyingAITutorEvaluation: An Evaluation Taxonomy for Pedagogical Ability Assessment of LLM-Powered AI Tutors&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;paper-cool&#34;&gt;Paper-Cool
&lt;/h2&gt;&lt;h3 id=&#34;q1-这篇论文试图解决什么问题&#34;&gt;&lt;strong&gt;Q1&lt;/strong&gt;: 这篇论文试图解决什么问题？
&lt;/h3&gt;&lt;p&gt;这篇论文试图解决的问题是如何有效地评估大型语言模型（LLMs）作为AI助教在教育对话中的表现，特别是在学生错误纠正方面。具体来说，论文试图解决以下几个问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;评估标准的统一性&lt;/strong&gt;：之前对AI助教的评估工作受限于主观协议和基准，缺乏统一的评估框架。论文提出了一个基于学习科学原则的统一评估分类体系，包含八个教学维度，旨在评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估的客观性和可靠性&lt;/strong&gt;：现有的评估方法往往依赖于主观评价，缺乏客观性和可比性。论文通过提出一个新的评估基准MRBench，并提供人工标注的八个教学维度，来提高评估的客观性和可靠性。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AI助教的实际教学能力&lt;/strong&gt;：论文通过对比不同LLMs作为AI助教的能力和人类助教的表现，揭示了当前LLMs在教学能力上的不足，并指出了改进的方向。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估过程的简化和标准化&lt;/strong&gt;：论文希望通过提出的分类体系、基准和人工标注的标签，简化AI助教的评估过程，并帮助跟踪AI助教发展的进步。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总的来说，这篇论文旨在通过提出一个新的评估框架和基准，来解决当前AI助教评估中存在的问题，推动AI助教的发展，并为未来的研究提供便利。&lt;/p&gt;
&lt;h3 id=&#34;q2-有哪些相关研究&#34;&gt;&lt;strong&gt;Q2&lt;/strong&gt;: 有哪些相关研究？
&lt;/h3&gt;&lt;p&gt;相关研究涵盖了以下几个领域：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;AI助教评估&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;Tack和Piech (2022) 以及 Tack 等人 (2023) 从教师语言、学生理解和帮助学生的角度评估模型的响应。&lt;/li&gt;
&lt;li&gt;Macina 等人 (2023) 专注于连贯性、正确性和公平教学来评估模型作为教师的响应。&lt;/li&gt;
&lt;li&gt;Wang 等人 (2024a) 评估了有用性、关怀和人类化。&lt;/li&gt;
&lt;li&gt;Daheim 等人 (2024) 集中于针对性、正确性和可执行性来评估教师响应的质量。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自然语言生成（NLG）和LLM基础评估&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;一般领域的NLG指标，如BLEU (Papineni et al., 2002)、BERTScore (Lin, 2004) 和 DialogRPT (Gao et al., 2020) 等，被用作衡量AI助教响应的连贯性和类人性，但这些指标不考虑教学价值，且常需要真实答案来评估匹配响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;面向教学的评估&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;传统学习科学中的评估方法主要设计用于评估人类教师，由于缺乏自我报告，不易直接应用于AI教师。&lt;/li&gt;
&lt;li&gt;研究人员考虑不同的教学维度及其相关定义 (Wollny et al., 2021; Tack et al., 2023; Denny et al., 2024)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学生错误纠正任务&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;专注于数学领域的教育对话，特别是学生的失误或困惑，AI教师的目标是回应以纠正这些错误或困惑。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些相关研究提供了评估AI助教的不同视角和方法，但存在评估方案和定义的差异，缺乏标准化，使得跟踪现有AI教师的进步和实际性能变得复杂。这篇论文提出的评估分类体系旨在统一这些评估方法，并提供一个基于学习科学原则的评估框架。&lt;/p&gt;
&lt;h3 id=&#34;q3-论文如何解决这个问题&#34;&gt;&lt;strong&gt;Q3&lt;/strong&gt;: 论文如何解决这个问题？
&lt;/h3&gt;&lt;p&gt;论文通过以下几个步骤解决AI助教评估的问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;提出统一评估分类体系&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文提出了一个基于学习科学原则的统一评估分类体系，包含八个教学维度，用于评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;构建评估基准MRBench&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文编译并发布了一个新的评估基准MRBench，该基准包含来自两个公共数据集的192个对话和1596个响应，这些响应来自七个最先进的基于LLM的和人类助教，并提供了八个教学维度的黄金标注。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类和LLM评估&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;利用部分对话历史，展示了学生的失误，生成了七个最先进的LLM作为助教的响应，并通过人类和LLM基础评估来评估这些模型的教学能力。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分析LLM作为评估者的可靠性&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文探讨了LLM作为评估者的可靠性，通过将LLM生成的评估与人类判断进行相关性分析，指出LLM在复杂教学维度上的评估是不可靠的。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;讨论改进方向&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文基于人类评估指出了LLMs在响应中表现出的特定教学能力的不足，并讨论了改进AI助教教学能力的最相关方向。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;资源公开&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文承诺将公开分类体系、基准和人类标注，以促进未来在这一重要领域的研究。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这些步骤，论文旨在简化AI助教的评估过程，并帮助跟踪AI助教发展的进步。此外，该研究还为创建和标注可用于强化学习和微调的数据集提供了可能性，帮助未来的AI助教与人类和教学价值观保持一致。&lt;/p&gt;
&lt;h3 id=&#34;q4-论文做了哪些实验&#34;&gt;&lt;strong&gt;Q4&lt;/strong&gt;: 论文做了哪些实验？
&lt;/h3&gt;&lt;p&gt;论文中进行了一系列实验来验证提出的评估分类体系的有效性，并评估不同LLM作为AI助教的教学能力。以下是主要的实验步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;评估分类体系验证&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;进行了验证实验，以确定提出的八个评估维度是否足够且无冗余。实验包括四名注释者对八个随机抽样的对话进行独立标注，并计算Fleiss&amp;rsquo; kappa值来衡量注释者间的一致性，结果表明有实质性的一致性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;基准准备&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;从Bridge和MathDial数据集中编译了MRBench评估基准，并为192个实例生成了基于七个最先进的LLM的响应。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类标注&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;四名训练有素的标注者使用验证过的分类体系对MRBench进行了标注，每个标注者负责48个对话，总共192个实例被标注，其中40个实例由两名标注者独立标注，以计算标注者间的一致性，平均Cohen&amp;rsquo;s kappa得分为0.71，表明有实质性的一致性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LLM基础标注&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;使用Prometheus2 LLM作为评估者对MRBench进行标注，以评估LLM作为评估者的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估指标&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;使用期望标注匹配率（DAMR）和标注相关性（AC）两个关键指标来定量评估LLM和人类教师的教学效果，并进行比较分析。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;关键发现&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;基于人类和LLM对MRBench的评估，总结了不同LLM作为AI助教的教学能力的关键发现，并讨论了LLM作为评估者的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些实验步骤不仅验证了提出的评估分类体系的有效性，而且提供了对不同LLM在教学能力方面的深入分析，并探讨了LLM作为评估者的可行性。通过这些实验，论文展示了当前LLM作为AI助教的能力和局限性，并指出了未来改进的方向。&lt;/p&gt;
&lt;h3 id=&#34;q5-有什么可以进一步探索的点&#34;&gt;&lt;strong&gt;Q5&lt;/strong&gt;: 有什么可以进一步探索的点？
&lt;/h3&gt;&lt;p&gt;论文在最后部分提出了一些可以进一步探索的点，这些包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;扩展到其他任务和学科领域&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文提出的评估分类体系主要集中在数学领域的学生错误纠正任务上。未来的研究可以验证并可能适应这个分类体系，以应用于其他任务（如概念学习）和学科（如科学、语言学习等）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;考虑学生视角&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;目前的分类体系和标注方案关注于教师响应的适当性，但未考虑辅导对话对学生学习过程和学习成果的整体影响。未来的研究可以将评估扩展到对话层面，以更好地评估AI教师的教学能力对学生学习的影响。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;使用其他LLM作为评估者&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文中限制了LLM基础评估到Prometheus2 LLM作为批评者的使用。未来的研究可以探索更多先进的LLM作为批评者，并尝试使用多种提示模板。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建和标注数据集&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文提到，创建和标注可用于强化学习和微调的数据集，可以帮助未来的AI教师与人类和教学价值观保持一致。这是一个重要的研究方向，可以促进AI教师的发展。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;改进LLM的教学能力&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文的研究发现，即使是最先进的LLM也缺乏一些有效的教学能力。未来的研究可以探索如何改进LLMs，使其更符合教学的有效性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估LLM生成内容的伦理和风险&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;考虑到LLM可能生成事实上不正确、无意义或甚至冒犯的内容，未来的研究需要考虑评估LLM在教育领域使用时的伦理风险，并探索如何减轻这些问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这些探索点为AI助教评估和开发的未来研究提供了明确的方向，并强调了需要进一步工作以提高AI教师的教学效果和可靠性。&lt;/p&gt;
&lt;h3 id=&#34;q6-总结一下论文的主要内容&#34;&gt;&lt;strong&gt;Q6&lt;/strong&gt;: 总结一下论文的主要内容
&lt;/h3&gt;&lt;p&gt;这篇论文的主要内容包括以下几个方面：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;研究动机&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文探讨了当前最先进的大型语言模型（LLMs）是否能够作为有效的AI助教，并展示必要的教学能力，特别是在教育对话中纠正学生错误和困惑的场景。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估分类体系&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;提出了一个统一的评估分类体系，包含八个基于学习科学原则的教学维度，用于评估LLM驱动的AI助教在数学领域的学生错误纠正中的教学价值。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;评估基准MRBench&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;发布了一个新的评估基准MRBench，包含192个对话和1596个响应，这些响应来自七个最先进的基于LLM的和人类助教，并提供了八个教学维度的黄金标注。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;人类和LLM评估&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;通过人类和LLM基础评估，分析了不同LLM作为AI助教的教学能力，并讨论了LLM作为评估者的可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;研究发现&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;发现尽管最先进的LLMs如GPT-4在回答问题方面表现良好，但作为教师时往往不够胜任，特别是在不立即透露答案和提供指导方面。&lt;/li&gt;
&lt;li&gt;Llama-3.1-405B在评估中表现最好，而GPT-4作为教师时太快透露答案，Gemini的响应不够连贯，Sonnet虽然注重人类化和鼓励性，但在其他维度上效果较差，Phi3的表现最差。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结论和未来工作&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文得出结论，当前的LLMs作为AI教师还不够好，有很大的改进空间，并指出了最相关的改进方向。&lt;/li&gt;
&lt;li&gt;论文希望发布的资源能够简化评估过程，并帮助跟踪AI教师的发展。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;局限性和伦理声明&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;论文承认了研究的局限性，并强调了LLM输出可能带来的风险，尤其是在教育领域可能误导学生和传播偏见的问题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这篇论文为AI助教的评估提供了一个统一的框架和基准，并通过对现有技术的评估，揭示了AI助教在教学能力上的不足和未来改进的方向。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>大语言模型教育纠错 · 论文笔记（二）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/</link>
        <pubDate>Wed, 16 Jul 2025 02:45:55 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2406.19949&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2406.19949 Calibrating LLMs with Preference Optimization on Thought Trees for Generating Rationale in Science Question Scoring&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;p&gt;存在的问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;基于分类器的黑盒方法虽然准确，但无法提供解释性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现有生成评分理由的方法（如AERA框架(&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2305.12962&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2305.12962 Distilling ChatGPT for Explainable Automated Student Answer Assessment&lt;/a&gt;）存在以下问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;评分准确性不如分类器方法。&lt;/li&gt;
&lt;li&gt;生成的评分理由可能不忠实于学生答案或评分标准。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;paper贡献&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出一种新框架，通过模仿人类评分过程生成更忠实的评分理由，同时匹配或超越分类器方法的评分性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;paper方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;模仿人类评分过程&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用大语言模型（LLM）生成“思维树”（Thought Tree），将评分任务分解为中间决策步骤。&lt;/li&gt;
&lt;li&gt;每条树路径代表一个评分决策序列，最终汇总为评分理由。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;合成数据生成&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从思维树路径中提取合成评分理由和偏好数据。&lt;/li&gt;
&lt;li&gt;通过两阶段训练校准LLM：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;监督微调（SFT）&lt;/strong&gt;：使用合成的评分理由数据。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;偏好优化（DPO）&lt;/strong&gt;：使用合成的偏好数据，提升评分理由的准确性和忠实性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;paper贡献&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;提出通过思维树生成更忠实的评分理由的方法。&lt;/li&gt;
&lt;li&gt;开发基于思维树路径正确性的合成偏好数据生成技术。&lt;/li&gt;
&lt;li&gt;实验表明，框架在QWK分数上比现有方法提升38%，同时生成更高质量的评分理由。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;framework&#34;&gt;Framework
&lt;/h2&gt;&lt;h3 id=&#34;problem-set-up&#34;&gt;Problem Set Up
&lt;/h3&gt;$$
D = \left \{ (x_i, y_i)\right \}
$$&lt;p&gt;
表示学生$i$对某道题目的答案与得分&lt;/p&gt;
&lt;p&gt;对于一道题目，可以划分出$M$个关键得分点$K = \left {k_j\right }$&lt;/p&gt;
$$
v(x_i, K)
$$&lt;p&gt;
该向量的第$j$维度若为1，则表示$x_i$成功回答了$k_j$，否则没有成功回答&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;独热向量 (One-hot Vector)&lt;/strong&gt;：用于表示一个样本&lt;strong&gt;只属于一个类别&lt;/strong&gt;的情况。例如，如果一个动物只能是“- 猫”或“狗”中的一种，那么“猫”可能表示为 &lt;code&gt;[1, 0]&lt;/code&gt;，“狗”表示为 &lt;code&gt;[0, 1]&lt;/code&gt;。向量中&lt;strong&gt;只有一个 1&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多热向量 (Multi-hot Vector)&lt;/strong&gt;：用于表示一个样本&lt;strong&gt;可以同时属于多个类别&lt;/strong&gt;的情况。例如，一个人既是“学生”又是“运动员”，那么可能表示为 &lt;code&gt;[1, 1, 0]&lt;/code&gt;（假设第一个位置是学生，第二个是运动员）。向量中可以有&lt;strong&gt;多个 1&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
$$
y_i = f_r(v(x_i,K))
$$&lt;p&gt;
省流：问题的关键在如何判断$x_i$正确回答了$k_j$，记为$1_{x_i}(k_j)$，是一个二分类任务&lt;/p&gt;
&lt;h3 id=&#34;stage-1-imitate-human-assessment-process-via-thought-trees&#34;&gt;Stage 1: Imitate Human Assessment Process via Thought Trees
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250716031327371.png&#34;
	width=&#34;961&#34;
	height=&#34;611&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250716031327371_hu_72b7ba402e41067b.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250716031327371_hu_9fdc14ddcae9f199.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Thought Trees&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;157&#34;
		data-flex-basis=&#34;377px&#34;
	
&gt;&lt;/p&gt;
$$
z_j^{(t)} = \text{LLM}_\theta(x_i, k_j), \quad t=1,2,...,n,\ \forall k_j \in K
$$&lt;p&gt;
$z_j^{(t)}$ 表示第 $t$ 次采样的决策&lt;/p&gt;
&lt;p&gt;我们将这些判定结果汇总，得到如下的平均决策概率：&lt;/p&gt;
$$
P(z_j^{\text{Yes}}) = \frac{|\{t : z_j^{(t)} = 1\}|}{n},\quad
P(z_j^{\text{No}}) = \frac{|\{t : z_j^{(t)} = 0\}|}{n}
$$&lt;p&gt;
为了后续聚合，我们为每个关键要素生成简洁的解释性理由 $r_j$，例如：&lt;/p&gt;
$$
r_j = \text{LLM}_\theta(x_i, k_j, z_j)
$$&lt;p&gt;
一旦所有关键要素评估完成，我们便能根据每一组决策 $\mathbf{Z}$ 构造路径。假设总共 $d$ 条路径（最多 $2^{M-1}$ 条），每条路径表示一种判定组合：&lt;/p&gt;
$$
\text{path}_l = \hat{\mathbf{v}}(\mathbf{Z}),\quad l=1,2,...,d
$$&lt;p&gt;其中，$\hat{\mathbf{v}}$ 是对向量 $\mathbf{v}$ 的估计，表示关键要素是否被覆盖；$\mathbf{Z}$ 是判定集合（由上面的判定概率组成）。路径的概率等于该路径上每个判定概率的乘积：&lt;/p&gt;
$$
P(\text{path}_l) = \prod_{j=1}^{M} P(z_j)
$$&lt;p&gt;
我们利用打分函数 $f_r$（例如程序化的rubric规则）对每条路径打分，获得预测得分：&lt;/p&gt;
$$
\hat{y}_{\text{path}_l} = f_r(\text{path}_l)
$$&lt;blockquote&gt;
&lt;p&gt;省流：蒙特卡洛树，从中抽取所有路径，计算概率和对应的分数，选择概率最高的&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;最后，选取概率最高的路径作为最终思维树输出结果：&lt;/p&gt;
$$
\hat{y}_{\text{tree}} = \hat{y}_{\text{path}_{l^*}} \quad l^* = \arg\max_l P(\text{path}_l)
$$&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;在实际操作中，我们使用LLM动态地将rubric文本转换为可执行Python代码，该代码以关键要素评估决策为输入，输出最终分数。&amp;rdquo;&lt;/p&gt;
&lt;p&gt;人话：LLM会将打分标准生成Python代码，直接传入关键要素的多热向量就可以直接算分&lt;/p&gt;&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>大语言模型数据清洗 · 论文笔记（五）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/</link>
        <pubDate>Thu, 10 Jul 2025 11:16:32 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&#34;essential-web-v10-24t-tokens-of-organized-web-data&#34;&gt;Essential-Web v1.0 24T tokens of organized web data
&lt;/h1&gt;&lt;h2 id=&#34;preview&#34;&gt;Preview
&lt;/h2&gt;&lt;p&gt;构建了多维度的分类体系，适合通过SQL等方式进行数据筛选出新的数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用开源模型进行数据标签的标注，得到了EAI-Distill-0.5b&lt;/li&gt;
&lt;li&gt;推理清洗了23.6B的数据，花费了90000 AMD MI300x GPU-hours&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;The inference job ran on 512 AMD MI300x for about 1 week.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;分类体系：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个有限的类别集合 $T=\left { C_1, C_2, &amp;hellip;, C_k \right } $。&lt;/li&gt;
&lt;li&gt;每个类别$C_i$都有一个非空、有限的标签集$L_i$。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;标注形式为$T(d) = \left { (\lambda_1, \mu_1), &amp;hellip;\right}$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;其中，$λ_i\in L_i$ 是类别$C_i$的主要标签&lt;/li&gt;
&lt;li&gt;$\mu_i \in (L_i \setminus {\lambda_i}) \cup {\bot}$ 是一个可选的次要标签，必须与$\lambda_i$不同
&lt;ul&gt;
&lt;li&gt;当文档适合两个标签时非常有用&lt;/li&gt;
&lt;li&gt;$\bot$表示弃权（abstention）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;所有类别和标签集都是预先固定的，这允许训练一个单一的静态分类器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710141259250.png&#34;
	width=&#34;923&#34;
	height=&#34;234&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710141259250_hu_35e32e8ea56e9d9f.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710141259250_hu_7dca944e0ae04e92.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;PipeLine&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;394&#34;
		data-flex-basis=&#34;946px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;实验设置&#34;&gt;实验设置
&lt;/h2&gt;&lt;blockquote&gt;
&lt;h2 id=&#34;chinchilla最优计算比例&#34;&gt;Chinchilla最优计算比例
&lt;/h2&gt;&lt;p&gt;Chinchilla缩放定律发现了一个最优比例：大约每个参数需要20个训练token &lt;a class=&#34;link&#34; href=&#34;https://epoch.ai/blog/chinchilla-scaling-a-replication-attempt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Epoch AI&lt;/a&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.analyticsvidhya.com/blog/2024/09/chinchilla-scaling-law/&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Analytics Vidhya&lt;/a&gt;。这个比例是DeepMind通过训练400多个语言模型得出的计算最优配置。&lt;/p&gt;
&lt;p&gt;具体来说：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Chinchilla模型有70B参数，在1.4万亿tokens上训练，达到20 tokens per parameter的比例 &lt;a class=&#34;link&#34; href=&#34;https://epoch.ai/blog/chinchilla-scaling-a-replication-attempt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Chinchilla Scaling: A Replication Attempt | Epoch AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;这个20:1的比例被认为是在给定计算预算下实现最佳性能的理想配置&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710142401273.png&#34;
	width=&#34;608&#34;
	height=&#34;269&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710142401273_hu_1c12a53edcfe7eaf.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%94/assets/image-20250710142401273_hu_1c71ffbb1c3f701e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Train&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;226&#34;
		data-flex-basis=&#34;542px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;所有数据集在训练前均使用了 13-gram Bloom Filter&lt;/p&gt;
&lt;p&gt;选用了两个2.3B模型对数据进行评估&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;预训练（3200亿Token）：该阶段帮助模型学到广泛的语言知识&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;General-base：仅使用网络数据（DCLM-baseline）做&lt;strong&gt;预训练&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Code-base：使用网络数据（DCLM-baseline）+代码数据（Stack v2 Dedup中的Python），各占50%做&lt;strong&gt;预训练&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;退火（800亿token）：为了评估特定领域数据集的性能，采用需要评估的新数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习率接近零的目的是在新的领域数据上进行“微调”，而不是进行大规模的“重新训练”&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;每个模型总计处理 4000 亿 token 数据量，是Chinchilla的10倍数据&lt;/p&gt;
&lt;h2 id=&#34;蒸馏&#34;&gt;蒸馏
&lt;/h2&gt;&lt;h3 id=&#34;蒸馏方案&#34;&gt;蒸馏方案
&lt;/h3&gt;&lt;h4 id=&#34;数据来源与规模&#34;&gt;&lt;strong&gt;数据来源与规模&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;标注数据&lt;/strong&gt;：使用Qwen2.5-32B-Instruct对104.6M文档共82Btoken进行两轮标注，生成合成标签用于蒸馏训练。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;第一轮&lt;/strong&gt;：标注8个分类类别（如FDC、Document Type V1/V2等）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第二轮&lt;/strong&gt;：扩展至12个类别（新增Bloom、Technical Correctness等）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;数据预处理&#34;&gt;&lt;strong&gt;数据预处理&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;子采样&lt;/strong&gt;：对超过30,000字符的文档，截取开头、随机中间段和结尾（Algorithm 12），避免长文本影响推理速度。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;质量过滤&lt;/strong&gt;：通过统计和模型信号（如DCLM分类器）过滤低质量文档（Algorithm 1）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;模型架构&#34;&gt;&lt;strong&gt;模型架构&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;基础模型&lt;/strong&gt;：Qwen2.5-0.5b-Instruct（5亿参数），基于Gemma 3架构，使用QK-norm稳定注意力。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;序列长度&lt;/strong&gt;：16,384 tokens，支持长上下文。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;训练参数&#34;&gt;&lt;strong&gt;训练参数&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;优化器&lt;/strong&gt;：AdamW（β1=0.9, β2=0.95），权重衰减0.1。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习率&lt;/strong&gt;：峰值1e-4，线性预热2B tokens，余弦衰减至1e-5，最后线性退火至0。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;批量大小&lt;/strong&gt;：全局2M tokens，梯度累积实现大批次训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练量&lt;/strong&gt;：82B tokens（合成标签数据）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;损失计算&lt;/strong&gt;：仅对教师模型生成的标签token计算损失，输入文档和系统提示被掩码。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;教师模型选择&#34;&gt;&lt;strong&gt;教师模型选择&lt;/strong&gt;
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;教师&lt;/strong&gt;：Qwen2.5-32B-Instruct，因其标注一致性（κ=0.74）与推理速度平衡（1.4 RPS/GPU）。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;蒸馏步骤&#34;&gt;&lt;strong&gt;蒸馏步骤&lt;/strong&gt;
&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;标签生成&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;教师模型生成多分类标签（如FDC层级、Document Type等），格式为&lt;code&gt;主标签,次标签&lt;/code&gt;（Algorithm 13）。&lt;/li&gt;
&lt;li&gt;压缩输出：从平均791 tokens缩短至51 tokens，提升推理速度50倍（Table 12）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;上下文蒸馏&lt;/strong&gt;：
&lt;ul&gt;
&lt;li&gt;移除教师模型的提示模板（Prompt 1/2），直接训练学生模型生成压缩格式标签。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;评估方案&#34;&gt;评估方案
&lt;/h3&gt;&lt;h4 id=&#34;metrics&#34;&gt;Metrics
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;正确性：多人分类的结果应该类似，验证模型打标签是否标准一致
&lt;ul&gt;
&lt;li&gt;使用GPT-4o和Claude Sonnet-3.5作为专家模型&lt;/li&gt;
&lt;li&gt;使用kappa系数作为指标&lt;/li&gt;
&lt;li&gt;取对4o的系数和对claude的系数的均值作为结果&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;检测方式是验证模型与专家模型的标准是否一致，对于指标paper中进行了变种&lt;/p&gt;
&lt;p&gt;对于某个模型的分类结果$S\in \left { \phi, (\text{label}), (\text{label1, label2}) \right }$​&lt;/p&gt;
&lt;p&gt;最少是一个标签（主标签），有时可以加一个次标签&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标注结果一致的判定：两模型的$S$​有交集&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然后套公式&lt;/p&gt;
&lt;p&gt;Qwen2.5-32B-Instruct ≈ 0.74&lt;/p&gt;
&lt;p&gt;EAI-Distill-0.5b ≈ 0.71~0.73&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;正交性：不同分类体系之间的标签应该是独立的
&lt;ul&gt;
&lt;li&gt;例如在某分类A下打了a，分类B始终是b，发生了绑定&lt;/li&gt;
&lt;li&gt;需计算互信息、香农熵&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
$$
&gt;\text{NMI}(X, Y) = \frac{2 I(X; Y)}{H(X) + H(Y)}
&gt;$$&lt;p&gt;其中：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$p(x)$按$x$出现的频率，$p(x,y)$按$x,y$同时出现的频率计算&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
&lt;li&gt;$I(X; Y)$：互信息&lt;/li&gt;
&lt;/ul&gt;
$$
&gt;  I(X; Y) = \sum_{x, y} p(x, y) \cdot \log \frac{p(x, y)}{p(x)p(y)}
&gt;  $$&lt;ul&gt;
&lt;li&gt;$H(X)$：X 的香农熵&lt;/li&gt;
&lt;/ul&gt;
$$
&gt;  H(X) = -\sum_{x} p(x) \cdot \log p(x)
&gt;  $$&lt;p&gt;Qwen2.5-32B 平均 NMI ≈ 0.079&lt;/p&gt;
&lt;p&gt;EAI-Distill-0.5b 平均 NMI ≈ 0.092&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Domain Recall
&lt;ul&gt;
&lt;li&gt;定了Golden URL（认为arxiv和……30 个 base URL的都是高质量数据）&lt;/li&gt;
&lt;li&gt;统计有多少能被模型召回&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;dataset&#34;&gt;Dataset
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Random Set：随机采样（需要避免撞车训练数据）&lt;/li&gt;
&lt;li&gt;STEM Set：从特定领域集合（科学领域）随机采样&lt;/li&gt;
&lt;li&gt;通过Golden URL采样&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>大语言模型教育纠错 · 论文笔记（一）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
        <pubDate>Mon, 23 Jun 2025 15:13:55 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2412.16838&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2412.16838 Ask-Before-Detection: Identifying and Mitigating Conformity Bias in LLM-Powered Error Detector for Math Word Problem Solutions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;intro&#34;&gt;Intro
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;AED（Automatic Error Detection）
&lt;ul&gt;
&lt;li&gt;本文定义为：给定问题-解答的输入对，识别错误步骤以及错误类型&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623164618186.png&#34;
	width=&#34;1119&#34;
	height=&#34;594&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623164618186_hu_529466cf46b0a084.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623164618186_hu_44cdd8acc43cea4e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;188&#34;
		data-flex-basis=&#34;452px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;如图，paper点出传统的方法使用可以对问题的&lt;strong&gt;常规解法&lt;/strong&gt;进行正确错误检测&lt;/p&gt;
&lt;p&gt;但是单个问题的解法可以&lt;strong&gt;存在多个&lt;/strong&gt;，认为之前的做法泛用性较差&lt;/p&gt;
&lt;p&gt;常规解法与非常规解法会产生7%的性能差距，先进的闭源模型也无法避免&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLM错误检测器表现&lt;code&gt;conformity bias&lt;/code&gt;（从众偏差）
&lt;ul&gt;
&lt;li&gt;倾向“遵循主流答案（训练中经常出现的）”而忽略可能也正确但不常见的其他解法&lt;/li&gt;
&lt;li&gt;导致模型对标准答案的识别准确，却对非常规解法的识别薄弱&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文针对缓解模型的&lt;code&gt;conformity bias&lt;/code&gt;进行工作&lt;/p&gt;
&lt;p&gt;提出AskBD框架，为每个Solution自适应生成参考答案（合适的参考答案能显著提升性能）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接调用模型，无微调，拓展性强&lt;/li&gt;
&lt;li&gt;自适应方式高度契合给定Solution，降低Bias&lt;/li&gt;
&lt;li&gt;框架可协同CoT技术增强性能&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;preliminary-study&#34;&gt;Preliminary Study
&lt;/h2&gt;&lt;p&gt;Paper构建了一个Alternative Solution数据集用于充分暴露模型的从众偏差效应，帮助进行后续的探索&lt;/p&gt;
&lt;h3 id=&#34;automatic-solution-permutation&#34;&gt;Automatic Solution Permutation
&lt;/h3&gt;&lt;p&gt;paper希望构建一个高质量的Alternative Solution数据集&lt;/p&gt;
&lt;p&gt;给定问题和Solution，希望替换掉整个解决方案为Alternative Solution&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;低质量&lt;/strong&gt;：对常规Solution只是简单的&lt;strong&gt;语义替换&lt;/strong&gt;，并没有深层的逻辑变换&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623172108657.png&#34;
	width=&#34;1094&#34;
	height=&#34;316&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623172108657_hu_c5b1e501f2aed034.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250623172108657_hu_c24cb789034894c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;346&#34;
		data-flex-basis=&#34;830px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ASP&lt;/strong&gt;（自动解法置换），如图，对应了Solution和数学表达式的关系，使用LLM prompt独立执行：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extract：常规Solution -&amp;gt; 数学表达式，完成后需要执行运算，检查是否能够得到正确计算结果，否则剔除&lt;/li&gt;
&lt;li&gt;Permute：因式分解、分配律，重新排列表达式（同样需要运算检验）&lt;/li&gt;
&lt;li&gt;Explain：置换后的表达式输入到LLM，引导生成高质量Alternative Solution&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;paper采样GPT-4o，从GSM8K数据集抽取200组问答对，构建常规数据集D&lt;/p&gt;
&lt;p&gt;对D中的每个样本，3次ASP生成3个Alternative Solution，由教育系研究生评审质量，选择三个之中最优的一个&lt;/p&gt;
&lt;p&gt;完成替换数据集D&amp;rsquo;的制作&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;erroneous-solution-generation&#34;&gt;Erroneous Solution Generation
&lt;/h3&gt;&lt;p&gt;需要将错误注入到D和D‘之中，生成测试样本&lt;/p&gt;
&lt;p&gt;主要参考：&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2406.00755&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2406.00755 Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;paper引入了四种错误：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\varepsilon_C$：calculation errors
&lt;ul&gt;
&lt;li&gt;Operands in expressions are correct but an error occurs in the calculated results.（表达式正确，计算结果出错）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Each gust blows the leaf forward 5 feet, so 11 gusts will blow it forward 5 ×11 = 50 feet. &lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\varepsilon_R$：reference errors
&lt;ul&gt;
&lt;li&gt;Expression are incorrectly referencing the question conditions or the results from prior steps.（错误引用了题目条件或之前的计算结果）&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Each gust blows the leaf forward 5 feet, so 10 gusts will blow it forward 5 ×10 = 50 feet. &lt;/code&gt;（题目原条件是11，不是10）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\varepsilon_M$ ：missing steps
&lt;ul&gt;
&lt;li&gt;Operands or expressions in the step that lack of references or support from the question conditions or prior steps.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Each swirl after a gust blows it back 2 feet, so 11 swirls will blow it back 2 ×11 = 22feet. Step 2. After 11 gusts, the leaf has traveled 55 − 22 = 33 feet down the sidewalk.&lt;/code&gt;（缺少了得到55这个数字的计算过程）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;$\varepsilon_H$：hallucinations（幻觉）
&lt;ul&gt;
&lt;li&gt;Statements or operands in the listed expression are fabricated or inconsistent with the question’s conditions.（虚构或与条件不一致）&lt;/li&gt;
&lt;li&gt;……&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;It is worth noting that this study specifically aims to explore conformity bias, and therefore, &lt;strong&gt;we do not include all possible error types.&lt;/strong&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;针对D和D&amp;rsquo;的每个样本，随机错误步骤的位置编号，每个样本生成了四种不同错误类型的样本，总共2000条&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154336790.png&#34;
	width=&#34;473&#34;
	height=&#34;212&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154336790_hu_396c502f844b6a72.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154336790_hu_4f52005375b8ff83.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Table 5&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;223&#34;
		data-flex-basis=&#34;535px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;analysis-and-findings&#34;&gt;Analysis and Findings
&lt;/h3&gt;&lt;h4 id=&#34;conformity-bias-identification&#34;&gt;Conformity Bias Identification
&lt;/h4&gt;&lt;p&gt;评估指标：The evaluation metric is the identification accuracy across both correct and erroneous solutions.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要识别错误位置、错误类型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paper使用提示词进行纠错，明确LLM本题存在替代解法，强调所有合理解决方案应该被接受，并且明确定义错误类别&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;6
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Given the &amp;lt;question&amp;gt;, please judge whether each step in &amp;lt;solution&amp;gt; is correct. **During the judging process, you should know that the &amp;lt;question&amp;gt; does not always have only one standard solution, and any reasonable &amp;lt;solution&amp;gt; should be accepted. You should pay attention to both the expressions and the statements in each step, and take care about the logic consistency between different steps. Additionally, consider arithmetic expression equivalency and avoid rejecting solutions solely because they use equivalent expressions.**
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;In each step, if no errors are found, respond with Step X: &amp;lt;correct&amp;gt;. If you find that the operands in the listed expressions are correct but an error occurs in the calculated result, respond with Step X: &amp;lt;calculation error&amp;gt;. If you find statements or operands in the listed expression are incorrectly referencing the question conditions or the results from prior steps, respond with Step X: &amp;lt;reference error&amp;gt;. If you find operands or expressions in the step that is lack of references or support from the question conditions or prior steps, respond with Step X: &amp;lt;missing step&amp;gt;. If you find statements or operands in the listed expression are fabricated or inconsistent with the question’s conditions, respond with: Step X: &amp;lt;hallucination&amp;gt;. If an error is a follow-on issue due to mistakes in previous steps rather than an independent error, respond with: Step X: &amp;lt;secondary error&amp;gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&amp;lt;question&amp;gt; [Question Text] &amp;lt;solution&amp;gt; [Solution Text]  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;Now, please start to respond.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;参与测试的LLM：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154400250.png&#34;
	width=&#34;441&#34;
	height=&#34;436&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154400250_hu_926cdad944d5de03.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154400250_hu_488d9a30ae5b5c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Table 6&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;101&#34;
		data-flex-basis=&#34;242px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;平均错误检测准确率的测试结果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154615400.png&#34;
	width=&#34;423&#34;
	height=&#34;305&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154615400_hu_e91b30da751b4c25.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626154615400_hu_4a904fb7a1a5feee.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Table 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;138&#34;
		data-flex-basis=&#34;332px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;证实了LLM在AED任务中存在明显的从众偏差&lt;/p&gt;
&lt;h4 id=&#34;solution-likelihood-score-analysis&#34;&gt;Solution Likelihood Score Analysis
&lt;/h4&gt;&lt;p&gt;对于当前问题$q$，我们可以计算LLM生成答案$s$的概率$P(s|q)$&lt;/p&gt;
&lt;p&gt;我们可以把$s$拆分成多个token：$s_i$&lt;/p&gt;
$$
P(s|q) = P(s_1, s_2, ...,s_{|s|} | q)
$$$$
P(s|q) = P(s_1|q) \times P(s_2|q,s_1) \times P(s_3|q,s_1,s_2) \times ...
$$$$
\log P(s|q) = \sum_{i=1}^{|s|} \log P(s_i|q,s_1:s_{i-1})
$$&lt;p&gt;
这个值就是&lt;strong&gt;对数似然分数&lt;/strong&gt; （Log-Likelihood Score）&lt;/p&gt;
&lt;p&gt;它衡量了模型在给定问题 &lt;em&gt;q&lt;/em&gt; 的情况下，对答案 &lt;em&gt;s&lt;/em&gt; 的“信任度”或理解程度&lt;/p&gt;
$$
\log L_{\theta}(s|q) = \frac{\log L_\theta(s|q)}{|s|}
$$&lt;p&gt;
其中$\theta$表示LLM的参数（闭源模型的似然分数无法获取，采取了开源模型的似然分数均值做伪指标）&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163200484.png&#34;
	width=&#34;562&#34;
	height=&#34;478&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163200484_hu_9a5549ae053eac12.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163200484_hu_cfa275c0f051adca.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Advance Model&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt; &lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163307581.png&#34;
	width=&#34;564&#34;
	height=&#34;480&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163307581_hu_49803f1626247f18.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163307581_hu_926314100216a0c1.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Base Model&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;同时喂入 了常规解法+替代解法，根据似然分数分成四个档进行对比&lt;/p&gt;
&lt;p&gt;显然似然值越高，Acc越高&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163347218.png&#34;
	width=&#34;577&#34;
	height=&#34;482&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163347218_hu_f8a58aeee4e02655.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626163347218_hu_3fada5ac724549e7.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Likehood Distribution&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;119&#34;
		data-flex-basis=&#34;287px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对于开源模型，替代解法的似然分数显然低于常规解法&lt;/p&gt;
&lt;h4 id=&#34;reference-based-detection-findings&#34;&gt;Reference-based Detection Findings
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;不太可能直接提高替代Solution的似然值
&lt;ul&gt;
&lt;li&gt;通过改变数据集进行微调，但是无法解决根本问题，仍有可能碰见其他的未遇见情况&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;参考：&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2407.09136&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2407.09136 Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors&lt;/a&gt;，提出引入参考答案提升了常规解法的检测性能&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;将学生解题步骤与标准答案对齐&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;本paper尝试推广到替代解法，但是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;实际情况下，参考答案并不是总能获取&lt;/li&gt;
&lt;li&gt;即便能够获取，一般也是常规解法的参考答案&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;paper采用了两条技术路线进行对比：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用常规解法作为参考答案&lt;/li&gt;
&lt;li&gt;自适应使用对应解法作为参考答案（？细节不太清楚，后文兴许会说）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171423880.png&#34;
	width=&#34;458&#34;
	height=&#34;343&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171423880_hu_38c9f63332d673bf.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171423880_hu_6c0b72c548df9ffd.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;常规解法作为参考答案&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;133&#34;
		data-flex-basis=&#34;320px&#34;
	
&gt;  &lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171459301.png&#34;
	width=&#34;459&#34;
	height=&#34;348&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171459301_hu_a726c1661fa683e4.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626171459301_hu_a94969f7c01bb73c.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;自适应选择参考答案&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;131&#34;
		data-flex-basis=&#34;316px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;根据与前文提示词的测试结果进行对比，引入参考答案对两个数据集的&lt;strong&gt;acc都有显著的提升&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但是使用常规解法做参考答案加剧了bias，而自适应选择明显缓解了bias&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;因此选择合适的参考答案是能起到关键作用的&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;提出AskBD（Ask-Before-Detection）框架，在评分过程中为每个待评答案动态生成适配的参考解法&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626172743783.png&#34;
	width=&#34;1227&#34;
	height=&#34;482&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626172743783_hu_dcc20bd97ebdb160.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626172743783_hu_1c39f4b9cd82f898.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pipeline&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;610px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;输入：问题文本$q$，解答文本$s$，LLM$f$，提示词$p$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Condition and question extractor(CQE)
&lt;ul&gt;
&lt;li&gt;从问题文本中抽取条件$q_c$和提问文本$q_i$&lt;/li&gt;
&lt;li&gt;$(q_c, q_i) = f([p_{cqe}, q])$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Solution Step Inquirer(SSI)
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;为提高生成结果的稳定性，SSI 会先总结每个步骤的结论再构建对应问题。&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;将解答文本转化为分步骤问题列表文本$Q$，&lt;strong&gt;末尾附加提问文本$q_i$&lt;/strong&gt;，以确保生成的参考解答能够回应原始问题的核心任务。&lt;/li&gt;
&lt;li&gt;$Q = [f([p_{ssi},s]), q_i]$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Step Question Responder (SQR)
&lt;ul&gt;
&lt;li&gt;通过条件文本总结$Q$中每个问题的答案，生成参考答案$r$&lt;/li&gt;
&lt;li&gt;$r = f([p_{sqr},q_c, Q])$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Reference-Enhanced Grader (REG)
&lt;ul&gt;
&lt;li&gt;根据$q, s, r$生成错误位置$y_s$和错误类型$y_e$&lt;/li&gt;
&lt;li&gt;$y=f([p_{reg},q,s,r])$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;输出：$y_s, y_e$&lt;/p&gt;
&lt;h2 id=&#34;experiment&#34;&gt;Experiment
&lt;/h2&gt;&lt;p&gt;实验目的是验证核心的三个问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;是否缓解从众偏差&lt;/li&gt;
&lt;li&gt;是否有额外的性能优势&lt;/li&gt;
&lt;li&gt;与CoT等推理技术的兼容性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;实验方法：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用前文数据集，相同的10个LLM，用于对比前两个问题&lt;/li&gt;
&lt;li&gt;整合了CoT技术，评估兼容性&lt;/li&gt;
&lt;li&gt;所有实验分别实验三种不同的随机seed，报告平均错误检测准确率&lt;/li&gt;
&lt;li&gt;前文测试和CoT方案作为两个baseline&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CoT的提示词&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt;1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;3
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Before the &amp;lt;response&amp;gt;, you should provide your step-by-step &amp;lt;thinking&amp;gt; about your judging process.
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; &amp;lt;question&amp;gt; [Question Text] &amp;lt;solution&amp;gt; [Solution Text]
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt; Now, please start to think first and then respond.
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626175506489.png&#34;
	width=&#34;884&#34;
	height=&#34;476&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626175506489_hu_bb07b79c8d6c3c11.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250626175506489_hu_5250d244aa5b549e.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;实验结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;185&#34;
		data-flex-basis=&#34;445px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;对于问题1&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;重点分析M0和M2在$\Delta$列的差距
&lt;ul&gt;
&lt;li&gt;Base版本的优化并不明显，认为是模型推理能力不足，限制了框架效用&lt;/li&gt;
&lt;li&gt;对比M1与M2，CoT也有缓解Bias的能力，在多数Advance模型中，框架的优化能力强于CoT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于问题2&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在D、D&amp;rsquo;列对比M2和M0
&lt;ul&gt;
&lt;li&gt;框架确实提升了acc性能&lt;/li&gt;
&lt;li&gt;对比M1 M2，CoT也体现出了性能提升
&lt;ul&gt;
&lt;li&gt;在base模型中CoT技术更胜一筹&lt;/li&gt;
&lt;li&gt;在Advance模型中框架更强（……）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;针对问题3&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;M3和M1对比，确实变强了&lt;/li&gt;
&lt;li&gt;兼容性好&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;limitation&#34;&gt;Limitation
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;只考虑了四个错误类型，忽略了学生解答中那些更罕见却更具挑战性的错误类型&lt;/li&gt;
&lt;li&gt;仅聚焦于数学应用题&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>大语言模型教育纠错 · 论文笔记（零）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/</link>
        <pubDate>Fri, 20 Jun 2025 14:52:55 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&#34;temporal-consistency-for-llm-reasoning-process-error-identification&#34;&gt;Temporal Consistency for LLM Reasoning Process Error Identification
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2503.14495&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2503.14495 Temporal Consistency for LLM Reasoning Process Error Identification&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;无训练过程&lt;/li&gt;
&lt;li&gt;纯迭代反思&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;应用领域主要是大模型自己的解题步骤的错误检测&lt;/p&gt;
&lt;p&gt;算是一个比较通用的做法，数学题之外有分步性质的应该也ok&lt;/p&gt;
&lt;p&gt;可以借鉴一下其&lt;strong&gt;Reflection&lt;/strong&gt;的方法&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;输入定义如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$P$：题目原文（例如数学问题）；&lt;/li&gt;
&lt;li&gt;$S = [s_1, s_2, &amp;hellip;, s_n]$：模型生成的解题步骤，按步分段；&lt;/li&gt;
&lt;li&gt;$L$：目标是预测哪一段 $s_i$ 是 &lt;strong&gt;首个错误步骤&lt;/strong&gt;（或无错误）；&lt;/li&gt;
&lt;li&gt;$R_t$：第 $t$ 轮的模型判断（包含错误定位和解释）；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总共准备了K个模型并行进行推理，对于单个模型需要做以下事情：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;给定题目、解题步骤、自己的上轮判断&lt;/li&gt;
&lt;li&gt;模型需要结合该信息判断、解释&lt;/li&gt;
&lt;li&gt;持续迭代&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对K个模型投票，票数最多的即为结果&lt;/p&gt;
&lt;p&gt;设定的终止条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单个模型连续$q$轮给出稳定结论&lt;/li&gt;
&lt;li&gt;K个模型的过去$q$轮的主体结果投票比例不能下降&lt;/li&gt;
&lt;li&gt;或者T轮迭代上限（防止死循环）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630152911616.png&#34;
	width=&#34;1790&#34;
	height=&#34;595&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630152911616_hu_d160ac2e2571813e.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630152911616_hu_970feda48933cee3.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Pipeline&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;300&#34;
		data-flex-basis=&#34;722px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;如图，三个模型进行迭代，最后得到一致的结果&lt;/p&gt;
&lt;p&gt;下面两个模型一开始不能得到正确答案，但是经过迭代得到正确结果&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;实验结果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630153824194.png&#34;
	width=&#34;501&#34;
	height=&#34;703&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630153824194_hu_1377b258bcc30ccd.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%99%E8%82%B2%E7%BA%A0%E9%94%99-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E9%9B%B6/assets/image-20250630153824194_hu_be26e3e2ea566e58.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Table&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;71&#34;
		data-flex-basis=&#34;171px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;todo&#34;&gt;Todo
&lt;/h1&gt;&lt;p&gt;2406.00755&lt;/p&gt;
</description>
        </item>
        <item>
        <title>大语言模型数据清洗 · 论文笔记（四）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%9B%9B/</link>
        <pubDate>Sat, 14 Jun 2025 16:35:54 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E5%9B%9B/</guid>
        <description>&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/html/2303.16854&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/html/2303.16854&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;preview&#34;&gt;Preview
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;解释 - 标注 双阶段方法
&lt;ul&gt;
&lt;li&gt;LLM生成少量人类标注的解释&lt;/li&gt;
&lt;li&gt;自动构建思维链+fewshot提示词&lt;/li&gt;
&lt;li&gt;自动标注&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach
&lt;/h2&gt;&lt;p&gt;从人类标注者的培训中可以发现，我们需要提供一定的引导、样例，才能规范人类标注一致性&lt;/p&gt;
&lt;p&gt;模型标注也是同理&lt;/p&gt;
&lt;h3 id=&#34;解释&#34;&gt;解释
&lt;/h3&gt;&lt;p&gt;使用GPT3.5进行生成解释：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Directions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Given&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;engine&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;google data studio sharepoint&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;consider&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;what&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;could&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;have&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mind&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;when&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;they&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;allow&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;misspellings&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;other&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ambiguity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;then&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classify&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;sharepoint migration tool file share&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;into&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;one&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;following&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Definitions&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;can&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;include&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;broader&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;narrower&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;selection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;competitor&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alternative&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;products&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;accessories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;products&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;often&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchased&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;related&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topics&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;well&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;direct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;There&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relationship&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;between&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;includes&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;but&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;limited&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;incorrect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unrelated&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unrelated&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wrong&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;location&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;when&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;location&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;important&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cannot&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;used&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;place&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nor&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;they&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;commonly&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchased&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;etc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Briefly&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;explain&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;why&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;response&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;length&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exceeding&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;100&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;words&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;h3 id=&#34;构建提示词&#34;&gt;构建提示词
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Given&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;engine&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;first&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;consider&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;what&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;could&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;have&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mind&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;when&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;they&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;allow&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;misspellings&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;other&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ambiguity&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;then&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;classify&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;into&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;one&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;following&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;definitions&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;categories&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;can&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;include&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;broader&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;narrower&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;selection&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;competitor&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alternative&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;products&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;accessories&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;products&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;often&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchased&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;related&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topics&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;well&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;direct&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;matches&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;There&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relationship&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;between&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;includes&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;but&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;limited&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;incorrect&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;unrelated&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;unrelated&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;topic&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;wrong&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;location&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;when&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;location&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;important&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;cannot&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;used&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;place&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nor&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;they&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;commonly&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchased&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;etc&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Please&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;predict&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;whether&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;answer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;should&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exact&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;### Examples:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;google&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;data&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;studio&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sharepoint&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sharepoint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;migration&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;share&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;sharepoint migration tool file share&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;google data studio sharepoint&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;it&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;pertains&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;different&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;or&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;technology&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;SharePoint&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;that&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mentioned&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;focuses&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;migration&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tool&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;SharePoint&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;file&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sharing&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;focused&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;on&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Google&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Data&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Studio&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;There&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;no&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relationship&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;between&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;two&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;so&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Therefore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;2.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;motorhomes&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sale&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;rv&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sale&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;used&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;class&lt;/span&gt; &lt;span class=&#34;nc&#34;&gt;c&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rv sale used class c&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rv sale used class c&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;contains&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;information&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;motorhomes sale&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;It&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;includes&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;specific&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;motorhome&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;class c&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;that&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;being&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sold&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;used&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;which&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;product&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;selection&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Additionally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;rv sale used class c&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;direct&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;match&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;commonly&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchased&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;together&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;products&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Therefore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;3.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;southern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exposure&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exchange&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;company&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;uk&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;poppy&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;uk poppy seeds&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;southern exposure seed exchange company&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;This&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;because&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;about&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;exchange&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;company&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;uk poppy seeds&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;specific&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;type&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;that&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;could&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;be&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offered&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;by&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;such&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;company&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Even&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;though&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;company&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mentioned&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;based&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;southern&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;hemisphere&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;they&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;may&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;still&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;offer&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;other&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;regions&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;including&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;UK&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;which&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;could&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;include&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;poppy&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;seeds&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Therefore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Not bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;4.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;nissan&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parts&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;canada&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchase&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;tires&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;purchase tires&amp;#34;&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;nissan parts canada&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;The&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;purchase tires&amp;#34;&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;directly&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;related&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;user&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;’&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;s&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nissan&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parts&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;in&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Canada&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Tires&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;part&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;of&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nissan&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;car&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;therefore&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;are&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevant&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nissan&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parts&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Additionally&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;does&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relate&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;to&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;common&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;purchase&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Nissan&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;parts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;and&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;it&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;not&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;a&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;substitute&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;search&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Therefore&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;the&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;relevance&lt;/span&gt; &lt;span class=&#34;ow&#34;&gt;is&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Bad&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;c1&#34;&gt;### Task:&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Query&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;query&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Keyword&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keyword&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Answer&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;</description>
        </item>
        <item>
        <title>大语言模型数据清洗 · 论文笔记（三）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/</link>
        <pubDate>Tue, 10 Jun 2025 14:54:12 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/</guid>
        <description>&lt;h1 id=&#34;finerweb-10bt-refining-web-data-with-llm-based-line-level-filtering&#34;&gt;FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2501.07314&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;2501.07314FinerWeb-10BT: Refining Web Data with LLM-Based Line-Level Filtering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/TurkuNLP/finerweb-10bt&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/TurkuNLP/finerweb-10bt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;GPT-4o mini 对 FineWeb 中 20,000 份文档样本进行逐行标注，使模型能够为低质量文本行创建描述性标签&lt;/li&gt;
&lt;li&gt;标签被归纳为九大类别，并训练 DeBERTa-v3 分类器将过滤规模扩展至 FineWeb 的 100 亿 token 子集&lt;/li&gt;
&lt;li&gt;结果表明：使用过滤数据训练的模型在 HellaSwag 基准测试中准确率更高，且能以最多减少 25%的数据量更快达到性能目标&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;核心问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How well can an LLM identify low-quality content missed by &lt;strong&gt;heuristic filters&lt;/strong&gt;?&lt;/li&gt;
&lt;li&gt;Does LLM-based &lt;strong&gt;quality filtering&lt;/strong&gt; of training datasets &lt;strong&gt;improve model performance&lt;/strong&gt;?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;paper定义高质量数据为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;human-written, continuous English text from the main content of a website, reflecting natural language use across diverse contexts and domains.&lt;/p&gt;
&lt;p&gt;网站主体内容中人类撰写的连贯英文文本，能反映跨领域自然语言使用。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;典型实例包括访谈核心文本、论坛帖子、新闻文章、博客和食谱。&lt;/li&gt;
&lt;li&gt;与之相对，低质量内容则包含导航菜单、版权声明、编程代码和元数据等重复性元素。&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;过滤分为三个级别：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;文档级：基于简单规则整篇剔除文档
&lt;ul&gt;
&lt;li&gt;少于三句话的文档&lt;/li&gt;
&lt;li&gt;存在过度重复内容的文档&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;行级：
&lt;ul&gt;
&lt;li&gt;删除含&lt;code&gt;javascript&lt;/code&gt;等术语的行、纯数字行或低于长度阈值的行&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;字符级：
&lt;ul&gt;
&lt;li&gt;移除维基百科常见的引用标记如&lt;code&gt;[1]&lt;/code&gt;和&lt;code&gt;[citation needed]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;现存的过滤方法具有数据集特异性，相关指标与数据集本身有关&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;行末标点比例≤0.12的文档（移除10.14% token，相比C4终止标点过滤的30%更高效）&lt;/li&gt;
&lt;li&gt;重复行字符比例≥0.1的文档（移除12.47% token）&lt;/li&gt;
&lt;li&gt;短行（&amp;lt;30字符）比例≥0.67的文档（移除3.73% token）&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;数据来源：Fineweb，构建来自 FineWeb 的 100 亿 token（约 1500 万文档）样本，称为 FineWeb-10BT&lt;/li&gt;
&lt;li&gt;抽样20,000份文档进行GPT-4o mini 标注
-   为每行生成描述性标签，分为高质量或低质量类别&lt;/li&gt;
&lt;li&gt;O1-preview将生成的大量标签归类为更小、更方便管理的集合&lt;/li&gt;
&lt;li&gt;训练基于encoder的分类器，scale到Fineweb10BT&lt;/li&gt;
&lt;li&gt;使用清洗前后的Fineweb10BT训练GPT-2，在HellaSwag上benchmark&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;全过程是数据驱动的，不依赖于固定的类别&lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments
&lt;/h2&gt;&lt;h3 id=&#34;gpt-4o-mini-标签标注&#34;&gt;GPT-4o mini 标签标注
&lt;/h3&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;51
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;52
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;53
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;54
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;55
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;56
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;57
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;58
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;59
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;60
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;61
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;62
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;63
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;64
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;65
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;66
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;67
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;68
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;69
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;70
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;71
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;72
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;73
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;74
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;75
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;76
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;77
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;78
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;79
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;80
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;81
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;82
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;83
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;84
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;85
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;86
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 系统提示词&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;system&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;You are an expert text classifier specializing in LLM training data. Your task is to classify each line of text based on its suitability for inclusion in a language model training dataset. High-quality content is clean, meaningful, well-structured, and useful for training language models. Low-quality content includes boilerplate elements (e.g., navigation menus, footers), non-linguistic symbols, formatting tags, placeholders like &amp;#39;Lorem ipsum&amp;#39;, and spammy, irrelevant, or toxic language.&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;c1&#34;&gt;# 用户提示词&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;    &lt;span class=&#34;n&#34;&gt;prompt&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;sa&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&amp;#34;&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Instructions:**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    1. **Line Identification and Separation**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Each line starts with &amp;#34;Line X:&amp;#34; where X is the line number. Treat each &amp;#34;Line X:&amp;#34; as a single unit, regardless of length; do not split lines.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Lines are separated by newline characters (`&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\\&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;n`) and dashes (`------`). If there&amp;#39;s no newline character, treat the entire text as a single line.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    2. **Contextual Classification**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Use the context of all lines when classifying each one, as they are sequential and from the same document.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - For example, a line starting with a hyphen might be part of a list and should be classified as &amp;#34;Clean.&amp;#34;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    3. **Assigning Labels**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Assign **exactly one label** to each line.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - If the line is suitable for inclusion, label it **&amp;#34;Clean&amp;#34;**.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - If not, assign a specific and descriptive label explaining why it&amp;#39;s unsuitable.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - **Prefer labels from the provided list**. Only create a new label (max three words) if absolutely necessary.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - **Do not use vague labels** like &amp;#34;Low-Quality,&amp;#34; &amp;#34;Bad,&amp;#34; &amp;#34;Unsuitable,&amp;#34; etc. Labels must be specific and descriptive.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    4. **Focus on Linguistic Content**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Retain valuable and diverse linguistic content suitable for language model pre-training, including natural language patterns, standard advertising copy, commercial language, and promotional content written in natural language.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    5. **Tolerance for Minor Errors and Toxic Language**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Minor grammatical errors, typos, or small mistakes do not disqualify a line from being &amp;#34;Clean.&amp;#34; Only exclude lines with pervasive errors that significantly hinder understanding.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Mild expletives and controversial opinions do not disqualify a line from being &amp;#34;Clean.&amp;#34; Only exclude lines with blatantly hateful, harmful or toxic content.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    6. **Output Format**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Your output must have exactly the same number of lines as the input, matching each line number correctly.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Output only the line number followed by the label, separated by a colon.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Do not include any additional text or explanations.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;       - Do not output dashes between the lines.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Guidelines for &amp;#34;Clean&amp;#34; Lines**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Assign &amp;#34;Clean&amp;#34; to lines that:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Represent natural language suitable for training language models.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Include informal internet language, grammatical errors, questions, partial sentences, and common online expressions.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Contain standard advertising or commercial language in natural sentences.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Have properly formatted titles, headings, and readable content, even with stylistic elements.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Include minor in-text elements like email addresses, dates, or URLs within natural sentences.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Are general promotional content written in natural language.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Guidelines for Non-&amp;#34;Clean&amp;#34; Lines**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Lines not classified as &amp;#34;Clean&amp;#34; need a specific and descriptive label. Examples include lines that:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Contain blatantly hateful or harmful language. 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Are long passages of non-English text (excluding common foreign phrases used in English).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Include disclaimers, copyright notices, terms, and conditions.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Consist of menu items, login links, buttons, or navigation menus.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Contain random characters, garbled text, or excessive symbols.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Include programming code, HTML tags, or markup languages (when actual code or markup appears).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Present keywords, tags, or similar data without sufficient context.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Are irrelevant or spam-like content not suitable for training.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    - Are **excessively** promotional without natural language structure (e.g., a list of product names and prices without sentences).
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Possible Labels for Non-&amp;#34;Clean&amp;#34; Lines**:
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;non_quality_labels&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Example Input:**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 1: Welcome to our website!
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    ------
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 2: Contact us at support@example.com.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    ------
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 3: ***** $$$$$
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    ------
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 4: &amp;lt;div&amp;gt;Content&amp;lt;/div&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    ------
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Example Output:**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 1: Clean  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 2: Clean  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 3: Encoding Errors  
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    Line 4: HTML Tags
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    **Now, classify the following lines:**
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;si&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;s2&#34;&gt;    &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;div class=&#34;chroma&#34;&gt;
&lt;table class=&#34;lntable&#34;&gt;&lt;tr&gt;&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code&gt;&lt;span class=&#34;lnt&#34;&gt; 1
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 2
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 3
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 4
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 5
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 6
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 7
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 8
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt; 9
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;10
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;11
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;12
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;13
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;14
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;15
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;16
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;17
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;18
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;19
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;20
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;21
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;22
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;23
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;24
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;25
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;26
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;27
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;28
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;29
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;30
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;31
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;32
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;33
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;34
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;35
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;36
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;37
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;38
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;39
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;40
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;41
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;42
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;43
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;44
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;45
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;46
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;47
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;48
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;49
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;50
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;51
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;52
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;53
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;54
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;55
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;56
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;57
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;58
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;59
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;60
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;61
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;62
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;63
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;64
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;65
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;66
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;67
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;68
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;69
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;70
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;71
&lt;/span&gt;&lt;span class=&#34;lnt&#34;&gt;72
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;
&lt;td class=&#34;lntd&#34;&gt;
&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;指令&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;1.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;行标识与分隔&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;每行以&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;开头&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;X为行号&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;将每个&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;视为一个独立单元&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;无论长度如何&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;；&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;请勿拆分&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;行之间用换行符&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（`&lt;/span&gt;\&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;`）&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;和短横线&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（`&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;------&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;`）&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;分隔&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;若无换行符&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;则将整个文本视为单行&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;2.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;上下文分类&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;分类时需考虑所有行的上下文&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;因为它们来自同一文档且顺序相关&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;例如&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;以连字符开头的行可能是列表的一部分&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;应标记为&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;3.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;标签分配&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;每行&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;必须分配一个标签&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;若适合纳入训练数据&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;标记为&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;若不适合&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;需提供具体描述性标签说明原因&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;优先使用提供的标签列表&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;仅在必要时创建新标签&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;最多三个单词&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;禁止使用模糊标签&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;如&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;低质量&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”、“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;差&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”、“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;不合适&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;等&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;标签必须具体明确&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;4.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;语言内容聚焦&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;保留对语言模型预训练有价值的多样化语言内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;包括自然语言模式&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;标准广告文案&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;商业用语和自然语言编写的推广内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;5.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;对轻微错误和毒性内容的容忍&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;轻微语法错误&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;拼写问题或小错误不影响标记为&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;仅当错误严重影响理解时才排除&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;轻度脏话或有争议的观点不影响标记为&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”。&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;仅排除明显仇恨&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;有害或毒性内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;mf&#34;&gt;6.&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;输出格式&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;输出行数必须与输入完全一致&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;且行号对应正确&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;每行输出格式为&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;X&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;标签&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;仅包含行号和标签&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;禁止额外解释或文本&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;   &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;行间禁止输出短横线&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;行标准&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;符合以下条件的行标记为&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;代表适合训练的自然语言&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;包含网络用语&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;语法错误&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;问题&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;不完整句子或常见网络表达&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;含有自然句式中的标准广告或商业用语&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;格式正确的标题&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;题头或可读内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;即使包含样式元素&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;自然句子中的邮箱&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;日期或URL等次要元素&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;以自然语言编写的常规推广内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;非&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;行标准&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;需提供具体描述性标签&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;，&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;例如&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;包含明显仇恨或有害内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;大段非英语文本&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;英语中常用的外语短语除外&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;免责声明&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;版权声明&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;条款协议&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;菜单项&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;登录链接&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;按钮或导航菜单&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;随机字符&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;乱码或过多符号&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;编程代码&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;、&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;HTML标签或标记语言&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;实际代码或标签出现时&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;缺乏上下文的关键词或标签&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;与训练无关的垃圾内容&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;过度推广&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;且无自然语言结构&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;（&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;如纯产品名和价格列表&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;）。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;非&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;“&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;”&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;行标签示例&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;non_quality_labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;输入示例&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;欢迎访问我们的网站&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;！&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;------&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;联系支持邮箱&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;support&lt;/span&gt;&lt;span class=&#34;nd&#34;&gt;@example.com&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;。&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;------&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*****&lt;/span&gt; &lt;span class=&#34;err&#34;&gt;$$$$$&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;------&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;div&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;内容&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;div&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;&amp;gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;------&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;输出示例&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;Clean&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;编码错误&lt;/span&gt;  
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;Line&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;HTML标签&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;请对以下行进行分类&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;err&#34;&gt;：&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;input&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/div&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;一开始并不提供任何的非Clean标签，由模型逐渐生成，优先使用已有的标签，否则进行扩充&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;未避免顺序带来的影响，每次迭代后随即打乱标签列表&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;文档最多被分割为多个chunk，每个chunk最多15行，方便结合上下文&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;单行不能超过200字符，否则按照标点进行切割为新的行&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;paper提到：超长行会导致模型的错误输出&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610184759762.png&#34;
	width=&#34;875&#34;
	height=&#34;596&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610184759762_hu_77126521573083f.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610184759762_hu_c2d6a7a748645646.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;50个最常见的标签 - 二维UMAP投影&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;146&#34;
		data-flex-basis=&#34;352px&#34;
	
&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其中每个圆点的大小对应相应类别的相对出现频率&lt;/p&gt;
&lt;p&gt;法律文本出现在左上角，成人及有害内容集中于右上方中部，而参考文献则靠近底部。联系方式（如时间、日期和电话号码）松散分布在左侧，技术类内容（如编程代码）则位于中部。这些分布模式表明，LLM 生成的标签能够有效区分文本行质量，为我们最终构建分类体系提供了可靠依据。&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;83%的数据被标记为清洁&lt;/li&gt;
&lt;li&gt;547个生成的标签，其中部分只出现了一次
&lt;ul&gt;
&lt;li&gt;人工复查，直接标记为清洁&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;标签分组&#34;&gt;标签分组
&lt;/h3&gt;&lt;p&gt;对于实现剩下的382个标签，通过O1-preview（推理模型）归类为更简洁、更易管理的宽泛类别&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;指导该模型创建清晰、明确的分类&lt;/li&gt;
&lt;li&gt;每个标签只能属于一个组别&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Categories&lt;/th&gt;
          &lt;th&gt;Lines&lt;/th&gt;
          &lt;th&gt;%&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Clean&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;283,267&lt;/td&gt;
          &lt;td&gt;86.24&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Formatting, Style &amp;amp; Errors 格式、风格与错误&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;13,150&lt;/td&gt;
          &lt;td&gt;4.00&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Bibliographical &amp;amp; Citation References 参考文献与引用规范&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;8,768&lt;/td&gt;
          &lt;td&gt;2.67&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Promotional &amp;amp; Spam Content 促销与垃圾内容&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;7,339&lt;/td&gt;
          &lt;td&gt;2.23&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Contact &amp;amp; Identification Information 联系与身份识别信息&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;3,898&lt;/td&gt;
          &lt;td&gt;1.19&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Navigation &amp;amp; Interface Elements 导航与界面元素&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;3,327&lt;/td&gt;
          &lt;td&gt;1.01&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Technical Specifications &amp;amp; Metadata 技术规范与元数据&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;3,298&lt;/td&gt;
          &lt;td&gt;1.00&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Legal &amp;amp; Administrative Content 法律与行政内容&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;2,992&lt;/td&gt;
          &lt;td&gt;0.91&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;em&gt;Offensive or Inappropriate Content 冒犯性或不当内容&lt;/em&gt;&lt;/td&gt;
          &lt;td&gt;2,433&lt;/td&gt;
          &lt;td&gt;0.74&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Total 总计&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;328,472&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;100&lt;/strong&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;模型可能会发生错误，例如未能分配全部标签、标签归入多个类别……&lt;/p&gt;
&lt;p&gt;人工修正一下即可&lt;/p&gt;
&lt;h4 id=&#34;inter-annotator-agreement--人工标注者一致性iaa实验&#34;&gt;Inter-Annotator Agreement  人工标注者一致性（IAA）实验
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;抽取50篇文档的726行，人工独立分类到九个标签之内&lt;/li&gt;
&lt;/ul&gt;
$$
\kappa = \frac{p_o - p_e}{1 - p_e}
$$&lt;blockquote&gt;
&lt;p&gt;假设两位标注员（A 和 B）对 100 条文本进行情感分类，标签为 &lt;strong&gt;正面（Positive）&lt;/strong&gt; 或 &lt;strong&gt;负面（Negative）&lt;/strong&gt;。他们的标注结果如下表：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;&lt;/th&gt;
          &lt;th&gt;B: Positive&lt;/th&gt;
          &lt;th&gt;B: Negative&lt;/th&gt;
          &lt;th&gt;&lt;strong&gt;总计&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A: Positive&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;50&lt;/td&gt;
          &lt;td&gt;10&lt;/td&gt;
          &lt;td&gt;60&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;A: Negative&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;20&lt;/td&gt;
          &lt;td&gt;40&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;总计&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;70&lt;/td&gt;
          &lt;td&gt;30&lt;/td&gt;
          &lt;td&gt;100&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;hr&gt;
&lt;p&gt;$p_o$是两位标注员&lt;strong&gt;实际一致的比例&lt;/strong&gt;，即对角线单元格的和除以总数。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;两位标注员在 70 条样本上达成一致（50 条 Positive + 20 条 Negative），因此$p_o = 0.7$&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;$p_e$ 是假设两位标注员&lt;strong&gt;随机标注&lt;/strong&gt;时预期的一致比例。需分别计算每个类别随机一致的联合概率，再求和。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A 标注 Positive 的概率&lt;/strong&gt;：$P_{\text{A+}} = \frac{60}{100} = 0.6$
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;A 标注 Negative 的概率&lt;/strong&gt;：$P_{\text{A-}} = \frac{40}{100} = 0.4$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B 标注 Positive 的概率&lt;/strong&gt;：$P_{\text{B+}} = \frac{70}{100} = 0.7$&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;B 标注 Negative 的概率&lt;/strong&gt;：$P_{\text{B-}} = \frac{30}{100} = 0.3$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;已知以上概率，接下来计算在随机标注的情况下，两人同时一致的概率：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;随机都标为 Positive 的概率：$P_{\text{A+}} \times P_{\text{B+}} = 0.6 \times 0.7 = 0.42$
&lt;ul&gt;
&lt;li&gt;随机都标为 Negative 的概率：$P_{\text{A-}} \times P_{\text{B-}} = 0.4 \times 0.3 = 0.12$&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;因此：$p_e = 0.42 + 0.12 = 0.54$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;解释&lt;/strong&gt;：&lt;br&gt;
如果两位标注员完全随机标注，预计会有 54% 的样本因巧合而一致。&lt;/p&gt;
&lt;hr&gt;
$$
&gt;   \kappa = \frac{p_o - p_e}{1 - p_e} = \frac{0.7 - 0.54}{1 - 0.54} = \frac{0.16}{0.46} \approx 0.348
&gt;   $$&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;κ ≈ 0.35&lt;/strong&gt;：介于 0.2~0.4 之间，说明两位标注员的一致性为“一般”（仅略高于随机水平）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对比简单一致率 70%&lt;/strong&gt;：若直接用 70% 会高估一致性，而 Cohen&amp;rsquo;s Kappa 通过剔除随机影响，给出了更严格的评估。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;\( p_o \)&lt;/strong&gt;：直接观察到的对角线比例。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;\( p_e \)&lt;/strong&gt;：基于边际分布的“随机一致”概率，反映巧合带来的虚假一致性。
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Kappa 的意义&lt;/strong&gt;：量化了&lt;strong&gt;超越随机水平的一致性&lt;/strong&gt;，避免高估可靠性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;κ值范围&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;一致性强度&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;&lt;strong&gt;解释&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;κ ≤ 0&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;比随机还差&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性低于随机猜测（罕见，可能表示系统性分歧或标注错误）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;0 &amp;lt; κ ≤ 0.2&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;轻微一致（可忽略）&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性极低，几乎无实际意义。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;0.2 &amp;lt; κ ≤ 0.4&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一般一致（弱）&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性较弱，但高于随机水平（需谨慎对待结果）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;0.4 &amp;lt; κ ≤ 0.6&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;中等一致&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性适中，结果有一定可靠性（常见于人工标注任务）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;0.6 &amp;lt; κ ≤ 0.8&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;高度一致&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性较强，结果可靠（如专业医生诊断或严格标注流程）。&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;0.8 &amp;lt; κ ≤ 1&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;几乎完全一致&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;一致性极高，接近完美（罕见，通常需检查是否过拟合或标注规则过于简单）。&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;通过IAA实验，得到：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;A1&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;strong&gt;A2&lt;/strong&gt;&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;strong&gt;Avg. 平均&lt;/strong&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;All labels 所有标签&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.79&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.60&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.70&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Clean vs. Non-clean 清洁与非清洁&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.78&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.67&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;0.73&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;基于 LLM 的分类方法总体上能为 FineWeb 文本生成可接受的标签。&lt;/p&gt;&lt;/blockquote&gt;
&lt;h3 id=&#34;分类器训练&#34;&gt;分类器训练
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;DeBERTa-v3&lt;/li&gt;
&lt;li&gt;Stella-en-400M-v5&lt;/li&gt;
&lt;li&gt;XLM-RoBERTa-base（支持多语言）&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;我们首先从文档中提取独立文本行，将每行作为单独样本。随后对数据进行随机打乱，并通过分层抽样划分为训练集（70%）、开发集（10%）和测试集（20%）。我们在每个模型上添加分类头，为每行文本生成 9 个类别的概率分布，同时微调分类头与基础模型。&lt;/p&gt;
&lt;p&gt;我们采用 bfloat16 精度，学习率设为 1e-5，批处理大小为 16。基于评估损失值实施早停机制（耐心值为 5），最大训练轮数设为 5 轮，但模型通常在首轮后即收敛。我们对交叉熵损失函数施加 0.1 的标签平滑处理以提升泛化能力。所有训练均在单块 A100 GPU 上完成。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610192229209.png&#34;
	width=&#34;899&#34;
	height=&#34;634&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610192229209_hu_9bd47b2a1e06ee33.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610192229209_hu_d11c68ec86e2dc63.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;分类器混淆矩阵&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;141&#34;
		data-flex-basis=&#34;340px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大多数误分类样本被归入 Clean 类别，表明其他类别间具有较强区分度&lt;/li&gt;
&lt;li&gt;冒犯性或不当内容区分度最低，源于 LLM 训练数据中对冒犯性材料定义边界存在固有困难&lt;/li&gt;
&lt;li&gt;参考文献与引用类别因其易于识别的格式和内容特征，成为区分度最高的类别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;分类器更倾向于将低质量文本行误标为&amp;quot;清洁&amp;quot;&lt;/p&gt;
&lt;p&gt;而非错误地将高质量行标记为低质量&lt;/p&gt;
&lt;p&gt;这种偏差有助于降低从数据集中丢弃有价值数据的风险&lt;/p&gt;
&lt;h3 id=&#34;数据清洗&#34;&gt;数据清洗
&lt;/h3&gt;&lt;p&gt;Clean数据占比86%确实可能会带来模型预测过度自信的问题&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;采用 Platt 缩放法
&lt;ul&gt;
&lt;li&gt;在保留测试集上训练 Platt 逻辑回归模型&lt;/li&gt;
&lt;li&gt;在为 FineWeb-10BT 数据集预测质量分数时将其叠加应用于分类器之上&lt;/li&gt;
&lt;li&gt;留坑，先不研究&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对整个数据集进行分片，每个分片128行为一个批次&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;转化为分类问题，只判断是否为Clean&lt;/li&gt;
&lt;li&gt;阈值分别设为0.5或0.9&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610194623280.png&#34;
	width=&#34;701&#34;
	height=&#34;485&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610194623280_hu_f739427efcc7e2.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%89/assets/image-20250610194623280_hu_31a6a550d7d615f5.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;GPT-2训练结果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;144&#34;
		data-flex-basis=&#34;346px&#34;
	
&gt;&lt;/p&gt;
</description>
        </item>
        <item>
        <title>大语言模型数据清洗 · 论文笔记（二）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/</link>
        <pubDate>Fri, 06 Jun 2025 13:12:32 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/</guid>
        <description>&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h1 id=&#34;the-pile-an-800gb-dataset-of-diverse-text-for-language-modeling&#34;&gt;The Pile: An 800GB Dataset of Diverse Text for Language Modeling
&lt;/h1&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2101.00027&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;arXiv 2101.00027 The Pile: An 800GB Dataset of Diverse Text for Language Modeling&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/EleutherAI/the-pile&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github EleutherAI/the-pile&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过合成多个数据集，提升多样性，提升大规模语言模型的跨领域通用知识与下游任务泛化能力&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;稍微看偏了，paper更多的精华在如何去衡量数据集对模型性能的提升水平&lt;/p&gt;
&lt;p&gt;和清洗关系不大&lt;/p&gt;
&lt;h2 id=&#34;the-pile-datasets&#34;&gt;The Pile Datasets
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;由22个部分组成&lt;/li&gt;
&lt;li&gt;由于不同数据集存在差异（维基百科质量更高），因此进行了加权处理
&lt;ul&gt;
&lt;li&gt;权重越高，被使用的概率越高（更可能被重复使用次数）&lt;/li&gt;
&lt;li&gt;例如维基百科重复采用3次&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;部分表格：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Dataset Name&lt;/th&gt;
          &lt;th&gt;Raw Size (before sampling)&lt;/th&gt;
          &lt;th&gt;Weight (%)&lt;/th&gt;
          &lt;th&gt;Epochs&lt;/th&gt;
          &lt;th&gt;Effective Size&lt;/th&gt;
          &lt;th&gt;Mean Document Size&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;Pile-CC&lt;/td&gt;
          &lt;td&gt;227.12 GiB&lt;/td&gt;
          &lt;td&gt;18.11%&lt;/td&gt;
          &lt;td&gt;1.0&lt;/td&gt;
          &lt;td&gt;227.12 GiB&lt;/td&gt;
          &lt;td&gt;4.33 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;PubMed Central&lt;/td&gt;
          &lt;td&gt;90.27 GiB&lt;/td&gt;
          &lt;td&gt;14.40%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;180.55 GiB&lt;/td&gt;
          &lt;td&gt;30.55 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Books3&lt;/td&gt;
          &lt;td&gt;100.96 GiB&lt;/td&gt;
          &lt;td&gt;12.07%&lt;/td&gt;
          &lt;td&gt;1.5&lt;/td&gt;
          &lt;td&gt;151.44 GiB&lt;/td&gt;
          &lt;td&gt;538.36 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;OpenWebText2&lt;/td&gt;
          &lt;td&gt;62.77 GiB&lt;/td&gt;
          &lt;td&gt;10.01%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;125.54 GiB&lt;/td&gt;
          &lt;td&gt;3.85 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;ArXiv&lt;/td&gt;
          &lt;td&gt;56.21 GiB&lt;/td&gt;
          &lt;td&gt;8.96%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;112.42 GiB&lt;/td&gt;
          &lt;td&gt;46.61 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Github&lt;/td&gt;
          &lt;td&gt;95.16 GiB&lt;/td&gt;
          &lt;td&gt;7.59%&lt;/td&gt;
          &lt;td&gt;1.0&lt;/td&gt;
          &lt;td&gt;95.16 GiB&lt;/td&gt;
          &lt;td&gt;5.25 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;FreeLaw&lt;/td&gt;
          &lt;td&gt;51.15 GiB&lt;/td&gt;
          &lt;td&gt;6.12%&lt;/td&gt;
          &lt;td&gt;1.5&lt;/td&gt;
          &lt;td&gt;76.73 GiB&lt;/td&gt;
          &lt;td&gt;15.06 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;StackExchange&lt;/td&gt;
          &lt;td&gt;32.20 GiB&lt;/td&gt;
          &lt;td&gt;5.13%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;64.39 GiB&lt;/td&gt;
          &lt;td&gt;2.16 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;USPTO Backgrounds&lt;/td&gt;
          &lt;td&gt;22.90 GiB&lt;/td&gt;
          &lt;td&gt;3.65%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;45.81 GiB&lt;/td&gt;
          &lt;td&gt;4.08 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;PubMed Abstracts&lt;/td&gt;
          &lt;td&gt;19.26 GiB&lt;/td&gt;
          &lt;td&gt;3.07%&lt;/td&gt;
          &lt;td&gt;2.0&lt;/td&gt;
          &lt;td&gt;38.53 GiB&lt;/td&gt;
          &lt;td&gt;1.30 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Gutenberg (PG-19)&lt;/td&gt;
          &lt;td&gt;10.88 GiB&lt;/td&gt;
          &lt;td&gt;2.17%&lt;/td&gt;
          &lt;td&gt;2.5&lt;/td&gt;
          &lt;td&gt;27.19 GiB&lt;/td&gt;
          &lt;td&gt;398.73 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;OpenSubtitles&lt;/td&gt;
          &lt;td&gt;12.98 GiB&lt;/td&gt;
          &lt;td&gt;1.55%&lt;/td&gt;
          &lt;td&gt;1.5&lt;/td&gt;
          &lt;td&gt;19.47 GiB&lt;/td&gt;
          &lt;td&gt;30.48 KiB&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Wikipedia (en)&lt;/td&gt;
          &lt;td&gt;6.38 GiB&lt;/td&gt;
          &lt;td&gt;1.53%&lt;/td&gt;
          &lt;td&gt;&lt;strong&gt;3.0&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;19.13 GiB&lt;/td&gt;
          &lt;td&gt;1.11 KiB&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;Raw Size：采样前的大小&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Weight ：采样后的大小占比&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Epochs：被采样次数&lt;/p&gt;
&lt;p&gt;Effective Size：采样后的有效大小&lt;/p&gt;
&lt;p&gt;Mean Document Size：平均文档大小&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;部分数据已被发布者清洗的很好，只进行了最小程度的预处理&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;pile-cc&#34;&gt;Pile-CC
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;由CC数据集清洗得到&lt;/li&gt;
&lt;li&gt;使用justText清洗raw HTTP responses including page HTML，相比于&lt;code&gt;.WET&lt;/code&gt;的纯文本效果更好&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;others&#34;&gt;Others
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;分类&lt;/th&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;来源&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;学术文献&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;ArXiv、PubMed Central、NIH ExPorter&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;图书与出版物&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Books3、Project Gutenberg (PG-19)、BookCorpus2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;代码与技术文档&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;GitHub、StackExchange&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;法律与政府文件&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;FreeLaw、USPTO Backgrounds&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;多语言与翻译文本&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;EuroParl、OpenSubtitles&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;社交与对话数据&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;HackerNews、Ubuntu IRC、Enron Emails&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;特殊领域数据&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;DeepMind Mathematics、PhilPapers（哲学）、YouTube字幕&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;网络爬取内容&lt;/td&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;Pile-CC（新构建的Clean Common Crawl子集）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;benchmarking-language-models-with-the-pile&#34;&gt;Benchmarking Language Models with the Pile
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;可以训练数据，同时因为涉及领域广泛，也可以基准测试&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;划分为训练集、验证集、测试集（$0.1%$测试集+验证集，虽然比例很低但是仍各自超过1G）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;尽管去重，但是肯定还是存在重复&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;paper中首选了BPB作为评测指标：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;输入：负对数似然损失（Negative Log-Likelihood Loss）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;模型在测试数据上输出一个损失值 $L$，表示其预测能力。&lt;/li&gt;
&lt;li&gt;越低的 $L$ 表示模型越能准确预测下一个词。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;转换为 BPB：&lt;/strong&gt;（bits per UTF-8 encoded byte）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用公式将损失 $L$ 转换为每字节的比特数&lt;/li&gt;
&lt;/ul&gt;
$$
    BPB = \frac{L_T}{L_B}\log_2 e^L = \frac{L_T}{L_B}\times \frac{L}{\ln2}
    $$&lt;ul&gt;
&lt;li&gt;其中：
&lt;ul&gt;
&lt;li&gt;$L_T$：数据集以 token 为单位的长度&lt;/li&gt;
&lt;li&gt;$L_B$：数据集以 UTF-8 编码字节为单位的长度&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;和困惑度有一点相似，用于衡量模型对数据的压缩效率或预测能力&lt;/p&gt;
&lt;p&gt;与Bits per Character (bpc)不同的一点，字符不是一个很好的定义（Unicode 中字符的界定可能复杂（例如组合字符、emoji 等），导致统计不一致。）&lt;/p&gt;
&lt;p&gt;同时bpb不受到分词的影响，UTF-8的字节定义是准确的，适合基于不同模型、分词进行比较&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;指标&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;优点&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;缺点&lt;/th&gt;
          &lt;th style=&#34;text-align: left&#34;&gt;适用场景&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Bits per Byte&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;分词无关、字节标准明确&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;对非字节级任务不直观&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;跨模型比较、数据压缩评估&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Bits per Char&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;更贴近人类理解&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;Unicode 字符定义模糊&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;字符级生成任务（需统一字符定义）&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;&lt;strong&gt;Perplexity&lt;/strong&gt;&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;直接反映预测不确定性&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;依赖分词、数值范围不稳定&lt;/td&gt;
          &lt;td style=&#34;text-align: left&#34;&gt;单一模型调参、生成质量评估&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;更加完整的解释&#34;&gt;更加完整的解释
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;自信息：指的是当我们接收到一个消息时所获得的信息量&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在信息论中，自信息衡量一个事件携带的信息量，由概率$p$决定。&lt;/p&gt;
$$
I(p) = -\log_2(p)
$$&lt;p&gt;为了编码这一事件，我们选择霍夫曼编码这类最优编码，同时为了最小化平均码长：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高频事件：分配短码&lt;/li&gt;
&lt;li&gt;低频事件：分配长码&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;如果事件 $A$ 的概率  $p = 1/2$ ，则  $I(A) = -\log_2(1/2) = 1$  比特。这表示需要用 1 位二进制码（如 &lt;code&gt;0&lt;/code&gt; 或 &lt;code&gt;1&lt;/code&gt;）编码。
-   如果事件  $B$  的概率  $p = 1/8$ ，则  $I(B) = -\log_2(1/8) = 3$  比特。需要用 3 位二进制码（如 &lt;code&gt;000&lt;/code&gt; 到 &lt;code&gt;111&lt;/code&gt; 之一）编码。&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
$$
L = -\ln p
$$&lt;p&gt;
一般使用的是自然对数，同时其恰好表示了概率为$p$的事件的信息量（单位为纳特（底数取e））&lt;/p&gt;
$$
Bits = I(p) = -\log_2(p) =-\frac{\ln p}{\ln 2} =\frac{L}{\ln 2}
$$&lt;p&gt;
&lt;strong&gt;同时，模型的损失是基于token计算的，即每个token的预测损失&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;所以这里的单位是：Bits per token&lt;/p&gt;
$$
bpb =  \frac{L_T}{L_B}\times \frac{L}{\ln2}
$$&lt;p&gt;
这样就得到了：Bits per Byte，消除了分词器、语种编码等其他影响，可以直接衡量模型输出的质量&lt;/p&gt;
&lt;h2 id=&#34;评测&#34;&gt;评测
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;然后paper实验验证了一下用训练集训练过的模型会更nb&lt;/p&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;通过分析哪些Pile子数据集的表现最差，就知道模型的训练数据分布在这块比较浅，就可以使用pile这块数据集进行补充&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了探索哪些数据集是模型表现较差的，显然不能直接使用困惑度进行比较（数据集熵值不一样）&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;结构化的数据（熵值低）困惑度天然会比非结构化的更低&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;困惑度可以用于衡量一个数据集是否更接近另一个数据集&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如CCNet，在维基百科内训练一个模型，计算其他数据集的困惑度&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;p&gt;所以如果要比较的话，可以通过模型的损失值，拟合得越好，说明训练数据中包含了这部分，否则就是缺失&lt;/p&gt;
&lt;p&gt;如果钱多的话，当然是直接把所有数据集用模型train一下，看看损失值，与没有train过的原模型（GPT-3），在测试集上比一下Loss&lt;/p&gt;
&lt;p&gt;paper这里钱不够，改用了GPT2做了一个trick：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;首先需要知道GPT3比GPT2强多少&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;参考数据集：OWT2（与GPT训练数据高度相似的一个数据集）&lt;/li&gt;
&lt;li&gt;用原生的GPT3和在Pile训练的GPT2进行比较&lt;/li&gt;
&lt;li&gt;得到一个基准差值&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
$$
L^{GPT-3}_{OWT2} - L^{GPT-2-Pile}_{OWT2}
$$&lt;ul&gt;
&lt;li&gt;
$$
L^{GPT-3}_{TargetSet} - L^{GPT-2-Pile}_{TargetSet}
$$&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;两个值作差：大概能衡量出在目标数据集上的提升水平&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250606162030021.png&#34;
	width=&#34;1172&#34;
	height=&#34;571&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250606162030021_hu_c35f4776da27d503.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%BA%8C/assets/image-20250606162030021_hu_a020da3b1c3d156d.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;205&#34;
		data-flex-basis=&#34;492px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Books3等数据集与GPT-3训练数据高度相似，因此不会有过多的提升（0）&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;清洗&#34;&gt;清洗
&lt;/h2&gt;&lt;p&gt;看不动了，以后再说，整理一下清洗的东西：&lt;/p&gt;
&lt;h3 id=&#34;c1-pile-ccclean-common-crawl&#34;&gt;C.1 Pile-CC（Clean Common Crawl）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Common Crawl 的 WARC 文件（2013–2020 年）。&lt;/li&gt;
&lt;li&gt;提取工具
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;jusText&lt;/code&gt; 提取网页正文，去除菜单、页脚等模板文本。&lt;/li&gt;
&lt;li&gt;对比了 &lt;code&gt;Trafilatura&lt;/code&gt;、&lt;code&gt;Newspaper&lt;/code&gt;、&lt;code&gt;Goose3&lt;/code&gt;、&lt;code&gt;DragNet&lt;/code&gt;，最终选择 &lt;code&gt;jusText&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;语言过滤
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;pycld2&lt;/code&gt; 检测网页语言，仅保留英文内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;质量控制
&lt;ul&gt;
&lt;li&gt;使用 FastText 分类器对 OpenWebText2 和 Common Crawl 进行分类，过滤低质量页面。&lt;/li&gt;
&lt;li&gt;参数 α = 3，使用 Pareto 分布阈值进行过滤。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;去重
&lt;ul&gt;
&lt;li&gt;使用 MinHash LSH 算法在内存中进行文档级去重。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;其他说明
&lt;ul&gt;
&lt;li&gt;未使用 WET 文件，因其包含大量模板文本。&lt;/li&gt;
&lt;li&gt;与 Brown et al. (2020) 类似，但只处理了部分 WARC 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c2-pubmed-centralpmc&#34;&gt;C.2 PubMed Central（PMC）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：美国国家生物技术信息中心（NCBI）提供。&lt;/li&gt;
&lt;li&gt;格式转换
&lt;ul&gt;
&lt;li&gt;使用 Pandoc 将 JATS 格式转为 Markdown。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;清理步骤
&lt;ul&gt;
&lt;li&gt;删除以 &lt;code&gt;:::&lt;/code&gt; 开头的行（Pandoc 添加的 HTML 类标签）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c3-books3&#34;&gt;C.3 Books3
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：未具体说明，但为高质量书籍数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;处理细节&lt;/strong&gt; ：无额外处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c4-openwebtext2owt2&#34;&gt;C.4 OpenWebText2（OWT2）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Reddit 提交链接。&lt;/li&gt;
&lt;li&gt;处理步骤
&lt;ul&gt;
&lt;li&gt;提取 URL 及其元数据。&lt;/li&gt;
&lt;li&gt;去除得分低于 3 的链接。&lt;/li&gt;
&lt;li&gt;使用 Newspaper 抓取网页内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;去重
&lt;ul&gt;
&lt;li&gt;使用 DataSketch 库进行文档级 MinHash LSH 去重。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c5-arxiv&#34;&gt;C.5 ArXiv
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：arXiv.org 学术论文。&lt;/li&gt;
&lt;li&gt;处理步骤
&lt;ul&gt;
&lt;li&gt;转换为纯文本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;去重
&lt;ul&gt;
&lt;li&gt;使用与验证/测试集对比的方法去重。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c6-github&#34;&gt;C.6 GitHub
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：GitHub 上的开源项目。&lt;/li&gt;
&lt;li&gt;获取方式
&lt;ul&gt;
&lt;li&gt;收集星标数 &amp;gt; 100 的仓库。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;提取内容
&lt;ul&gt;
&lt;li&gt;提取可用于语言建模的文本（代码、README、注释等）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;限制条件
&lt;ul&gt;
&lt;li&gt;单个仓库克隆和提取时间不超过 300 秒。&lt;/li&gt;
&lt;li&gt;文件大小上限为 100KB（避免大文件中的重复自动生成内容）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c7-freelaw&#34;&gt;C.7 FreeLaw
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：法律数据库。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;未提供详细清洗步骤。&lt;/li&gt;
&lt;li&gt;数据来自已有结构化格式，可能已做过预处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c8-stack-exchange&#34;&gt;C.8 Stack Exchange
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Stack Overflow 等问答网站。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;提取问题、回答、评论。&lt;/li&gt;
&lt;li&gt;按照层级结构组织。&lt;/li&gt;
&lt;li&gt;保留 &lt;code&gt;/me&lt;/code&gt; 类型的动作描述，删除系统消息。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c9-uspto-backgrounds&#34;&gt;C.9 USPTO Backgrounds
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：美国专利商标局（USPTO）公开数据。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;处理 XML 格式的专利文件。&lt;/li&gt;
&lt;li&gt;提取“Background”部分内容。&lt;/li&gt;
&lt;li&gt;处理不同格式变化（APS → XML）。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c10-pubmed-abstracts&#34;&gt;C.10 PubMed Abstracts
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：PubMed 数据库摘要。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;排除缺失或格式错误的条目。&lt;/li&gt;
&lt;li&gt;合并标题和摘要，去除版权信息。&lt;/li&gt;
&lt;li&gt;排除已在 PMC 中出现的内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c11-project-gutenbergpg-19&#34;&gt;C.11 Project Gutenberg（PG-19）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：古登堡计划电子书。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;处理方式&lt;/strong&gt; ：无额外处理。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c12-opensubtitles&#34;&gt;C.12 OpenSubtitles
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Tiedemann (2016) 提供的英文字幕数据。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;提取 XML 文件中的字幕文本。&lt;/li&gt;
&lt;li&gt;忽略元数据。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c13-wikipedia-en&#34;&gt;C.13 Wikipedia (en)
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Wikipedia English dataset from TensorFlow Datasets。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;wikipedia/20200301.en&lt;/code&gt; 数据集。&lt;/li&gt;
&lt;li&gt;在每篇文章开头添加标题。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c14-deepmind-mathematicsdm-math&#34;&gt;C.14 DeepMind Mathematics（DM Math）
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：DeepMind 数学数据集。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;包含 Easy、Medium、Hard 难度。&lt;/li&gt;
&lt;li&gt;将每个题目拆分为 8 KiB 块。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c15-ubuntu-irc&#34;&gt;C.15 Ubuntu IRC
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Ubuntu IRC 日志（2004–2020）。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;删除系统消息（如加入、离开频道）。&lt;/li&gt;
&lt;li&gt;保留 &lt;code&gt;/me&lt;/code&gt; 动作。&lt;/li&gt;
&lt;li&gt;去除时间戳。&lt;/li&gt;
&lt;li&gt;每周日志合并为一个文档，按日期分隔。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c16-bookcorpus2&#34;&gt;C.16 BookCorpus2
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：基于 Kobayashi (2018) 方法重新构建。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;收集更多书籍（共 17,868 本，原版为 11,038 本）。&lt;/li&gt;
&lt;li&gt;使用修改后的 EPUB 解析器提取文本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c17-europarl&#34;&gt;C.17 EuroParl
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：欧洲议会会议记录。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;已经是干净文本，无需额外清洗。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c18-hackernews&#34;&gt;C.18 HackerNews
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Hacker News 提交链接。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;提取文章标题、URL、子标题、作者。&lt;/li&gt;
&lt;li&gt;按照评论层级组织内容。&lt;/li&gt;
&lt;li&gt;使用 html2text 提取 HTML 文本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c19-youtube-subtitles&#34;&gt;C.19 YouTube Subtitles
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：YouTube 视频字幕。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;三阶段构建：
&lt;ol&gt;
&lt;li&gt;GPT-3 生成搜索关键词。&lt;/li&gt;
&lt;li&gt;下载相关视频。&lt;/li&gt;
&lt;li&gt;提取字幕并按时间对齐。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;多语言字幕按分钟段落对齐，并标注语言。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c20-philpapers&#34;&gt;C.20 PhilPapers
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：PhilPapers 数据库（哲学论文）。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;使用 OAI-MPH 协议抓取元数据。&lt;/li&gt;
&lt;li&gt;转换为纯文本。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c21-nih-exporter&#34;&gt;C.21 NIH ExPorter
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：NIH Grant Application 数据。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;合并 ExPORTER 和 CRISP 数据。&lt;/li&gt;
&lt;li&gt;按申请 ID 去重。&lt;/li&gt;
&lt;li&gt;删除空或太短的摘要。&lt;/li&gt;
&lt;li&gt;去除行政模板内容。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h3 id=&#34;c22-enron-emails&#34;&gt;C.22 Enron Emails
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;来源&lt;/strong&gt; ：Enron 公司邮件存档。&lt;/li&gt;
&lt;li&gt;处理方式
&lt;ul&gt;
&lt;li&gt;使用 &lt;code&gt;mailparser&lt;/code&gt; 提取邮件正文作为文档。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>大语言模型数据清洗 · 论文笔记（一）</title>
        <link>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</link>
        <pubDate>Fri, 06 Jun 2025 01:38:32 +0800</pubDate>
        
        <guid>https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/</guid>
        <description>&lt;h2 id=&#34;ccnet-extracting-high-quality-monolingual-datasets-from-web-crawl-data&#34;&gt;CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data
&lt;/h2&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1911.00359&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ArXiv1911.00359 CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://github.com/facebookresearch/cc_net&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Github facebookresearch/cc_net: Tools to download and cleanup Common Crawl data&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;概述&#34;&gt;概述
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;为保证数据质量，倾向于通过维基百科等高质量数据来源合成&lt;code&gt;ad-hoc datasets&lt;/code&gt;（专门构建的数据集），但是低资源语言不好做&lt;/li&gt;
&lt;li&gt;paper从CC数据集出发，执行了FastText所提出的pipeline，但不同之处：
&lt;ul&gt;
&lt;li&gt;保留文档级别的结构，支持Bert等需要段落级别的模型训练
&lt;ul&gt;
&lt;li&gt;之前的方法切成单个句子，只关心局部上下文，切分成了n-gram&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;增加一个可选的&lt;strong&gt;单语言过滤&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;针对目标语言进行筛选&lt;/li&gt;
&lt;li&gt;筛选出接近维基百科风格的文档
&lt;ul&gt;
&lt;li&gt;在目标语言的维基百科等语料上训练一个语言模型&lt;/li&gt;
&lt;li&gt;通过困惑度进行文档打分，只保留那些 perplexity 较低的文档&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;与传统方法相比：
&lt;ul&gt;
&lt;li&gt;传统方法：多数只适用于英语的特殊方法，手动设置规则&lt;/li&gt;
&lt;li&gt;paper：通用性强，适用于多种语言&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;method&#34;&gt;Method
&lt;/h2&gt;&lt;p&gt;&lt;img src=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250605191955047.png&#34;
	width=&#34;1142&#34;
	height=&#34;594&#34;
	srcset=&#34;https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250605191955047_hu_c0a8034b02c0e136.png 480w, https://example.com/p/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20250605191955047_hu_f79931d4cc592469.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Figure 1&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;192&#34;
		data-flex-basis=&#34;461px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下载网页快照的.WET文件，为每个段落生成哈希值，转化为.bin的二进制文件&lt;/li&gt;
&lt;li&gt;独立处理每个WET下的文档，通过哈希进行去重，识别语言，计算困惑度&lt;/li&gt;
&lt;li&gt;按照语言和困惑度得分重新分组，保存为 JSON 格式的文件&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;预处理&#34;&gt;预处理
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;每个快照包含大约 20 到 30TB 的未压缩纯文本&lt;/li&gt;
&lt;li&gt;将 WET 文件分组为每个 5GB 的分片（shard），转化为json格式&lt;/li&gt;
&lt;li&gt;json中的每一条entry：记录了url、文本等信息，代表了一个网页的内容
&lt;ul&gt;
&lt;li&gt;文本中含有段落&lt;/li&gt;
&lt;li&gt;所以这里的逻辑是：快照（.WET） &amp;gt; shard &amp;gt; entry &amp;gt; 段落&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;去重&#34;&gt;去重
&lt;/h3&gt;&lt;p&gt;需要删除不同网页之间的重复段落（占了70%），为方便去重：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标准化&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;字符全部小写&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所有数字变成0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;删除所有Unicode的Punctuation Marks（标点符号）、Accent Marks（重音符号），完成段落标准化&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;标点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;英文标点：&lt;code&gt;.,!?;:&amp;quot;&#39;()[]{}-–—…@#$%^&amp;amp;*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;中文标点：&lt;code&gt;，。！？；：“”‘’（）【】《》……&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;法语标点：&lt;code&gt;«»&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;阿拉伯语标点：&lt;code&gt;،؛؟&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;日文标点：&lt;code&gt;、。，・「」『』&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Accent：表示发音变化或区分拼写&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;法语：&lt;code&gt;à, é, ô, ù, ç&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;西班牙语：&lt;code&gt;ñ, á, é&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;德语：&lt;code&gt;ä, ö, ü&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;波兰语：&lt;code&gt;ą, ę, ś, ź&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;希腊语：&lt;code&gt;ά, έ, ό&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;计算哈希
&lt;ul&gt;
&lt;li&gt;对每个shard的每个段落计算SHA哈希值（160位），保存为二进制文件&lt;code&gt;.bin&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;每个段落的前64位作为其id便于维护&lt;/li&gt;
&lt;li&gt;对每个段落，查询处理过的**一些（见后文）**shard的二进制文件，若出现过则舍弃，否则保存在本shard二进制文件中&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于很多步骤都是独立的，因此支持并行&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于网页数据，需要去掉导航栏、cookie、联系信息等&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;语言识别&#34;&gt;语言识别
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;FastText模型，基于 Wikipedia、Tatoeba 和 SETimes 数据集进行训练&lt;/li&gt;
&lt;li&gt;支持176种语言，为每一种语言输出0-1的置信度（总和为1）&lt;/li&gt;
&lt;li&gt;若某语言得分超过0.5则进行确认，否则舍弃（无法识别语言）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;语言模型过滤&#34;&gt;语言模型过滤
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;对每种语言，训练了一个tokenizer和语言模型
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;KenLM 库&lt;/strong&gt;实现的5-gram模型（处理大量数据效率高）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;使用tokenizer对每一个entry进行分词，使用语言模型计算每个段落的困惑度&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;消融实验&#34;&gt;消融实验
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;先去重再语言识别
&lt;ul&gt;
&lt;li&gt;可以去除一些英文的Cookie警告，防止误识别为英文&lt;/li&gt;
&lt;li&gt;去重跨越的shard越多去除内容越多，去重效果越好，但是自然开销变大
&lt;ul&gt;
&lt;li&gt;选择50均衡了资源与性能&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;最终数据集的指标&#34;&gt;最终数据集的指标
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;使用训练好的语言模型对段落进行困惑度（perplexity）评分，作为衡量文本质量的代理指标。&lt;/li&gt;
&lt;li&gt;结果发现：
&lt;ul&gt;
&lt;li&gt;高质量内容（如新闻、写作规范的内容）通常位于数据集的“头部”（head）&lt;/li&gt;
&lt;li&gt;含有关键词列表或与 Wikipedia 差异较大的口语化内容会落在“尾部”（tail）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;不同语言的困惑度分布差异较大，这主要是由于训练语言模型所使用的 Wikipedia 数据大小不同，而不是某些语言本身缺乏高质量内容。&lt;/li&gt;
&lt;li&gt;因此，为每种语言设置了不同的困惑度阈值，将语料库划分为三个部分：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Head（头部）&lt;/strong&gt; ：高质量段落&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Middle（中部）&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tail（尾部）&lt;/strong&gt; ：较低质量段落&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了验证数据集的质量，作者使用 fastText 和 BERT 模型进行实验：&lt;/p&gt;
&lt;h4 id=&#34;fasttext-实验&#34;&gt;fastText 实验
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;对英语和波兰语的不同质量子集（head、mid、tail）训练词向量&lt;/li&gt;
&lt;li&gt;在标准的类比任务数据集（Mikolov et al., 2013）上评估性能&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;结果表明&lt;/strong&gt; ：随着从 tail 到 head 的变化，模型性能逐步提升，说明基于困惑度的过滤方法能有效提升数据质量&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;子集&lt;/th&gt;
          &lt;th&gt;英语总分&lt;/th&gt;
          &lt;th&gt;波兰语总分&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;head&lt;/td&gt;
          &lt;td&gt;77.9&lt;/td&gt;
          &lt;td&gt;65.3&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;mid&lt;/td&gt;
          &lt;td&gt;74.2&lt;/td&gt;
          &lt;td&gt;62.8&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;tail&lt;/td&gt;
          &lt;td&gt;62.0&lt;/td&gt;
          &lt;td&gt;59.9&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h4 id=&#34;bert-实验&#34;&gt;BERT 实验
&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;分别使用 Wikipedia 和 CCNet 提取的 head 数据训练 BERT-BASE 模型&lt;/li&gt;
&lt;li&gt;训练语言包括：英语（en）、俄语（ru）、中文（zh）、乌尔都语（ur）&lt;/li&gt;
&lt;li&gt;使用 XNLI 任务评估模型表现&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;语言&lt;/th&gt;
          &lt;th&gt;Wikipedia 准确率&lt;/th&gt;
          &lt;th&gt;CCNet 准确率&lt;/th&gt;
          &lt;th&gt;提升幅度&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;en&lt;/td&gt;
          &lt;td&gt;82.8&lt;/td&gt;
          &lt;td&gt;85.0&lt;/td&gt;
          &lt;td&gt;+2.2&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;ru&lt;/td&gt;
          &lt;td&gt;73.3&lt;/td&gt;
          &lt;td&gt;76.4&lt;/td&gt;
          &lt;td&gt;+3.1&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;zh&lt;/td&gt;
          &lt;td&gt;77.0&lt;/td&gt;
          &lt;td&gt;77.9&lt;/td&gt;
          &lt;td&gt;+0.9&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;ur&lt;/td&gt;
          &lt;td&gt;57.3&lt;/td&gt;
          &lt;td&gt;64.3&lt;/td&gt;
          &lt;td&gt;+7.0 ✅（显著提升）&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;blockquote&gt;
&lt;p&gt;特别是对于低资源语言乌尔都语（ur），Wikipedia 数据太小导致模型几乎无效，而使用 CCNet 提取的数据训练后，准确率提升了 &lt;strong&gt;7 个百分点&lt;/strong&gt; ，证明了该数据集对低资源语言预训练的重要性。&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;维基百科的数据不足，CCNet从CC中提取了高质量语言专用的数据，效果显著&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
