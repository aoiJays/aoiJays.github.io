<!DOCTYPE html>
<html lang="zh-cn" dir="ltr">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content="Large Language Diffusion Models">
<title>Diffusion Language Model · 论文笔记（一）</title>

<link rel='canonical' href='https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/'>

<link rel="stylesheet" href="/scss/style.min.b9c8156d464c343bdacaf14a871581fb94cbbdb9dd5cbce4ba017361187cc930.css"><meta property='og:title' content="Diffusion Language Model · 论文笔记（一）">
<meta property='og:description' content="Large Language Diffusion Models">
<meta property='og:url' content='https://example.com/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/'>
<meta property='og:site_name' content='BiribiriBird'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='DLM' /><meta property='article:tag' content='LLaDA' /><meta property='article:published_time' content='2025-09-26T15:46:00&#43;08:00'/><meta property='article:modified_time' content='2025-09-26T15:46:00&#43;08:00'/>
<meta name="twitter:title" content="Diffusion Language Model · 论文笔记（一）">
<meta name="twitter:description" content="Large Language Diffusion Models">
    <link rel="shortcut icon" href="/favicon.ico" />

    </head>
    <body class="
    article-page
    ">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            localStorage.setItem(colorSchemeKey, "light");
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky ">
    <button class="hamburger hamburger--spin" type="button" id="toggle-menu" aria-label="切换菜单">
        <span class="hamburger-box">
            <span class="hamburger-inner"></span>
        </span>
    </button>

    <header>
        
            
            <figure class="site-avatar">
                <a href="/">
                
                    
                    
                    
                        
                        <img src="/img/avatar_hu_bbc58a5457fc2680.png" width="300"
                            height="300" class="site-logo" loading="lazy" alt="Avatar">
                    
                
                </a>
                
                    <span class="emoji">🍥</span>
                
            </figure>
            
        
        
        <div class="site-meta">
            <h1 class="site-name"><a href="/">BiribiriBird</a></h1>
            <h2 class="site-description">Segmentation fault!</h2>
        </div>
    </header><ol class="menu-social">
            
                <li>
                    <a 
                        href='https://space.bilibili.com/498088093'
                        target="_blank"
                        title="Bilibili"
                        rel="me"
                    >
                        
                        
                            <svg t="1743648787418" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2107" width="24" height="24"><path d="M306.005333 117.632L444.330667 256h135.296l138.368-138.325333a42.666667 42.666667 0 1 1 60.373333 60.373333l-78.037333 77.952L789.333333 256A149.333333 149.333333 0 0 1 938.666667 405.333333v341.333334a149.333333 149.333333 0 0 1-149.333334 149.333333h-554.666666A149.333333 149.333333 0 0 1 85.333333 746.666667v-341.333334A149.333333 149.333333 0 0 1 234.666667 256h88.96L245.632 177.962667a42.666667 42.666667 0 0 1 60.373333-60.373334zM789.333333 341.333333h-554.666666a64 64 0 0 0-63.701334 57.856L170.666667 405.333333v341.333334a64 64 0 0 0 57.856 63.701333L234.666667 810.666667h554.666666a64 64 0 0 0 63.701334-57.813334L853.333333 746.666667v-341.333334A64 64 0 0 0 789.333333 341.333333zM341.333333 469.333333a42.666667 42.666667 0 0 1 42.666667 42.666667v85.333333a42.666667 42.666667 0 1 1-85.333333 0v-85.333333a42.666667 42.666667 0 0 1 42.666666-42.666667z m341.333334 0a42.666667 42.666667 0 0 1 42.666666 42.666667v85.333333a42.666667 42.666667 0 1 1-85.333333 0v-85.333333a42.666667 42.666667 0 0 1 42.666667-42.666667z" p-id="2108" fill="#8a8a8a"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='mailto:neworld1111@163.com'
                        target="_blank"
                        title="Email"
                        rel="me"
                    >
                        
                        
                            <svg t="1743649088353" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="6924" width="24" height="24"><path d="M874.666667 181.333333H149.333333c-40.533333 0-74.666667 34.133333-74.666666 74.666667v512c0 40.533333 34.133333 74.666667 74.666666 74.666667h725.333334c40.533333 0 74.666667-34.133333 74.666666-74.666667V256c0-40.533333-34.133333-74.666667-74.666666-74.666667z m-725.333334 64h725.333334c6.4 0 10.666667 4.266667 10.666666 10.666667v25.6L512 516.266667l-373.333333-234.666667V256c0-6.4 4.266667-10.666667 10.666666-10.666667z m725.333334 533.333334H149.333333c-6.4 0-10.666667-4.266667-10.666666-10.666667V356.266667l356.266666 224c4.266667 4.266667 10.666667 4.266667 17.066667 4.266666s12.8-2.133333 17.066667-4.266666l356.266666-224V768c0 6.4-4.266667 10.666667-10.666666 10.666667z" fill="#666666" p-id="6925"></path></svg>
                        
                    </a>
                </li>
            
                <li>
                    <a 
                        href='https://github.com/aoiJays'
                        target="_blank"
                        title="GitHub"
                        rel="me"
                    >
                        
                        
                            <svg t="1743648734849" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="5802" width="24" height="24"><path d="M511.957333 21.333333C241.024 21.333333 21.333333 240.981333 21.333333 512c0 216.832 140.544 400.725333 335.573334 465.664 24.490667 4.394667 32.256-10.069333 32.256-23.082667 0-11.690667 0.256-44.245333 0-85.205333-136.448 29.610667-164.736-64.64-164.736-64.64-22.314667-56.704-54.4-71.765333-54.4-71.765333-44.586667-30.464 3.285333-29.824 3.285333-29.824 49.194667 3.413333 75.178667 50.517333 75.178667 50.517333 43.776 75.008 114.816 53.333333 142.762666 40.789333 4.522667-31.658667 17.152-53.376 31.189334-65.536-108.970667-12.458667-223.488-54.485333-223.488-242.602666 0-53.546667 19.114667-97.322667 50.517333-131.669334-5.034667-12.330667-21.930667-62.293333 4.778667-129.834666 0 0 41.258667-13.184 134.912 50.346666a469.802667 469.802667 0 0 1 122.88-16.554666c41.642667 0.213333 83.626667 5.632 122.88 16.554666 93.653333-63.488 134.784-50.346667 134.784-50.346666 26.752 67.541333 9.898667 117.504 4.864 129.834666 31.402667 34.346667 50.474667 78.122667 50.474666 131.669334 0 188.586667-114.730667 230.016-224.042666 242.090666 17.578667 15.232 33.578667 44.672 33.578666 90.453334v135.850666c0 13.141333 7.936 27.605333 32.853334 22.869334C862.250667 912.597333 1002.666667 728.746667 1002.666667 512 1002.666667 240.981333 783.018667 21.333333 511.957333 21.333333z" p-id="5803" fill="#8a8a8a"></path></svg>
                        
                    </a>
                </li>
            
        </ol><ol class="menu" id="main-menu">
        
        
        
        <li >
            <a href='/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="5 12 3 12 12 3 21 12 19 12" />
  <path d="M5 12v7a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-7" />
  <path d="M9 21v-6a2 2 0 0 1 2 -2h2a2 2 0 0 1 2 2v6" />
</svg>



                
                <span>主页</span>
            </a>
        </li>
        
        
        <li >
            <a href='/about-me/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>About me</span>
            </a>
        </li>
        
        
        <li >
            <a href='/archives/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <rect x="3" y="4" width="18" height="4" rx="2" />
  <path d="M5 8v10a2 2 0 0 0 2 2h10a2 2 0 0 0 2 -2v-10" />
  <line x1="10" y1="12" x2="14" y2="12" />
</svg>



                
                <span>Archives</span>
            </a>
        </li>
        
        
        <li >
            <a href='/search/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="10" cy="10" r="7" />
  <line x1="21" y1="21" x2="15" y2="15" />
</svg>



                
                <span>Search</span>
            </a>
        </li>
        
        
        <li >
            <a href='/experience/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M10 14a3.5 3.5 0 0 0 5 0l4 -4a3.5 3.5 0 0 0 -5 -5l-.5 .5" />
  <path d="M14 10a3.5 3.5 0 0 0 -5 0l-4 4a3.5 3.5 0 0 0 5 5l.5 -.5" />
</svg>



                
                <span>Experience</span>
            </a>
        </li>
        
        
        <li >
            <a href='/friend/' >
                
                
                
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="7" r="4" />
  <path d="M6 21v-2a4 4 0 0 1 4 -4h4a4 4 0 0 1 4 4v2" />
</svg>



                
                <span>Friend</span>
            </a>
        </li>
        
        <li class="menu-bottom-section">
            <ol class="menu">

                
            </ol>
        </li>
    </ol>
</aside>

    <aside class="sidebar right-sidebar sticky">
        
            
                
    <section class="widget archives">
        <div class="widget-icon">
            <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



        </div>
        <h2 class="widget-title section-title">目录</h2>
        
        <div class="widget--toc">
            <nav id="TableOfContents">
  <ol>
    <li><a href="#intro">Intro</a></li>
    <li><a href="#approach">Approach</a>
      <ol>
        <li><a href="#概率公式">概率公式</a>
          <ol>
            <li><a href="#前向过程forward-process">前向过程Forward Process</a></li>
            <li><a href="#反向过程reverse-process">反向过程Reverse Process</a></li>
            <li><a href="#mask-predictor">Mask Predictor</a></li>
            <li><a href="#损失函数">损失函数</a></li>
            <li><a href="#负对数似然的上界">负对数似然的上界</a></li>
          </ol>
        </li>
        <li><a href="#pre-training">Pre-Training</a></li>
        <li><a href="#sft">SFT</a></li>
        <li><a href="#inference">Inference</a></li>
      </ol>
    </li>
    <li><a href="#experiments">Experiments</a>
      <ol>
        <li><a href="#实验1--scalability">实验1 · Scalability</a>
          <ol>
            <li><a href="#实验设计">实验设计</a></li>
            <li><a href="#实验结果">实验结果</a></li>
          </ol>
        </li>
        <li><a href="#实验2--benchmark">实验2 · Benchmark</a>
          <ol>
            <li><a href="#实验设计-1">实验设计</a></li>
            <li><a href="#实验结果-1">实验结果</a></li>
          </ol>
        </li>
        <li><a href="#实验2--补充实验">实验2 · 补充实验</a></li>
        <li><a href="#实验3--reversal-reasoning-and-analyses">实验3 · Reversal Reasoning and Analyses</a>
          <ol>
            <li><a href="#实验设计-2">实验设计</a></li>
            <li><a href="#附录补充">附录补充</a></li>
          </ol>
        </li>
        <li><a href="#case-studies">Case Studies</a></li>
      </ol>
    </li>
  </ol>
</nav>
        </div>
    </section>

            
        
    </aside>


            <main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" >
                论文笔记
            </a>
        
            <a href="/categories/dlm/" >
                DLM
            </a>
        
    </header>
    

    <div class="article-title-wrapper">
        <h2 class="article-title">
            <a href="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/">Diffusion Language Model · 论文笔记（一）</a>
        </h2>
    
        
        <h3 class="article-subtitle">
            Large Language Diffusion Models
        </h3>
        
    </div>

    
    
    
    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Sep 26, 2025</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    阅读时长: 11 分钟
                </time>
            </div>
        
    </footer>
    

    
</div>

</header>

    <section class="article-content">
    
    
    <p><a class="link" href="https://arxiv.org/abs/2502.09992"  target="_blank" rel="noopener"
    >Large Language Diffusion Models</a></p>
<p>[TOC]</p>
<h2 id="intro">Intro
</h2><p>理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布</p>
<p>因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布</p>
<p>因此指令跟随、上下文学习并不是ARM的专利</p>
<h2 id="approach">Approach
</h2><h3 id="概率公式">概率公式
</h3><h4 id="前向过程forward-process">前向过程Forward Process
</h4><p>序列中逐渐添加mask，直到所有序列全部masked</p>
<ul>
<li>每个标记有一定概率masked，或者保持unmasked状态</li>
<li>给定原始数据$x_0$，随机采样的一个时间点$t \in [0,1]$
<ul>
<li>$t=0$表示起点，全部token都是unmasked</li>
<li>$t=1$表示终点，全部token都已经masked</li>
</ul>
</li>
<li>序列中每个token的masked概率就是$t$</li>
<li>该时刻的序列被定义为$x_t$​</li>
</ul>
<blockquote>
<p>Bert的mask比例是固定的</p></blockquote>
<h4 id="反向过程reverse-process">反向过程Reverse Process
</h4><p>参数为$\theta$的模型，生成序列$x_0$的概率$p_\theta(x_0)$就是我们需要训练的模型</p>
<p>我们希望其尽可能接近真实数据的分布$p_{data}(x_0)$</p>
<ul>
<li>反向过程的目标：$x_{t=1}$出发，恢复$x_0$</li>
<li>方法：通过<code>mask predictor</code>，逐步填充<code>masked token</code></li>
</ul>
<h4 id="mask-predictor">Mask Predictor
</h4><p>LLaDA的核心是一个<code>mask predictor</code></p>
$$
p_\theta\left (\cdot\mid x_t \right)
$$<p>
其中的<code>·</code>表示一个占位符</p>
<p>例如：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">I [MASK] cats.
</span></span></code></pre></td></tr></table>
</div>
</div><p>则$p_\theta(\cdot \mid [I,[MASK],cats])$ 就是一个基于词表的概率分布表</p>
<div class="table-wrapper"><table>
  <thead>
      <tr>
          <th style="text-align: center">token</th>
          <th style="text-align: center">p</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">like</td>
          <td style="text-align: center">0.9</td>
      </tr>
      <tr>
          <td style="text-align: center">eat</td>
          <td style="text-align: center">0.1</td>
      </tr>
  </tbody>
</table></div>
<p>此时<code>Mask Predictor</code>会对所有<code>[MASK]</code>进行预测</p>
<p>不像ARM只预测一个token</p>
<p>假设序列为：$(x_t^1, x_t^2,&hellip;,x_t^L)$</p>
<p>我们的目标即为，对于<code>masked</code>位置$i$，最大化概率：</p>
$$
p_\theta(x_0^i\mid x_t)
$$<p>其中$x_0^i$就是原序列的ground truth</p>
<h4 id="损失函数">损失函数
</h4><p>对于单个<code>masked</code>位置，我们希望概率尽可能大</p>
<p>因此需要使用交叉熵损失</p>
<hr>
<p>补习一下交叉熵</p>
<p>假设真实分布是$q(y)$，模型分布是$p_\theta(y)$</p>
<p>交叉熵定义为：</p>
$$
H(q,p_\theta) = -\sum_y  q(y)\log{p_\theta(y)}
$$<p>
在该任务下，$q(y)$​是一个独热分布</p>
$$
q(y) = \left\{\begin{matrix}
  1&,y=x_0^i \\
  0&,otherwise
\end{matrix}\right.
$$<p>
代入得</p>
$$
\mathcal{L}(x_0^i,x_t) = -\log p_\theta(x_0^i\mid x_t)
$$<p>
故整体的损失就只需要对所有<code>masked</code>位置求和</p>
$$
\mathcal{L}(x_0,x_t) = -\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$<p>
其中$1\left[ x^i_t=M\right] $表示指示函数，确保代入计算的数值是<code>[MASK]</code></p>
<p>但是这样是不合理的，序列中<code>[MASK]</code>越多，损失似乎会越大</p>
<p>因此需要做一下归一化</p>
<p>对于$x_t$，其在该时刻会有$tL$个token被<code>masked</code></p>
<p>因此需要代入一个$\frac{1}{t}$​（值得一提的是，$\frac{1}{t} \geq 1$）</p>
$$
\mathcal{L}(x_0,x_t) = -\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}
$$<p>
建立关于模型参数$\theta$的损失函数则有：</p>
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$<ul>
<li>$\triangleq$代表<code>定义为</code></li>
</ul>
<blockquote>
<p>这里不是在陈述一个“事实”，而是在<strong>引入损失函数的定义</strong>。</p>
<p>如果写$=$，读者可能会以为“这是某个推导得到的等式”；</p>
<p>如果写 $\triangleq$，读者一眼就知道：哦，这是“定义”，不是推导。</p></blockquote>
<ul>
<li>从均匀分布$U(0,1)$采样的任意$t$，从数据集中采样的任意数据$x_0$，根据前向传播方法得到的$x_t$</li>
</ul>
<h4 id="负对数似然的上界">负对数似然的上界
</h4><p>我们本身的目标是使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$</p>
<p>但是我们从未单独定义、训练$p_\theta(x)$这个模型</p>
<p>而是定义了一个$p_\theta(x_0^i\mid x_t)$，不断重复迭代，起到了$p_\theta(x)$的作用</p>
<p>因此我们上述内容得到的$L(\theta)$是作为模型$p_\theta(x_0^i\mid x_t)$的损失函数</p>
<p>我们如何保证训练出这个模型，可以使得$p_\theta(x_0)$的分布接近$p_{data}(x_0)$？</p>
<hr>
<p>我们定义真实的似然函数是</p>
$$
\mathcal{L}(\theta) = -\mathbb{E}_{p_{data}(x_0)}\left [\log{p_{\theta}(x_0)}\right]
$$<ul>
<li>按照真实分布，采样数据$x_0$，得到的似然函数期望</li>
</ul>
<p>我们需要最小化这个式子</p>
<p>定义前向加噪分布$q$：</p>
$$
q(x_t\mid x_0) = \prod_{i=1}^L \left [ (1-t)\times 1(x_t^i=x_0^i) + t\times 1(x_t^i=M) \right]
$$<p>
其描述了通过已知的前向过程，从$x_0$得到$x_t$的概率</p>
<blockquote>
<p>$q$是我们引入的噪声分布，并非需要训练的参数模型，不使用$p$定义</p></blockquote>
<p>由于前向过程是已知的，我们考虑使用它表示$p_\theta$</p>
$$
p_\theta(x_0) = \sum_{x_t}p_\theta(x_0\mid x_t)p_\theta(x_t)= \sum_{x_t}p_\theta(x_0, x_t) 
$$<blockquote>
<p>原始句子出现的总概率就是把所有可能路径的概率加起来。</p>
<p>（先得到$x_t$，再生成$x_0$）</p></blockquote>
<p>引入已知的$q$</p>
$$
p_\theta(x_0) = \sum_{x_t}q(x_t\mid x_0)\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}
$$<p>
其中$\sum_{x_t}q(x_t\mid x_0)] \times (\cdot)$，可以理解为对$q(x_t\mid x_0)$的期望</p>
<blockquote>
<p>离散情况下：$\mathbb{E}_{x\sim r}(g(x)) = \sum_x r(x)g(x)$</p></blockquote>
<p>此时$x_0$是当作固定值，所有都可以看作关于$x_t$的函数</p>
<p>因此则有：</p>
$$
p_\theta(x_0) = \mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]
$$<blockquote>
<p>任意从$q$分布中采样$x_t$</p></blockquote>
<p>采样 Jensen不等式：</p>
$$
\log \mathbb{E}(z) \geq \mathbb{E}(\log{z})
$$<p>
此时则有：</p>
$$
\log{p_\theta}(x_0) = \log{\mathbb{E}_{ q(x_t\mid x_0)}\left[\frac{p_\theta(x_0,x_t)}{q(x_t\mid x_0)}\right]} \geq \mathbb{E}_{ q(x_t\mid x_0)}\left[\log{p_\theta(x_0,x_t)} - \log{q(x_t\mid x_0)} \right]
$$$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0,x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$<p>
分解一下联合概率</p>
$$
\log{p_\theta(x_0,x_t)} = \log{p_\theta(x_0\mid x_t)} + \log{p_\theta(x_t)}
$$<p>
代回则有：</p>
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}-\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_t)} + \mathbb{E}_{ q(x_t\mid x_0)} \log{q(x_t\mid x_0)}
$$<ul>
<li>$p_\theta(x_t)$：其中$x_t$是前向过程人为生成的，因此与$\theta$无关</li>
<li>$q(x_t\mid x_0)$是噪声项，与$\theta$无关</li>
</ul>
<p>因此可以写成</p>
$$
-\log{p_\theta}(x_0) \leq -\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)} + \text{const}
$$<p>
两边同时对真实数据计算期望</p>
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}(\mathbb{E}_{ q(x_t\mid x_0)}\log{p_\theta(x_0\mid x_t)}) + \text{const}
$$<p>
左边实质上就是我们需要优化的目标，命名为负对数似然$\text{NLL}$</p>
<p>根据</p>
$$
\log {p_\theta(x_0\mid x_t)} = \sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)
$$<p>
代入得：</p>
$$
-\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq -\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const}
$$<p>
事实上右边就是：</p>
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] = 
 -\mathbb{E}_{t,x_0,x_t}\left[\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right] = tL(\theta)
$$<p>
上述内容都是正项（负概率对数），$t \in [0,1]$，因此满足</p>
$$
-\mathbb{E}_{p_{data}(x_0)}\mathbb{E}_{ q(x_t\mid x_0)}\left[\sum_{i=1}^L 1\left [ x^i_t=M\right ] \log{p_\theta}(x_0^i\mid x_t)\right] + \text{const} \leq L(\theta) + \text{const}
$$<p>
则有：</p>
$$
\text{NLL} = -\mathbb{E}_{p_{data}(x_0)}(\log{p_\theta}(x_0) ) \leq L(\theta) + \text{const}
$$<p>
至此，成功证明了$L(\theta)$决定了$\text{NLL}$​的上界（常数可以忽略）</p>
<h3 id="pre-training">Pre-Training
</h3><p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2.png"
	width="1628"
	height="456"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_9a85fb3e2f02f873.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/x2_hu_3f7bfef69b5c6f92.png 1024w"
	loading="lazy"
	
		alt="A Conceptual Overview of LLaDA."
	
	
		class="gallery-image" 
		data-flex-grow="357"
		data-flex-basis="856px"
	
></p>
<ul>
<li>输入：<code>mask predictor</code>$p_\theta$，训练数据$p_{data}$</li>
<li>输出：$p_{\theta}$（收敛）</li>
</ul>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976.png"
	width="1570"
	height="385"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_f227ce11f9ed9b16.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007220922976_hu_7a4e250f2607c545.png 1024w"
	loading="lazy"
	
		alt="Pre-Train Algorithm"
	
	
		class="gallery-image" 
		data-flex-grow="407"
		data-flex-basis="978px"
	
></p>
<ul>
<li><code>mask predictor</code>采用Transformer架构
<ul>
<li>不采用<code>causal mask</code>，能看见双向上下文</li>
<li>未使用KV Cache，采用标准的<code>Vanilla Multi-Head Attention</code>，每个头单独一份<code>k,q,v</code></li>
<li>Transformer架构尽量与LLaMA3对齐，从<code>attention</code>和<code>FFN</code>两个参数大头中，选择了减少<code>FFN</code>的参数量，保持参数规模可以比较</li>
</ul>
</li>
</ul>
<blockquote>
<p>在自回归 LLM 生成时，生成新 token 时可以复用之前的 K/V 矩阵（不用重新算整个序列的注意力）。</p>
<p>这是 KV cache 的意义：极大加速推理，节省显存。</p>
<p>但 LLaDA 每一步预测的是 <strong>全局被 mask 的位置（不是单个 token）</strong>，所以每一步输入分布会变，全序列 K/V 都要重新计算 → KV cache 无法使用。</p></blockquote>
<ul>
<li>99%的数据固定长度4096</li>
<li>1%的数据随机采样长度</li>
</ul>
<h3 id="sft">SFT
</h3><p>对于问答对$(p_0,r_0)$，我们不改变提问部分，只对<strong>回答部分</strong>进行掩码加噪得到$r_t$</p>
<p>损失函数设计为：</p>
$$
-\mathbb{E}_{t,p_0,r_0,r_t}\left[\frac{1}{t}\sum_{i=1}^{L'}1[r^i_t = M]p_\theta(r_0^i\mid p_0,r_t)\right]
$$<ul>
<li>$r_0$的长度是天然动态的，使用<code>EOS</code>填充</li>
</ul>
<h3 id="inference">Inference
</h3><p>给定$p_0$，我们从完全掩码的$r_1$开始</p>
<p>设定超参数如下：</p>
<ul>
<li>迭代次数（采样步骤总数）：a trade off between efficency and quality</li>
<li>生成长度：实质上是一个上界</li>
</ul>
<p>假设我们从时间$t\in(0, 1]$转移到$s\in[0,t)$，需要做的事是：</p>
<ul>
<li>$p_0,r_t$作为模型的输入，预测$r_0$（模型会<code>unmask</code>所有被掩码的token）</li>
<li>由于我们只转移到$s$，因此需要保留$sL$个掩码
<ul>
<li>对预测出的$r_0$，从中<strong>随机</strong><code>remask</code>$\frac{s}{t}L$个token</li>
<li>得到$r_s$</li>
</ul>
</li>
<li>$s = t, r_t = r_s$重复迭代</li>
</ul>
<p>默认将$|t-s|$​是一个定值，以定长的步长进行迭代</p>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257.png"
	width="1647"
	height="627"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_539b4cf9daaad9c4.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222150257_hu_3153327713f2069d.png 1024w"
	loading="lazy"
	
		alt="Reverse Process"
	
	
		class="gallery-image" 
		data-flex-grow="262"
		data-flex-basis="630px"
	
></p>
<hr>
<p>理论上remask策略是随机的</p>
<p>但是论文给出了两种基于<code>退火</code>的remask策略</p>
<blockquote>
<p>在生成过程中，需要随机性逐步递减，冻结高确定性的部分、把随机性集中在不确定区域</p></blockquote>
<ul>
<li><strong>low-confidence remasking</strong>：取置信度最低的$\frac{s}{t}L$个预测token进行remask</li>
<li><strong>semi-autoregressive</strong> remasking：对序列进行分块，从左到右顺序生成</li>
</ul>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789.png"
	width="1473"
	height="558"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_c595a78f46bb5a31.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007222344789_hu_8789f573c459aaef.png 1024w"
	loading="lazy"
	
		alt="semi-autoregressive"
	
	
		class="gallery-image" 
		data-flex-grow="263"
		data-flex-basis="633px"
	
></p>
<h2 id="experiments">Experiments
</h2><h3 id="实验1--scalability">实验1 · Scalability
</h3><ul>
<li>验证：LLaDA是否与自回归模型ARM具有相同的可拓展性</li>
</ul>
<blockquote>
<p>目的：证明论文的核心论点：</p>
<p>​	理想情况下，无限数据+无限模型容量+正确训练 ，可以收敛到真实分布</p>
<p>​	因此不管是ARM还是DLM，只要是合格的条件生成模型，都能学到真实语言分布</p>
<p>​	因此指令跟随、上下文学习并不是ARM的专利</p></blockquote>
<h4 id="实验设计">实验设计
</h4><p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932.png"
	width="1568"
	height="583"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_4f064368af95b6bc.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003215457932_hu_8bdc715fd4ddd50c.png 1024w"
	loading="lazy"
	
		alt="架构"
	
	
		class="gallery-image" 
		data-flex-grow="268"
		data-flex-basis="645px"
	
></p>
<p>针对MDM和ARM两类语言模型，进行如下控制变量</p>
<ul>
<li>模型结构：采样同一套Transformer架构（优化器、参数量……各种机制），只修改了mask</li>
<li>参数量：在1B规模下完全一致，在7B规模由于资源限制有一些不同
<ul>
<li>Transformer：causal mask</li>
</ul>
</li>
<li>数据：预训练语料相同</li>
</ul>
<p>唯一的实验的变量：</p>
<ul>
<li>FLOPs：使用6ND公式作为横轴
<ul>
<li><strong>N</strong> 是模型的非 embedding 参数量（固定，比如 1B 或 8B）</li>
<li><strong>D</strong> 是训练过的 token 数量（数据量，可以变化）</li>
<li>实验通过改变 D ，计算出 6 × N × D （即训练 FLOPs）作为横轴的计算预算</li>
</ul>
</li>
</ul>
<p>实验指标：</p>
<ul>
<li>MMLU、ARC-C、CMMLU、PIQA、GSM8K、HumanEval</li>
</ul>
<p>（多任务、推理、中文、物理、数学、代码）</p>
<h4 id="实验结果">实验结果
</h4><p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678.png"
	width="1472"
	height="707"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_598efe4120660d9c.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251003214259678_hu_8ec2789b84388b8c.png 1024w"
	loading="lazy"
	
		alt="实验结果"
	
	
		class="gallery-image" 
		data-flex-grow="208"
		data-flex-basis="499px"
	
></p>
<ul>
<li>部分任务体现优势</li>
<li>对于性能稍逊的任务（PIQA），差距也在逐渐缩小</li>
</ul>
<p>同时喷了先前的一篇工作的结论：<code>达到相同的似然需要16倍算力</code></p>
<ul>
<li>似然是间接指标（LLaDA的lower bound）</li>
<li>先前的工作只有GPT2的参数量，本文提高到7-8B</li>
</ul>
<blockquote>
<p>Nie, S., Zhu, F., Du, C., Pang, T., Liu, Q., Zeng, G., Lin, M., and Li, C. Scaling up masked diffusion models on text. arXiv preprint arXiv:2410.18514, 2024.</p></blockquote>
<p>结论：LLaDA 在相同训练规模与算力条件下，表现出与 ARM 相似甚至更强的可扩展性。</p>
<h3 id="实验2--benchmark">实验2 · Benchmark
</h3><ul>
<li>验证：LLaDA经过预训练和SFT之后是否能够和已有的ARM在<strong>上下文学习</strong>与<strong>指令遵循能力</strong>上进行竞争</li>
</ul>
<h4 id="实验设计-1">实验设计
</h4><ul>
<li>实验对象：LLaDA、一系列参数相当的模型
<ul>
<li>Base阶段：比较了所有模型的预训练base模型</li>
<li>instruct阶段：LLaDA只进行了SFT，其他模型均完成了SFT+RL
<ul>
<li>原文：交给未来的工作</li>
</ul>
</li>
</ul>
</li>
<li>任务：通用、数学科学、代码、中文等常见benchmark</li>
</ul>
<h4 id="实验结果-1">实验结果
</h4><p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237.png"
	width="1208"
	height="717"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_caa01b5802879200.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173251237_hu_c7856aff322a4d51.png 1024w"
	loading="lazy"
	
		alt="Base"
	
	
		class="gallery-image" 
		data-flex-grow="168"
		data-flex-basis="404px"
	
></p>
<ul>
<li>
<p>在所有任务上超过LLaMA2 7B，与LLaMA3 8B相当</p>
<ul>
<li>所有模型的训练数据存在差异</li>
<li>作者认为LLaDA的优势区间与劣势区间的主要原因在数据质量与分布上</li>
</ul>
</li>
<li>
<p>GSM8K数据集上体现了显著的优势，论文针对这个情况做了补充实验，证明不存在数据泄露</p>
</li>
</ul>
<hr>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440.png"
	width="1205"
	height="574"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_db0f9cb7c5edf55e.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007173309440_hu_5dfaa65484da0854.png 1024w"
	loading="lazy"
	
		alt="SFT&#43;RL"
	
	
		class="gallery-image" 
		data-flex-grow="209"
		data-flex-basis="503px"
	
></p>
<ul>
<li>SFT数据质量较差，出现了性能下降（MMLU）</li>
<li>没有采用RL，因此性能略微落后LLaMA3 8B</li>
</ul>
<p>结论：</p>
<ul>
<li>在数据集透明度不足的情况下，以丰富的标准化流程、多样化任务，足以证明LLaDA的性能卓越，是唯一具备竞争力的非自回归模型</li>
</ul>
<h3 id="实验2--补充实验">实验2 · 补充实验
</h3><p>验证：LLaDA在GSM8K数据集中的优势不来源于数据泄露（data leakage），检测在全新数据集中仍然能保证推理能力</p>
<p>省流：找了一个2024年的新数据集，模仿GSM8K的形式做一遍实验</p>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145.png"
	width="548"
	height="173"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_ad845088f2d886d0.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007201005145_hu_8a04e4c9a021371f.png 1024w"
	loading="lazy"
	
		alt="iGSM Dataset"
	
	
		class="gallery-image" 
		data-flex-grow="316"
		data-flex-basis="760px"
	
></p>
<ul>
<li>LLaDA在所有难度（解题步骤数）中均显著优势</li>
<li>两类模型随着难度上升准确度逐渐下降，但是LLaDA下降较慢</li>
</ul>
<p>结论：</p>
<ul>
<li>LLaDA允许模型在每一步同时考虑全局 token 关系，因此在多变量方程、层次关系推理中优于单向自回归</li>
</ul>
<h3 id="实验3--reversal-reasoning-and-analyses">实验3 · Reversal Reasoning and Analyses
</h3><blockquote>
<p>Reversal Curse（反向诅咒）：ARM从左到右生成序列，因此反向生成或逆序推理的表现很差</p></blockquote>
<p>验证：LLaDA是否克服了反向诅咒</p>
<h4 id="实验设计-2">实验设计
</h4><ul>
<li>数据：496对著名中文诗句（上下两句），每一句子（A,B）构成两个任务
<ul>
<li>Forward：给定A预测B</li>
<li>Backward：给定B预测A</li>
</ul>
</li>
</ul>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">窈窕淑女的下一句是什么？直接输出句子即可。 Answer: 君子好逑。
</span></span><span class="line"><span class="cl">不拘一格降人才的上一句是什么？直接输出句子即可。 Answer: 我劝天公重抖擞。
</span></span></code></pre></td></tr></table>
</div>
</div><p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318.png"
	width="990"
	height="401"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_f6b1b119ec68bdcd.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007205703318_hu_9284f82611816e94.png 1024w"
	loading="lazy"
	
		alt="Reversal Curse"
	
	
		class="gallery-image" 
		data-flex-grow="246"
		data-flex-basis="592px"
	
></p>
<ul>
<li>
<p>GPT-4o 和 Qwen2.5 均有更大数据和RL优化，但仍失败</p>
</li>
<li>
<p>LLaDA 虽仅 SFT，无RL，仍在 reversal 上大幅领先</p>
</li>
</ul>
<h4 id="附录补充">附录补充
</h4><p>论文从三个角度补充了为什么LLaDA是无方向偏置的模型</p>
<ul>
<li>理论证明：LLaDA本质上等价于在所有生成顺序上做平均，从而消除方向偏置
<ul>
<li>解释为什么 diffusion 结构在数学上是方向对称的</li>
</ul>
</li>
<li>实现机制：理论正确的情况下，需要确保算法实现不出现<strong>从左到右</strong>
<ul>
<li>确保生成算法本身不引入方向信息</li>
</ul>
</li>
<li>超参数层面：通过实验说明采样步数与效率不会干扰方向一致性
<ul>
<li>排除方向性差异由采样精度造成的可能性</li>
</ul>
</li>
</ul>
<h5 id="a2-inference">A.2 Inference
</h5><ul>
<li>目的：证明LLaDA训练和推理目标等价于<strong>对所有可能生成顺序的平均建模</strong></li>
</ul>
<p>对于训练时的核心目标函数：</p>
$$
L(\theta) \triangleq
 -\mathbb{E}_{t,x_0,x_t}\left[\frac{1}{t}\sum_{i=1}^L1\left[ x^i_t=M\right] \log{p_\theta\left( x_0^i\mid x_t\right)}\right]
$$<p>
训练目标是：模型在任意mask模式下，都能预测出原token</p>
<p>该训练模式不会看到任何固定方向的序列，故天然是双向建模的</p>
<blockquote>
<p>推不动了，pass一下</p></blockquote>
<h5 id="remasking">Remasking
</h5><p>反向过程的核心：预测 - 重新掩码 - 继续预测</p>
<p>上文提到了三种不同的掩码策略</p>
<p>论文使用GSM8K进行了消融实验</p>
<ul>
<li>生成长度固定512</li>
<li>采样步数固定256（长度的一半）</li>
<li>block：32</li>
</ul>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549.png"
	width="1329"
	height="223"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_a185194ec8c04b53.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007223806549_hu_9209def52fe04210.png 1024w"
	loading="lazy"
	
		alt="remask"
	
	
		class="gallery-image" 
		data-flex-grow="595"
		data-flex-basis="1430px"
	
></p>
<ul>
<li>Base模型：最低置信度即可，半自回归是不需要的</li>
<li>Instruct模型：必须是最低置信度+半自回归
<ul>
<li>单独最低置信度会严重降低性能</li>
</ul>
</li>
</ul>
<blockquote>
<p>论文解释：SFT阶段引入了大量<code>EOS</code>，模型一般会给<code>EOS</code>较大的置信度。因此推理时<code>EOS</code>会被大量生成，并且几乎不可能被<code>remask</code>（置信度非常高）</p>
<p>因此需要引入半自回归，保证每个块内收敛出连续的内容，抑制<code>EOS</code>早产</p></blockquote>
<p>尽管引入半自回归，但是块内仍然是并行的（？）</p>
<p>补充一下，模型对生成长度这个超参数非常不敏感</p>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980.png"
	width="1607"
	height="288"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_496709f8b88b2255.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225517980_hu_4144e169c1540880.png 1024w"
	loading="lazy"
	
		alt="Length"
	
	
		class="gallery-image" 
		data-flex-grow="557"
		data-flex-basis="1339px"
	
></p>
<p>但对采样步数非常敏感（生成长度1024）</p>
<p><img src="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929.png"
	width="1472"
	height="508"
	srcset="/p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_afc64e86f9a1bafe.png 480w, /p/diffusion-language-model-%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%E4%B8%80/assets/image-20251007225835929_hu_6b0c69ba2a7f9c56.png 1024w"
	loading="lazy"
	
		alt="采样"
	
	
		class="gallery-image" 
		data-flex-grow="289"
		data-flex-basis="695px"
	
></p>
<hr>
<p>结论：</p>
<ul>
<li>LLaDA不受自回归方向性的约束，具有更平衡的前后向建模能力</li>
</ul>
<h3 id="case-studies">Case Studies
</h3><p>附录中展示了一些其他例子，说明生成的对话是出色的（单轮、多轮）</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/dlm/">DLM</a>
        
            <a href="/tags/llada/">LLaDA</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
        <link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI&#43;WdtXRGWt2kTvGFasHpSy3SV"crossorigin="anonymous"
            ><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG&#43;vnGctmUb0ZY0l8"crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"integrity="sha384-&#43;VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4&#43;/RRE05"crossorigin="anonymous"
                defer
                >
            </script><script>
    window.addEventListener("DOMContentLoaded", () => {
	const mainArticleElement = document.querySelector(".main-article");
        renderMathInElement(mainArticleElement, {
            delimiters: [
                { left: "$$", right: "$$", display: true },
                { left: "$", right: "$", display: false },
                { left: "\\(", right: "\\)", display: false },
                { left: "\\[", right: "\\]", display: true }
            ],
            ignoredClasses: ["gist"]
        });})
</script>

    
</article>

    

    

<aside class="related-content--wrapper">
    <h2 class="section-title">相关文章</h2>
    <div class="related-content">
        <div class="flex article-list--tile">
            
                
<article class="">
    <a href="/p/llada-moe-asparse-moediffusion-language-model/">
        
        

        <div class="article-details">
            <h2 class="article-title">LLaDA-MoE ASparse MoEDiffusion Language Model</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/ultrallada-scaling-the-context-length-to-128k-for-diffusion-large-language-models/">
        
        

        <div class="article-details">
            <h2 class="article-title">UltraLLaDA Scaling the Context Length to 128K for Diffusion Large Language Models</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/llada-rec-discrete-diffusion-for-parallel-semantic-id-generation-in-generative-recommendation/">
        
        

        <div class="article-details">
            <h2 class="article-title">LLaDA-Rec - Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/tidar-think-in-diffusion-talk-in-autoregression/">
        
        

        <div class="article-details">
            <h2 class="article-title">TiDAR - Think in Diffusion, Talk in Autoregression</h2>
        </div>
    </a>
</article>

            
                
<article class="">
    <a href="/p/awesome-llada/">
        
        

        <div class="article-details">
            <h2 class="article-title">Awesome LLaDA</h2>
        </div>
    </a>
</article>

            
        </div>
    </div>
</aside>

     
    
        
    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2020 - 
        
        2026 Example Person
    </section>
    
    <section class="powerby">
        使用 <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> 构建 <br />
        主题 <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.30.0">Stack</a></b> 由 <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a> 设计
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css"crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css"crossorigin="anonymous"
            >

            </main>
        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js"integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z&#43;KMkF24hUW8WePSA9HM="crossorigin="anonymous"
                
                >
            </script><script type="text/javascript" src="/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
